,paper_id,paper_title,prim_audience,abstract,session_item_index,room_name,session_name,session_id
0,5981.0,Spatio-Temporal Feedback Diffusion Control Model,Academician,"This work aims to develop a spatio-temporal feedback diffusion control model designed to capture complex system dynamics and adapt in real-time through continuous feedback integration. The model addresses four primary objectives: (i) accurately representing spatial and temporal patterns in dynamic environments, (ii) continuously adjusting control actions based on real-time feedback, (iii) efficiently handling large-scale, high-resolution data, and (iv) enhancing adaptive control in applications such as process automation. By incorporating diffusion mechanisms into the feedback control loop, the model adapts to evolving spatial distributions and temporal changes, enabling precise control even under fluctuating conditions. To achieve high efficiency with large datasets, the framework leverages advanced computational techniques and data handling strategies that support scalable processing without compromising response time. This approach ensures the model's applicability across diverse scenarios, including high-speed automation systems, where quick adjustments are essential. The model’s continuous feedback loop facilitates rapid adaptation, allowing for real-time control modifications that improve system resilience and performance. Initial simulations show promise for this method in maintaining stability and optimizing control actions in complex, dynamic environments. This research contributes to the field of adaptive control by providing a robust, data-driven solution for managing systems with intricate spatio-temporal behaviors, offering significant potential for applications in automated process control and beyond.",1,120 | Cobb Galleria Centre,Advanced Data-Driven Methods in Smart Systems,0
1,5988.0,Task Dependency and Uncertainty-Aware Multi-Task Learning for Machining Process Monitoring,,"Machining process monitoring (MPM) is crucial for predictive maintenance and quality control within smart manufacturing. Additionally, monitoring multiple elements with limited sensors is essential for real-world applications, making it suitable for a multi-task learning (MTL) framework. MTL is an active research area that improves predictive performance across multiple outputs by extracting shared informative features for several tasks within a single model. However, the performance of MTL models is highly dependent on the weights of multiple loss functions, and determining the optimal weights is challenging due to the varying difficulty of each task. Moreover, conventional MTL models cannot consider the relationships between outputs, leading to inaccuracy and unrealistic predictions due to the loss of useful inter-task information. To address these issues, this study proposes a novel architecture incorporating task uncertainty for optimal weighting and task dependency to better capture inter-task relationships. Experiments are conducted on real-world side end-milling datasets using a computer numerical control (CNC) machine, utilizing vibration data alone to predict cutting forces, tool wear, and surface roughness. The proposed method enables accurate and efficient real-time quality control in smart manufacturing.",2,120 | Cobb Galleria Centre,Advanced Data-Driven Methods in Smart Systems,0
2,6233.0,Probabilistic Adaptive Graph Transformer for Uncertainty-Aware Multivariate Time Series Anomaly Detection,Academician,"Multivariate time series anomaly detection (MTAD) is important for ensuring reliable operation and improving service quality in industrial systems. Forecasting-based methods have been a primary approach for MTAD, detecting anomalies by assuming that anomalous data points produce higher forecasting errors than normal ones. However, existing forecasting-based MTAD methods often have limited anomaly detection performance because they generally overlook forecast uncertainty, leading to overconfident detections. In this study, we propose a probabilistic forecasting-based MTAD method that evaluates forecasting errors by comparing observed values with their predictive distributions, reflecting forecast uncertainty, and uses these errors as anomaly scores. Our approach determines the predictive distribution using a deep state-space model, capturing stochasticity in multivariate time series through nonlinear stochastic state transitions. Additionally, we integrate an adaptive graph transformer to capture complex intra- and inter-variable dependencies through a multi-head self-attention mechanism and graph convolutions with a learnable graph, respectively. Experiments demonstrate that our method outperforms current state-of-the-art techniques.",3,120 | Cobb Galleria Centre,Advanced Data-Driven Methods in Smart Systems,0
3,8857.0,ADs: Active Data-selection for Data Quality Assurance in Data-sharing over Advanced Manufacturing Systems,Academician,"Machine learning (ML) methods are widely used in manufacturing applications, which usually require a large amount of training data. However, data collection needs extensive costs and time investments in the manufacturing system, and data scarcity commonly exists. With the development of the industrial internet of things (IIoT), data-sharing is widely enabled among multiple machines with similar functionality to augment the dataset for building ML models. Despite the machines being designed similarly, the distribution mismatch inevitably exists in their data. However, the effective application of ML methods is built upon the assumption that the training and testing data are sampled from the same distribution. Thus, an intelligent data-sharing framework is needed to ensure the quality of the shared data such that only beneficial information is shared to improve the performance of ML methods. In this work, we propose an Active Data-selection (ADs) framework to ensure the quality of the shared data among multiple machines. It is designed as a self-supervised learning framework by integrating the architecture of contrastive learning (CL) and active learning (AL). A novel acquisition function is developed for active learning by integrating the information measure for benefiting down-stream tasks and the similarity score for data quality assurance. To validate the ADs framework, we did simulation study and case study using collected real-world in-situ monitoring data. With a high-quality dataset queried by our proposed framework, the proposed ADs consistently outperforms the classical AL method and is comparable to or even better than the ML trained on fully labeled data.",4,120 | Cobb Galleria Centre,Advanced Data-Driven Methods in Smart Systems,0
4,6264.0,RDDPM: Robust Denoising Diffusion Probabilistic Models for Unsupervised Anomaly Detection and Segmentation in Contaminated Data,Academician,"Anomaly detection in noisy images is essential for quality control in industries like steel, composites, and textiles. Traditional unsupervised anomaly segmentation models, including Robust Principal Component Analysis and Smooth Sparse Decomposition, rely on restrictive data assumptions including anomaly sparsity and a low-rank structure in the normal background. On the other hand, while diffusion-based anomaly detection methods are effective in handling non-linear patterns and non-sparse anomalies, they require large amounts of normal images for training, limiting their applicability in contaminated data. In this paper, we propose a novel method called Robust Diffusion, which leverages robust regression to train a diffusion model without uncontaminated data. Through simulations and a real-world case study on solar cells, we demonstrate that our approach outperforms both traditional statistical models and existing diffusion-based methods in detection accuracy. We also introduce a benchmark synthetic dataset, the RDDPM dataset , containing 100,000 normal and 50,000 anomalous samples with complex patterns and diverse anomaly types under varying lighting intensities. This dataset is designed to rigorously evaluate models for unsupervised anomaly segmentation in contaminated data with complex patterns.",1,120 | Cobb Galleria Centre,Advances in Quality Control with Data Driven Approaches,1
5,6682.0,Quality Control and Defect Detection for WAAM Production Using CloudCompare,,"Wire Arc Additive Manufacturing (WAAM) is a technique used for 3D printing or repairing metal parts by progressively layering metal until the desired 3D structure is fully formed. This research focuses on developing real-time, in-process monitoring and quality control techniques to evaluate multiple quality indicators continuously throughout the WAAM process. It further explores methods for detecting and classifying defects, such as porosity, cracks, lack of fusion, and geometric deviations, that may occur during WAAM. To enhance defect prediction and process optimization, predictive models are generated using machine learning algorithms or physics-based simulations. These models aim to forecast potential defects based on variations in process parameters, allowing for proactive adjustments. 3D scanning is employed to capture the physical form of manufactured parts as point cloud data, which is processed on a computer using Visual Studio code. A complete 360° scan takes approximately two minutes, with the data processing requiring only about five seconds. The resulting point cloud data is visualized in CloudCompare, a specialized software for processing 3D point cloud data. Within CloudCompare, geometrical attributes such as curvature and volume density are calculated and represented as distribution curves, aiding in clarity and precision during analysis. By examining these visualized parts, specific defects can be identified and categorized based on distinctive characteristics. Furthermore, Gaussian distribution curves are utilized to calculate the percentage of defective areas, offering quantitative insights for effective quality control and helping ensure that the final product meets desired specifications.",2,120 | Cobb Galleria Centre,Advances in Quality Control with Data Driven Approaches,1
6,8888.0,Deep Subspace Learning for Surface Anomaly Classification Based on 3D Point Cloud Data,Academician,"Surface anomaly classification is critical for manufacturing system fault diagnosis and quality control. However, the following challenges always hinder accurate anomaly classification in practice: (1) Anomaly patterns exhibit intra-class variation and inter-class similarity, presenting challenges in the accurate classification of each sample. (2) Despite the predefined classes, unseen types of anomalies can occur during production, which also requires precise detection. (3) Anomalous data is rare in manufacturing processes, leading to limited data for model training. This paper proposes a novel deep subspace learning-based classification model to address these challenges. Specifically, we utilize a lightweight encoder to extract the latent representations with a novel subspace classifier for subsequent discrimination. To accomplish this objective, we develop a novel loss function designed to encourage representations of the same class to be in the same subspace, while simultaneously enhancing the distinctiveness of subspaces associated with different classes. Extensive numerical experiments demonstrate our method achieves better anomaly classification results than benchmark methods and can effectively identify unknown anomalies.",3,120 | Cobb Galleria Centre,Advances in Quality Control with Data Driven Approaches,1
7,5674.0,Federated Dynamic Bayesian Network Structure Learning with Continuous Optimization,,"Traditionally, learning the structure of a Dynamic Bayesian network has been centralized, with all data pooled in one location. However, in real-world scenarios, data is often dispersed among multiple parties (e.g., companies, devices) that aim to collaboratively learn a Dynamic Bayesian network while preserving their data privacy and security. In this study, we introduce a federated learning approach for estimating the structure of a Dynamic Bayesian network from data distributed horizontally across different parties. We propose a distributed structure learning method that leverages continuous optimization so that only model parameters are exchanged during optimization. Experimental results on synthetic and real datasets reveal that our method outperforms other techniques, particularly when there are many clients with limited individual sample sizes.",1,120 | Cobb Galleria Centre,Bayesian and Markov Models for Engineering Systems,2
8,5712.0,Optimizing System Availability: Integrating Quantity-Based  Discounts and Delivery Lead Times for Component Procurement  using Markov process,Academician,"Purpose: The model allocates the system components orders to the suppliers to minimize the parts purchase costs, the system construction delay penalties, and maximize the system availability during its operations. It considers the quantity-based discount and variation of delivery lead time by ordering similar components which means providing components from the same supplier. The model also reflects the prerequisite relationships between construction activities and calculates the delay penalty resulting from parts delivery lead time. Methodology: This research presents a mathematical model for selecting suppliers of components of an industrial series-parallel multi-state system. A nonlinear binary mathematical program selects system components using the Markov process results. It minimizes the total system construction phase costs, including the components' price, the system construction delay penalty, and the system exploitation phase costs, including the system shutdown and working at half capacity. Findings: The model allocates optimal orders for the components of a typical industrial system, consisting of four elements. The main industrial system analyzed is the feedwater system (FWS) of heat-recovery steam-generator boilers used in combined cycle power plants in Iran. This system includes 22 components; however, due to the complexities of the Markov model, they were grouped into four subsystems. The proposed approach combines nonlinear binary programming with Markov process results to optimize life cycle parameters of the system, including construction costs and operational availability. Originality: By integrating Markov chain results into binary nonlinear mathematical programming, this study aims to balance the objectives of the construction and operational phases of an industrial unit.",2,120 | Cobb Galleria Centre,Bayesian and Markov Models for Engineering Systems,2
9,5744.0,Availability-Based Design Optimization Using  Markov Chain and a Fuzzy Goal Programming  Approach,Academician,"Designing system configurations is essential for ensuring the safety and efficiency, particularly in strategic sectors such as aerospace, and energy. Optimizing system configurations involves maximizing system availability while minimizing overall costs. This study focuses on the development of a feed water system (FWS) for heat-recovery steam-generator boilers used in combined cycle power plants in Iran. The process includes sourcing components from various suppliers, each offering different part characteristics and pricing, followed by installation at the power plant and subsequent utilization. The project is constrained by a deadline for installation and construction, with penalties incurred for project completion beyond this timeframe. This paper introduces a novel binary nonlinear fuzzy goal programming (FGP) model designed for the selection of part suppliers for multistate parallel-series industrial systems, considering both availability and manufacturing costs. The model incorporates factors such as quantity-based discounts, component purchase costs, and penalties for delays in system construction. Additionally, a fuzzy target programming approach is used to minimize deviations from the target expense goals. A system reliability block diagram is employed to represent the system's operational states—whether it operates at full capacity, partial capacity, or is non-operational. The Markov chain model is utilized to depict the sequence of possible states, where each state's probability is dependent on the reliability of the system components. The model also accounts for the impact of ordering multiple parts from the same supplier, which influences both economies of scale and delivery lead times. Results demonstrate the practical applicability of this approach in optimizing system reliability.",3,120 | Cobb Galleria Centre,Bayesian and Markov Models for Engineering Systems,2
10,5807.0,Explainable Federated Bayesian Causal Inference and Its Applications in Advanced Manufacturing,,"Causal inference has recently gained notable attention across various fields like biology, healthcare, and environmental science, especially within explainable artificial intelligence (xAI) systems, for uncovering the causal relationships among multiple variables and outcomes. Yet, it has not been fully recognized and deployed in the manufacturing systems. In this paper, we propose an explainable, scalable, and flexible federated Bayesian (xFB) learning framework to explore causality through the global treatment effect for manufacturing system. By leveraging federated Bayesian learning, we efficiently sample global parameters to derive the propensity score for each client. We then personalize the global parameter and estimate the treatment effect using propensity score matching (PSM). Trough simulations with various datasets and a real-world Electrohydrodynamic (EHD) printing data, we demonstrate that our method outperforms basic Bayesian causal inference and several state-of-the-art federated learning benchmarks.",4,120 | Cobb Galleria Centre,Bayesian and Markov Models for Engineering Systems,2
11,5814.0,Geometry-aware Active Learning of Spatiotemporal Dynamic Systems,Academician,"The rapid developments in advanced sensing and imaging have significantly enhanced information visibility, opening opportunities for predictive modeling of complex dynamic systems. However, sensing signals acquired from such complex systems are often distributed across a 3D geometry and rapidly evolving over time, posing significant challenges in spatiotemporal predictive modeling. This paper proposes a geometry-aware active learning framework for modeling spatiotemporal dynamic systems. Specifically, we propose to effectively integrate the temporal correlations and geometric manifold features via a spatiotemporal Gaussian Process, enabling reliable prediction of high-dimensional dynamic behaviors. Additionally, we develop an adaptive active learning strategy to strategically identify informative spatial locations for data collection and further maximize the prediction accuracy. This strategy achieves the adaptive trade-off between the prediction uncertainty and geodesic-distance-based space-filling design. We implement the proposed framework to model the spatiotemporal electrodynamics in a 3D heart geometry. Numerical experiments demonstrate that our framework outperforms traditional approaches lacking the mechanism of geometric information incorporation or effective data collection.",1,120 | Cobb Galleria Centre,Bayesian and Markov Models for Engineering Systems II,3
12,6532.0,Accelerated Discovery of Solvent Ratios for Aerosol Jet Printing Inks Using Temporal Multi-Objective Bayesian Optimization,,"Aerosol-Jet Printing (AJP), a direct-write process, deposits functional materials onto substrates to create electrically conductive features for flexible electronics and printed circuits. However, a critical challenge in AJP lies in the complex temporal dynamics of solvent evaporation during printing, where uncontrolled evaporation affects structural integrity and functional performance. Classical approaches to optimizing solvent compositions rely on time-consuming trial-and-error experiments, significantly diminishing AJP's rapid manufacturing advantage. While researchers can specify ideal mass retention targets across time points based on their understanding of print quality requirements, achieving these targets remains challenging due to the unknown relationship between multiple solvents and temperature. To overcome the abovementioned difficulties, we introduce a closed-loop data-driven optimization framework, that incorporates domain knowledge of desired mass retention and operates on a six-dimensional design space. Our study comprises five solvent components (xylenes, terpineol, isopropyl alcohol, and toluene, d-limonene) and temperature. The objective is to minimize the discrepancy between predicted and expert-defined target mass retention values across five-time points. This methodology integrates multi-output Gaussian Process (GP) modeling with an asymmetric Bayesian discrepancy covariance structure to capture temporal dependencies in solvent evaporation dynamics. Additionally, we employ Expected max-min improvement as an acquisition function to design experiments sequentially, efficiently exploring the different solvent proportions while minimizing the number of required experiments. Our approach advances traditional optimization procedures by integrating uncertainty quantification with efficient design sampling. Furthermore, we validate our framework's effectiveness by achieving optimal solvent proportions that match desired mass fraction targets through real-world experiments, successfully bridging domain expertise and practical implementation.",2,120 | Cobb Galleria Centre,Bayesian and Markov Models for Engineering Systems II,3
13,8932.0,Partially Observable Markov Decision Processes (POMDP) Framework for Decision-Making Under Uncertainty Using Current Detection Based Monitoring System,Academician,"In Electrohydrodynamic printing (EHD), maintaining consistent printing quality and specifications remains challenging due to the complex relationships between operational parameters. This study demonstrates using the Partially Observable Markov Decision Processes (POMDP) framework to create an EHD printing closed-loop control system designed to enhance the decision-making process under uncertainty. The dynamic nature of EHD printing facilitates modeling the process as a sequential problem where the actual printed pattern specification is not directly observable in real-time, however, operational parameters will be adjusted according to observational feedback from a current detection-based monitor system. The POMDP framework defines states as the line width segment of the printed pattern. These states are developed by unsupervised machine-learning algorithms that explore the printed lines and discover inclusive line width segments. These segments describe the state space. Note that image-processing techniques are used to engineer and extract features from final output microscope images which feed into machine learning algorithm. Actions modify key operational parameters such as the applied voltage and standoff distance, while observations are the signal received from the monitoring system after being processed. Different algorithms are applied to find the optimal solution for the POMDP problem in EHD printing settings. The results evaluated the performance and applicability of multiple solution algorithms including exact approaches, approximation approaches, and other planning-based approaches. This work offers a reliable and robust quality assurance framework for EHD settings that would facilitate automation, handle process variability, and improve final quality.",3,120 | Cobb Galleria Centre,Bayesian and Markov Models for Engineering Systems II,3
14,8540.0,Bayesian Optimization of System Designs using Hybrid Physical and Digital Experiments,Academician,"We consider the problem of learning and optimizing the performance of a system by sequentially executing a limited number of physical and digital experiments on system designs within a design space. In this problem, physical experiments are assumed to be unbiased but costly. By comparison, digital experiments (e.g., using a simulation model) are assumed to be less costly, but their response may be biased due to the simulation model’s inability to fully represent the physical system. This problem has numerous applications, including optimizing the design of an engineered system whose performance (e.g., mechanical properties and reliability) is affected by various design variables as well as external and internal stressors. Without digital experiments, the problem of sequentially planning physical experiments amounts to optimizing a noisy and expensive-to-evaluate black-box function, which can be solved using Bayesian Optimization (BO). Our research extends BO by planning digital experiments between subsequent physical experiments for the purpose of (i) calibrating the simulation model and (ii) exploring solutions likely to generate a desirable physical experiment response. We evaluate the proposed methodology, referred to as “Simulation-Enhanced Bayesian Optimization” (SEBO), using a number of one- and two-dimensional benchmark functions. A notional bias function is utilized to characterize the simulation model’s bias as a function of the design variables and simulation model parameters. To demonstrate the advantages of the proposed methodology, we compare SEBO and BO with respect to the number of physical and digital experiments required to exceed a desired objective function value.",4,120 | Cobb Galleria Centre,Bayesian and Markov Models for Engineering Systems II,3
15,5756.0,Joint Model for Multi-Type Failure Event Prediction from Multi-Sensor Time Series and Survival Data,Academician,"Modern industrial systems often have multiple components subject to multiple types of failure, and their conditions are monitored by multiple sensors, generating multiple time-series signals. Additionally, time-to-failure data are commonly available. Predicting multiple components' remaining useful life (RUL) is crucial by leveraging multi-sensor time series and multi-type failure event data. In most existing models, failure mode identification and RUL prediction are performed independently, ignoring the inherent relationship between these two tasks. Some models integrate multiple failure modes classification and event prediction using black-box machine learning approaches, which lack statistical rigor and cannot characterize the inherent uncertainty in the system and data. This paper introduces a unified approach to jointly model the multi-sensor time-series data and RUL prediction concerning multiple failures. We integrate a Cox proportional hazards model with a Multi-output Convolutional Gaussian Process (MCGP) in a hierarchical Bayesian framework. We employ a Dirichlet prior to account for multiple failure modes to precisely capture distinct degradation paths specific to each failure mode and accurately predict the failures within the Cox proportional hazard model. Variational Bayes are used for inference, where we derive an Evidence Lower Bound (ELBO). Due to the unified framework that characterizes the associations between multiple sensor signals and multi-type failure events and their uncertainties, the proposed method outperforms existing methods.",1,120 | Cobb Galleria Centre,"Gaussian Process and its Advances in Modeling, Prognosis, and Optimization",4
16,6553.0,Constrained Bayesian Optimization under Bivariate Gaussian Process with Application to Curing Process Optimization,Academician,"Bayesian Optimization (BO), leveraging Gaussian Process (GP) models, has proven to be a powerful tool for minimizing expensive-to-evaluate objective functions by efficiently exploring the search space. Extensions, such as Constrained Bayesian Optimization (cBO), have further enhanced BO’s utility in practical scenarios by focusing the search within feasible regions defined by constraint functions. However, a significant limitation in cBO arises from the standard assumption of independence between objective and constraint functions, which may not hold in real-world applications where these functions are often correlated. To address this, we introduce a Bi-Variable Gaussian Process (BiV-GP) model within a constrained BO framework that models objective and constraint functions as correlated outputs of multiple Gaussian Processes. By capturing these dependencies directly, the BiV-GP model aims to improve optimization performance, particularly for problems characterized by complex interdependencies. Nevertheless, our experimental results indicate that BiV-GP struggles to consistently outperform simpler models like Ordinary Kriging. This underperformance can be attributed to the model’s reliance on a separable covariance matrix for representing dependencies, which may inadequately capture the relationships between functions and interactions across multiple dimensions. Additionally, the complexity of the BiV-GP model introduces challenges related to parameter estimation and computational efficiency. We show case the proposed approach with a case study on curing process optimization.",2,120 | Cobb Galleria Centre,"Gaussian Process and its Advances in Modeling, Prognosis, and Optimization",4
17,7004.0,Gaussian Processes (GPs) for spatial-temporal power curve estimation,,"In this study, we address the challenge of modeling wind power curves by incorporating the effects of terrain. Unlike environmental variables such as wind speed and temperature, which vary both spatially and temporally, terrain features such as ruggedness, slope, and ridges vary between turbine locations but remain constant over time. To effectively handle these two modes of variation, we employ a Kronecker product Gaussian Process (GP) model. By jointly modeling data from multiple turbines, this approach integrates terrain information into the power curve estimation process, leading to more accurate, site-specific predictions. Moreover, it enhances the model’s generalizability to turbines located in different terrains, underscoring the value of incorporating spatially-varying terrain data in wind energy forecasting.",3,120 | Cobb Galleria Centre,"Gaussian Process and its Advances in Modeling, Prognosis, and Optimization",4
18,8605.0,Federated Automatic Latent Variable Selection in Multi-output Gaussian Processes,Academician,"This paper explores a federated learning approach that automatically selects the number of latent processes in multi-output Gaussian processes (MGPs). The MGP has seen great success as a transfer learning tool when data is generated from multiple sources/units/entities. A common approach in MGPs to transfer knowledge across units involves gathering all data from each unit to a central server and extracting common independent latent processes to express each unit as a linear combination of the shared latent patterns. However, this approach poses key challenges in (i) determining the adequate number of latent processes and (ii) relying on centralized learning which leads to potential privacy risks and significant computational burdens on the central server. To address these issues, we propose a hierarchical model that places spike-and-slab priors on the coefficients of each latent process. These priors help automatically select only needed latent processes by shrinking the coefficients of unnecessary ones to zero. To estimate the model while avoiding the drawbacks of centralized learning, we propose a variational inference-based approach, that formulates model inference as an optimization problem compatible with federated settings. We then design a federated learning algorithm that allows units to jointly select and infer the common latent processes without sharing their data. We also discuss an efficient learning approach for a new unit within our proposed federated framework. Simulation and case studies on Li-ion battery degradation and air temperature data demonstrate the advantageous features of our proposed approach.",4,120 | Cobb Galleria Centre,"Gaussian Process and its Advances in Modeling, Prognosis, and Optimization",4
19,5794.0,Numerical approach for modelling defect effects on exhaust gas temperature of gas turbine,Academician,"Ensuring reliable and efficient operation of gas turbines is paramount for minimizing downtime, reducing maintenance costs, and preventing catastrophic failures in power plants. This study models the effects of defects on exhaust gas temperature distributions in gas turbines operated by a Hong Kong power plant. While machine learning and data-driven methods are commonly leveraged for fault detection, their effectiveness is constrained by limited failure data, hindering accurate classification of fault types and locations. To address this scarcity, a physics-based model was developed to generate exhaust temperature distributions under different fault conditions. Specifically, the hot gas path—including the combustion and turbine sections—of the GE 9FA gas turbine was simulated and analyzed using multi-physics modelling and simulation. Geometrical dimensions were obtained through on-site measurements, ensuring that the simulation models closely represent operational turbines despite potential minor measurement uncertainties. Defects were introduced by modifying boundary conditions, such as altering the air-fuel ratio, and adjusting the geometry to simulate operational anomalies like leakage, material loss, or breakage. Multi-physics modelling and simulation including combustion, temperature, and fluid flow were conducted to capture the turbine's thermal behavior under these conditions. The patterns of resulting temperature distribution illustrate the influence of defects on exhaust gas temperatures and identify critical areas susceptible to failure. Furthermore, the defect-generated data were utilized to validate the accuracy and reliability of the physics-based model. This work provides a framework for defect analysis in gas turbines, compensating for the lack of empirical failure data and enhancing predictive maintenance strategies.",1,105 | Cobb Galleria Centre,Data Driven Analysis under Data Constraints,5
20,6258.0,Theoretical Analysis and Design of An Online Monitoring and Sampling Scheme Under Partial Observations,Academician,"Rapid advancements in sensor technology have enabled high-dimensional streaming data collection for real-time system monitoring across diverse applications. However, due to constraints such as limited sensors, transmission bandwidth, and data storage, only a subset of observations can be collected at each data acquisition time. Although adaptive sampling strategies have been proposed to be integrated into monitoring schemes for quick anomaly detection, the theoretical analysis of detection performance in practical settings has not been addressed. Few studies have provided upper bounds for the average detection time, but the bounds are either asymptotic or excessively large, failing to provide valuable guidance on parameter design for optimal detection performance in practical use. To fill this gap, we propose a holistic monitoring-based probabilistic sampling framework under partial observations and theoretically analyze the average detection time to inform optimal design. By leveraging the sampling probabilities of variables, which are explicit functions of monitoring statistics, we derive formulas for practical average detection time as functions of design parameters and system settings, such as the number of sampled variables, total variables in the system, and anomalous variables. These theoretical results enable the determination of parameter settings and number of sampled variables to achieve optimal detection performance while satisfying false alarm requirements and considering practical resource constraints such as sampling costs. Simulation studies across various parameters and system settings validate the effectiveness of the theoretical results, demonstrating significant practical value for optimal performance design.",2,105 | Cobb Galleria Centre,Data Driven Analysis under Data Constraints,5
21,6559.0,Robust Anomaly Detection under Data Contamination for Pixel-Level Defective Region Segmentation,Academician,"Industrial image-based anomaly detection plays a significant role in quality control, where pixel-level defective region segmentation is an important task to locate anomalies for various applications. Due to the data-abundance and label-scarcity nature of industrial products, the unsupervised learning approach has attracted great attention because such a method does not require a labeling process. However, it is often based on a normality assumption that all training data is normal, which can be broken in practice. Thus, a contaminated dataset, i.e., a mixture of normal and anomalous samples without label, should be considered. To deal with this challenge, we propose an unsupervised robust anomaly detection framework under data contamination for pixel-level defective region segmentation. Specifically, unlike matrix-decomposition-based methods that need certain explicit assumptions on the normal background, our proposed model, built on an autoencoder structure with a memory module, allows flexible patterns presented in the background and holds two mild assumptions: (i) across-sample low-rank, and (ii) sparse anomalies. Additionally, the proposed model will be trained and pixel-wisely locate anomalies by decomposing each input image into a normal background and an anomaly mask in a one-stop fashion. The effectiveness is demonstrated in the case study on a public anomaly detection dataset.",3,105 | Cobb Galleria Centre,Data Driven Analysis under Data Constraints,5
22,4945.0,Data-Driven Failure Prediction in Metro Trains: An ML-Based Approach to Predictive Maintenance,Practitioner,"This study aims to predict metro-train failures using machine learning (ML) models, specifically focusing on XGBoost and Random Forest. Data from the MetroPT dataset were labeled based on time intervals relative to failure: from the start of the dataset to 24 hours before failure, 24 to 2 hours before, 2 hours prior, and the failure event itself. Sensor data, including analog signals (pressure, temperature, current) and digital signals (control, discrete signals), as well as GPS data (latitude, longitude, speed), were utilized to train and evaluate the models. The XGBoost model outperformed Random Forest, achieving high accuracy and precision in failure prediction, particularly within the 12-to-2-hour window before failure. The models' performance was assessed through multi-class ROC curves, reflecting the true and false positive rates across the four time categories. A confusion matrix further illustrated the accuracy by comparing predicted and true labels. Feature importance analysis highlighted key variables like DV-pressure, Reservoirs, oil_compressor, TP3, and TP2, which had high predictive power for failures. This work supports predictive maintenance, enabling timely interventions to extend the remaining useful life (RUL) and conduct root cause analysis (RCA).",1,105 | Cobb Galleria Centre,Data-Driven Modeling and Monitoring for Smart Systems,6
23,6199.0,Online Structural Change-point Detection of High-dimensional Streaming Data via Sparse Spectral Graphical Models,Academician,"In complex systems, high-dimensional streaming data collected by sensors provides valuable insights into system states through the analysis of cross-correlation structures between entities. This paper introduces a novel online structural change-point detection methodology aimed at estimating the evolving cross-correlation structure of HD streaming data in real-time. Traditional methods for dynamic graphical modeling of system structures suffer from computational inefficiencies and tend to introduce fictitious dynamics when changes are sparse or absent. Existing approaches also face limitations in sequential change-point detection and real-time graphical representation. To address these challenges, our approach sequentially estimates the cross-correlation structure within sliding time windows by constructing sparse spectral graphical models that capture conditional dependencies in the frequency domain. The sparsity within each time window and the regularization between consecutive windows are achieved using the group LASSO penalty, and structural changes are monitored with an exponentially weighted moving average control chart, allowing for efficient and timely detection of state changes. The methodology is implemented using the Alternating Direction Method of Multipliers, facilitating real-time monitoring of system dynamics. The performance of the proposed methodology is demonstrated through simulations and case studies.",2,105 | Cobb Galleria Centre,Data-Driven Modeling and Monitoring for Smart Systems,6
24,6241.0,Exploration and Evaluation of Multiple Types of Regression for limited sample Probability of Detection,,"This paper provided statistical approaches to improve Probability of Detection (POD) analysis when sample sizes were limited. Due to the scarcity of experimental data, the empirical POD method could not meet the required set of NDE simulated flaws. And we also assumed that there was not enough well-known physics information to produce simulated data so traditional Model-Assisted POD (MAPOD) might not work. To address the above situation, we applied advanced statistical regression models and combined them with a physics-based model to reduce uncertainties caused by limited sample sizes. To estimate the parameters of the model, we used a range of techniques—such as robust regression, Bayesian methods, bootstrapping for limited-sample POD, and physics-based POD models—to analyze these limited samples. To this end, we conducted extensive simulation studies across diverse scenarios, enabling us to compare the performance of MAPOD methods against a presumed ground truth. The POD estimates derived from a large dataset served as the ground truth. We then randomly sampled smaller subsets from this dataset to simulate limited-sample conditions. By comparing the MAPOD estimates with the ground truth, we evaluated the impact of different methods under various conditions, including the presence of outliers and nonlinear relationships in the data. Our findings offer valuable insights into how different MAPOD methods influence POD analysis in limited-sample studies across multiple scenarios. The use of such methods can importantly improve estimation precision. This research provides practitioners with a practical framework for selecting appropriate MAPOD methods tailored to specific conditions.",3,105 | Cobb Galleria Centre,Data-Driven Modeling and Monitoring for Smart Systems,6
25,8556.0,Point-ITR: Interpretable Task-Oriented Importance Sampling for Large-Scale 3D Point Cloud Processing,Practitioner,"The increasing adoption of advanced three-dimensional (3D) scanning technologies, such as Light Detection and Ranging (LiDAR) and structured light systems, has made large-scale point clouds containing millions of 3D measurement points standard in applications like autonomous driving and manufacturing. However, processing immense amounts of 3D data imposes significant computational loads, often resulting in discarded critical information and suboptimal outcomes for downstream tasks. This paper introduces Point-ITR, a task-oriented sampling method that selectively retains the most informative points within large-scale point clouds and offers interpretability. Specifically, we propose a gradient-based importance sampling framework for intra-sample selection (selecting points within a 3D point cloud) and a feature-based weighting scheme for inter-sample selection (selecting among different 3D point cloud samples). Additionally, we introduce an iterative random sampling (ItrRS) module for preprocessing and an Offset Residual Block that utilizes a reference design model to accelerate training and testing, which allows a simple fully connected network to process large-scale point clouds. Our approach not only improves prediction accuracy across downstream tasks but also ensures that the rich detail captured is fully leveraged for interpretation, offering a more effective and efficient solution. We validate our methodology through simulation studies and real-world case applications in additive manufacturing, demonstrating its robustness and practical applicability.",1,105 | Cobb Galleria Centre,Defect Detection in 3D Point Cloud Inspection,7
26,8875.0,Uni-3DAD: GAN-Inversion Aided Universal 3D Anomaly Detection on Model-free Products,,"Anomaly detection is a long-standing challenge in manufacturing systems, aiming to locate surface defects and improve product quality. Traditionally, anomaly detection has relied on human inspectors or image-based methods. However, 3D point clouds have gained attention due to their robustness to environmental factors and their ability to represent geometric data. Existing 3D anomaly detection methods generally fall into two categories. One compares scanned 3D point clouds with design files, assuming these files are always available. However, such assumptions are often violated in many real-world applications where model-free products exist, such as fresh produce (i.e., ``Cookie"", ``Bagel"", ``Potato"", etc.), dentures, bone, etc. The other category compares patches of scanned 3D point clouds with a library of normal patches named memory bank. However, those methods usually fail to detect incomplete shapes, which is a fairly common defect type (i.e., missing pieces of different products). The main challenge is that missing areas in 3D point clouds represent the absence of scanned points. This makes it infeasible to compare the missing region with existing point cloud patches in the memory bank. To address these two challenges, we proposed a 3D anomaly detection framework capable of identifying all types of defects on model-free products. Our method integrates two detection modules: a feature-based detection module and a reconstruction-based detection module. Feature-based detection covers geometric defects, such as dents, holes, and cracks, while the reconstruction-based method detects missing regions. Additionally, we employ a One-class Support Vector Machine (OCSVM) to fuse the detection results from both modules.",2,105 | Cobb Galleria Centre,Defect Detection in 3D Point Cloud Inspection,7
27,6079.0,Automated Wood Chip Size Estimation with Deep Learning-Based Object Detection,Academician,"Accurate and efficient wood chip dimension estimation is critical in industries, where chip size affects energy output and process optimization. Traditional manual methods for measuring chip geometric properties are labor-intensive, time-consuming, and often unrepresentative of the full dataset. Leveraging deep learning-based object detection techniques, this study investigates the use of various YOLO-based models for automating the measurement process from annotated images. We annotated a large-scale image dataset comprising 300 images and 7,100 instances across multiple sources. Our study compared different object detection models with 5-fold cross-validation to detect wood chips and predict their dimensions. In addition, predicted and actual chip dimensions were compared by generating histograms for each fold. The discard threshold of 20% was designed to reduce detection errors associated with proximity to vision boundaries. YOLOv9s outperformed all other models, achieving a mAP50 of 99.5%, with precision and recall values of 99% across all folds. The generated histograms demonstrated a strong alignment between predicted and actual wood chip dimensions. By using cross-validation and optimized model architectures, our approach marks a significant step forward from traditional methods, offering an efficient and scalable solution for real-time use in industrial settings.",1,105 | Cobb Galleria Centre,Emerging Methods in Vision-Based Decision Support,8
28,6625.0,Person Re-identification in Video-based Trajectory Mapping during Simulated Active Shooting Scenario,Academician,"In high-risk environments such as active shooting scenarios, protecting soft targets and ensuring safety is crucial. To prevent such events, accurate tracking and identification of individuals is critical for analyzing and understanding the behavioral factors of the occupants. This is also fundamental for real-time decision-making and post-event analysis. However, person re-identification (ReID) plays a vital role in such multi-person tracking scenarios, where individuals must be correctly identified across multiple frames and camera views despite changes in appearance, pose, or occlusion. Without consistent person ReID, it is challenging to construct accurate trajectories. To this end, this study presents a comparative evaluation of the multi-object tracking (MOT) methods, namely, Simple Online and Real-time Tracking (SORT), DeepSORT, and ByteTrack across the MOTChallenge benchmark dataset and our custom dataset. The tracking methods were integrated with a deep learning-based object detection model, YOLOv8, for real-time detection and continuous tracking. By utilizing video sequences, we mapped the trajectories of individuals, ensuring consistent re-identification even under conditions of crowd occlusion, variable lighting, and high movement. The key focus of this study is to evaluate the accuracy of trajectory mapping in complex environments, with attention to occlusion handling and identity switches. The results highlight the effectiveness of each tracking method, demonstrating the potential for integration with advanced object detection models in multi-person tracking tasks in emergency scenarios.",2,105 | Cobb Galleria Centre,Emerging Methods in Vision-Based Decision Support,8
29,7046.0,Generating High-Resolution Wood Chip Images with Diffusion Transformers for Enhanced Moisture Content Prediction,Academician,"Wood chips are a leading renewable energy source and a vital raw material in pelleting mills, bio-refineries, and paper mills. Their quality, especially for fuel and pulp applications, largely depends on accurately determining moisture content. With precise moisture content level measurements, manufacturers can fine-tune their processes to optimize quality and reduce waste, resulting in more efficient and sustainable operations. However, current data-driven methods used to measure the moisture content of wood chips need a large amount of data to train the models to eventually achieve state-of-the-art results. The major drawback is that collecting such a large dataset is time-consuming and the current approaches can take several days to do so. In this study, we propose a diffusion transformer, an advanced generative model that would be used to generate a variety of wood chip images with different moisture content levels. The proposed diffusion transformer demonstrates strong preliminary results, achieving high-quality generated images. The generated wood chip images serve as a crucial resource to train and enhance the robustness of computer vision models, supplementing existing smaller datasets for real-time industrial applications.",3,105 | Cobb Galleria Centre,Emerging Methods in Vision-Based Decision Support,8
30,6884.0,Reduce Label Waste Percentages,Practitioner,"A Label Graphics Company serves a varied market with self-adhesive labels. To investigate the problem of DMAIC-based projects to reduce label waste percentages, targeting paper waste levels below the 10% threshold set by quality management standards. During the Define phase, the quality department selected two presses—Allied Gear 2 and 3—for detailed evaluation. It examined several critical factors to shape our strategic approach, including employee process flowcharts, CTQ (Critical to Quality) analysis, and SIPOC mapping. By analyzing job ticket data spanning 2023, we evaluated the number of clicks per order for machines producing paper labels. The study, reinforced by visual graphs, capability analysis, and hypothesis testing, revealed that Allied Gear 2 consistently exceeded the 10% waste threshold, underscoring the necessity for corrective actions. These insights pointed to targeted interventions to mitigate waste on Allied Gear 2, enhance overall operational efficiency, and align production processes with quality objectives.",4,105 | Cobb Galleria Centre,Emerging Methods in Vision-Based Decision Support,8
31,5210.0,Reinforcement Learning for Fuselage Shape Control during Aircraft Assembly,,"Critical safety requirements necessitate ultra-high precision quality control during the assembly of large aerospace components to reduce the mismatch between parts to be joined. Traditional methods use heuristic shape adjustment or surrogate model-based control. These methods are limited by reliance on accurate model learning and inadequate robustness to varying initial assembly conditions. To address these limitations, this paper proposes a model-free reinforcement learning approach for adaptive fuselage shape control during aircraft assembly. The trained reinforcement learning agent directly adjusts the aircraft components in response to their part variations and enables an autonomous system (like AlphaGo) to learn the optimal shape control policy. Specifically, the reinforcement learning environment is built on the finite element simulator. A reward function is developed to capture the optimization objective and introduces a scheme to enforce the original constraints. The proximal policy optimization algorithm is modified to speed up the learning progress and achieve better final performance. In the case study, the root-mean-square gap between components is reduced by 98.4% on average compared with their initial shape mismatch. Our proposed method outperforms the benchmark methods with smaller final shape errors, smaller maximum forces, and lower variations across different test samples.",1,116 | Cobb Galleria Centre,Advances in Reinforcement Learning for Complex Systems,9
32,5763.0,DynaMark-RL: A Reinforcement Learning Framework for Dynamic Watermarking in Industrial Machine Tool Controllers,Academician,"With worldwide growth in industrial automation and digitalization of the manufacturing environment, the manufacturing plants are no longer isolated from cyberattacks. Consequently, the industrial machine tool controllers (MTCs) are increasingly integrated with cloud applications, external sensors, and other data sources. While this transformation holds immense potential for efficiency and innovation, it simultaneously presents formidable cybersecurity challenges. The interconnectedness of MTCs with external networks makes them more vulnerable and primary attack targets for cyberattacks. This paper addresses the problem of secure MTCs by designing dynamic physical watermark signals using reinforcement learning (RL). We study the detection of replay attacks on MTCs, and introduce a watermarking RL framework for joint optimization of detection of replay attacks and control performance.",2,116 | Cobb Galleria Centre,Advances in Reinforcement Learning for Complex Systems,9
33,6646.0,Interactive Resource Planning and Change Detection via Multi-agent Reinforcement Learning,Academician,"Rapid advancements in sensor technology generate vast amounts of data, posing challenges for statistical process control (SPC). Resource constraints, such as limited bandwidth and processing power, restrict real-time data observability, necessitating dynamic decisions on which data streams to monitor for prompt anomaly detection. Given the high costs of monitoring resources, it is essential to consider the operational expenses associated with online monitoring. To fill this research gap, we reformulate the problem of change detection and resource allocation as a leader-follower decision-making process, utilizing bilevel programming to reconcile the conflicting objectives of cost control and anomaly detection. Then, we present a generic framework of policy learning based on deep reinforcement learning (DRL) to optimize decision-makers' policies. To enhance scalability and mitigate the curse of dimensionality, we decompose the complex change detection task and employ multi-agent reinforcement learning for improved collaboration. Unlike existing methods, our approach prioritizes long-term returns over myopic outcomes, effectively addressing scalability issues in DRL. Moreover, we are the first to incorporate operational costs into change detection. Simulation studies and a real case study validate the efficacy and applicability of the proposed method.",3,116 | Cobb Galleria Centre,Advances in Reinforcement Learning for Complex Systems,9
34,5060.0,3D Propagation Modeling in 4D Printing,Academician,"Additive manufacturing (AM), commonly known as 3D printing, has seen significant advancements, particularly in the development of stimuli-responsive, 3D printable, and programmable materials. These advancements have given rise to 4D printing, a technique that combines AM with intelligent materials, adding dynamic functionality as a fourth dimension. Among these stimuli-responsive materials, shape memory polymers have gained attention due to their important applications in stress-absorbing components. However, the precise 3D shape morphing of 4D printed products is influenced by both 3D printing conditions and stimuli activation, making it challenging to achieve accurate control. This talk will present recent advances in modeling the dynamic 3D propagation of 4D printing processes, enabling transformative applications and a deeper understanding of this emerging technology.",1,116 | Cobb Galleria Centre,High-dimension and Large Scale Data Analysis for Manufacturing Systems,10
35,6659.0,Detecting Changes in High Dimensional Model Relationship Profiles,Practitioner,"Advancements in sensor and data collecting technology have resulted in data sets that are both high dimensional and heterogeneous, with forms such as scalars, waveform signals, images, videos, and 3D point clouds. These advancements have created the need to construct and monitor statistical models that are able to model the relationship between heterogeneous and high dimensional inputs and a high dimensional output. This project (1) discusses how to model the relationship between such inputs and output using Multiple Tensor-on-Tensor Regression (MTOT) and (2) presents different approaches for how to monitor this model relationship to detect potential changes over time. Monitoring the model relationship can be done either by monitoring the core tensor of the MTOT model and/or by monitoring the residuals of new data samples. This methodology is validated through several case studies including monitoring overlay error from semiconductor wafer shape data.",2,116 | Cobb Galleria Centre,High-dimension and Large Scale Data Analysis for Manufacturing Systems,10
36,6922.0,Timeseries Foundation Model for Anomaly Detection in Manufacturing Sensor Data,Academician,"This study aims to develop a foundation model for anomaly detection in industrial data. Foundation models, trained on extensive datasets, have demonstrated remarkable success in fields such as natural language processing and image analysis, and there is growing interest in applying them to time-series data. However, existing time-series foundation models primarily focus on prediction, with no models specifically tailored for anomaly detection. Anomaly detection, or identifying deviations from normal behavior, is crucial in industrial settings, especially in high-precision areas like semiconductor manufacturing. Early identification of potential issues in manufacturing processes can enhance efficiency and maintain quality standards. This study proposes a time-series foundation model specifically designed for anomaly detection in large-scale industrial datasets. Key challenges in adapting foundation models for anomaly detection include overgeneralization due to high predictive accuracy and overstationarization resulting from essential normalization layers. Ideally, anomaly detection models should yield high reconstruction errors for abnormal data. However, time-series foundation models developed for prediction often produce low reconstruction errors even for abnormal data, particularly in simpler datasets. To address these challenges, our model incorporates both reconstruction error and embedding vector-based anomaly scores, leveraging the effectiveness of embedding-based anomaly detection in simple time-series data. We also calculate an indicator for simple time-series data to balance the weights between reconstruction error and embedding vector-based scores. Finally, we utilize quality-related statistics and time-series length differences to help mitigate overstationarization. The model is validated on real industrial data and compared qualitatively to traditional quality monitoring methods, such as control charts, using simulated data.",3,116 | Cobb Galleria Centre,High-dimension and Large Scale Data Analysis for Manufacturing Systems,10
37,5132.0,Wind Turbine Failure Prediction with Generative Language Model,Practitioner,"Reliable failure detection systems are important for the operation of wind turbines. Since pre-trained generative language models have been proven capable of pattern induction, real-world event reasoning, and performing time series analysis while generating human-readable explanations, they provide new opportunities for analyzing operational data to detect imminent failure for wind turbines. In this work, we explore the potential of leveraging generative language models to predict wind turbine failures. We first convert SCADA (Supervisory Control and Data Acquisition) wind turbine data into question-answer prompts. By fine-tuning the language model with those prompts, we apply LoRA (low-rank adaptation) to lower the computational cost while adding trainable prompt embeddings to improve the model performance. We also use attention-head visualizations for improved interpretable failure detection results. We tested the method using GPT-2 with EDP (Electricidade de Portugal) wind farm SCADA dataset. The results show that the performance of generative language models is comparable to those of the state-of-the-art wind turbine failure detection methods.",1,120 | Cobb Galleria Centre,Statistical Modeling and Optimization in Quality Analytics,11
38,5618.0,An Adaptive Approach for Online Monitoring of_x000B_Large Scale Data Streams,Academician,"We propose an adaptive top-r method to monitor large-scale data streams where the change may affect a set of unknown data streams at some unknown time. Motivated by parallel and distributed computing, we propose to develop global monitoring schemes by parallel running local detection procedures and then use the Benjamin–Hochberg false discovery rate control procedure to estimate the number of changed data streams adaptively. Our approach is illustrated in two concrete examples: one is a homogeneous case when all data streams are independent and identically distributed with the same known pre-change and post-change distributions. The other is when all data are normally distributed, and the mean shifts are unknown and can be positive or negative. Theoretically, we show that when the pre-change and post-change distributions are completely specified, our proposed method can estimate the number of changed data streams for both the pre-change and post-change status. Moreover, we perform simulations and two case studies to show its detection efficiency.",2,120 | Cobb Galleria Centre,Statistical Modeling and Optimization in Quality Analytics,11
39,5628.0,Machine Learning for Predictive Maintenance in  Semiconductor SEM Equipment,Practitioner,"Predictive maintenance has emerged as a pivotal strategy in the semiconductor industry, leveraging machine learning algorithms to anticipate equipment failures and optimize maintenance schedules. Semiconductor manufacturing facilities rely on complex machinery and equipment to produce integrated circuits and other semiconductor devices. Any unexpected downtime or equipment failure can lead to significant production losses and costly repairs. Predictive maintenance stands out from other maintenance categories as it precludes any failures to equipment components. By implementing predictive maintenance solutions powered by machine learning, semiconductor companies can monitor the health of their equipment in real-time, detect potential issues before they escalate into failures, and schedule maintenance proactively to minimize downtime. In this paper, the application of predictive maintenance techniques within semiconductor manufacturing is explored, focusing on the development and deployment of machine learning models to predict equipment failures based on sensor data collected from scanning electron microscope (SEM) metrology equipment. The approach adopted in this study involves simulating sensor output data from wafer runs, training machine learning models on the generated dataset, and deploying the models as real-time inference endpoints. By integrating Azure Machine Learning Designer and Power BI, the accuracy and performance of the predictive maintenance model is assessed. Results of the study demonstrate the efficacy of machine learning in accurately forecasting equipment failures, thereby minimizing downtime and optimizing operational efficiency in semiconductor manufacturing facilities. Overall, this study underscores the transformative potential of predictive maintenance in semiconductor manufacturing, paving the way for enhanced operational resilience and sustained competitiveness in the industry.",3,120 | Cobb Galleria Centre,Statistical Modeling and Optimization in Quality Analytics,11
40,6638.0,Temporal Dynamics of Deposition Quality in Aerosol Jet Printing using Time-Varying Effects Models,Academician,"Aerosol Jet Printing (AJP) is a contactless additive manufacturing process that prints functional inks on substrates to create conductive features. These inks consist of conductive nanoparticles, binding agents, surfactants, and solvents. A key challenge in AJP is controlling process drift, as ink solvents evaporate continuously during aerosolization, transport, and deposition. This solvent loss causes operating pressure fluctuations due to nanoparticle buildup on focusing lenses and nozzle orifices, especially in systems with multi-stage lenses that add further constrictions to the aerosol path. Previous studies have monitored changes to print quality at fixed intervals, capturing only the final effects of process drift. This approach lacks continuous monitoring of the process, which is needed to predict when and how print quality will deteriorate. There is no established metric or proxy to signal process drift toward suboptimal outcomes. System pressure may act as a real-time proxy for process state. We hypothesize that monitoring system pressure provides continuous insights into evolving process conditions, bridging gaps in understanding print stability. This study explores how system pressure variations impact printed line morphology using time-varying effects models (TVEM). Unlike classical regression models, TVEM captures the temporal effects of predictors by allowing model coefficients to evolve over time. This framework advances current knowledge by linking variations in pressure model coefficients to critical aspects of line morphology and emphasizes quality trends rather than hardware-specific outputs. By analyzing how model coefficients change over time, TVEM can link pressure coefficient changes to morphological features like line width, edge roughness, uniformity, and overspray.",1,120 | Cobb Galleria Centre,"Modeling, Monitoring and Control in Additive Manufacturing",12
41,6784.0,A novel low-dimensional learning approach for automated classification of microstructure data with application to additive manufacturing,Academician,"In several manufacturing applications, attaining enhanced material properties relies on mastering and tailoring precise microstructural characteristics, while maintaining re- peatability from build to build. As a result, microstructure analysis and characterization play a central role in product qualification and process verification procedures. The com- mon industrial practice for analysing microstructure data primarily relies on the subjective evaluation of human experts. This practice is time consuming, expensive, and inherently affected by subjective assessments. Such limitations have motivated continuously increas- ing research efforts devoted to the development of data analytics and machine learning solutions for automated processing and classification of microstructural measurements. This study presents a novel approach to automatically extract informative features from complex microstructural data gathered through electron backscattered diffraction (EBSD). The proposed method relies on the extraction and utilization of both morphological grain properties and crystal orientation distribution features by means of a low-dimensional learning approach specifically conceived to deal with the underlying nonlinear pattern of multi-dimensional ESBD data. The comparison against benchmark approaches high- lights the benefits of the proposed solution from a practical usage perspective. In order to show the potential of the newly proposed solution, both simulation and real case studies involving additive manufacturing are considered.",2,120 | Cobb Galleria Centre,"Modeling, Monitoring and Control in Additive Manufacturing",12
42,8635.0,Automatic Melt Pool Tracking and Segmentation in Laser Powder Bed Fusion using X-ray Image Sequence,Academician,"Laser powder bed fusion (LPBF) is the most extensively used technique for metal additive manufacturing (AM). This method uses a laser to melt metal powder onto the substrate by creating a melt pool. The solid-liquid interface of the melt pool plays a critical role in determining the cooling process, which further dominates the product's microstructure and mechanical properties. High-speed X-ray imaging has been used to study the subsurface melt pool phenomena in real-time. However, segmenting out the melt pool from X-ray images is challenging due to the high noise level and low contrast. Due to the novelty of high-speed X-ray imaging, the data processing technique is underdeveloped. Researchers still rely on manual annotating and basic image processing for object segmentation. The former method is time-consuming, and the latter one lacks accuracy and robustness. This paper implements a video object segmentation deep learning model to automatically track and segment the melt pool in the X-ray image sequence for the first time in the literature. The proposed model is semi-supervised, only requiring the manual annotation of the melt pool boundary at the first frame to predict it for the rest of the video. The proposed model incorporates spatio-temporal attention modules to learn the correlations in X-ray image sequences effectively. The experimental results indicate that adopting attention modules improves the melt pool segmentation accuracy compared to the existing state-of-the-art methods.",3,120 | Cobb Galleria Centre,"Modeling, Monitoring and Control in Additive Manufacturing",12
43,6402.0,GeoPIPN: A Novel Geometry-Aware Deep Learning PDE Solver for Faster Thermal Quality Control on Unseen Geometries,Academician,"This presentation proposes Geometry and Physics Informed PointNet ( GeoPIPN ), a novel deep learning-based Partial Differential Equation (PDE) solver tailored for quality control and reliability engineering by rapidly solving PDEs across unseen geometries without further training . PDEs model physical processes like thermal distribution, crucial in additive manufacturing, where precise thermal control ensures desired material properties, uniform microstructure, and dimensional accuracy – key to product integrity and durability. Traditional numerical PDE solvers are often too slow to support the fast-paced, iterative design phase, while classical Physics Informed Neural Networks (PINNs) require retraining for each new geometry , making them inefficient for continuously evolving designs. GeoPIPN overcomes these limitations of generalizing model across different geometric configurations by leveraging a combination of geometry-aware global and boundary features along with local features. Through two case studies, each involving distinct geometric domains governed by thermal distribution physics, we demonstrate significant superior performance of GeoPIPN compared to current state-of-the-art benchmark . GeoPIPN thus offers a powerful solution for quality control and reliability engineering, enabling consistent, repeatable simulations across varying geometries and advancing PDE-driven design to support high-quality, reliable manufacturing processes.",4,120 | Cobb Galleria Centre,"Modeling, Monitoring and Control in Additive Manufacturing",12
44,5396.0,Topology preserving fitting of trimmed NURBS CAD models to deformed solids,Academician,"With Manufacturing 4.0 and digital twins becoming prevalent, leveraging data from different sources, serial numbers, and lifecycle stages for quantitative evaluation is crucial to improve manufacturing. However, inconsistent geometries between as-built and as-designed parts cause difficulties in inspection using data obtained from various modalities. To address this issue, in this work, we create a framework for mapping between the as-designed CAD representation and the as-built part using surface measurements. The main challenges are to capture the correspondence between entities while maintaining topological consistency with the design and eliminating the gaps between connected surfaces. These challenges are even more pronounced when handling complex multi-surface parts with trimmed surfaces and large-scale deformations. Our framework uses the standard mathematical representation of surface geometry in CAD, i.e., the non-uniform rational B-splines (NURBS). We deform the as-designed CAD model using constrained optimization to match the surface measurements of the as-built part. To preserve the topology (the interconnectivity of its surfaces) and obtain a gap-free connection between adjacent surfaces, we formulate new optimization constraints that keep the edge consistent in the geometric space. We test our framework on multiple CAD models that have trimmed NURBS surfaces. This research enables accurate mapping of metrology data from the actual part to the corresponding CAD model, enabling applications such as in-situ monitoring, defect localization, and control of manufacturing processes.",1,120 | Cobb Galleria Centre,"Modeling, Optimization, and Learning for Complex Systems",13
45,5607.0,Sequential Experimental Design for Constructing Reduced-Order Models,Academician,"Projection-based Reduced-Order Modeling requires generating data from full-order systems (through numerical methods). Because a full-order system is often parameterized by a set of parameters, challenges immediately arise when one needs to (sequentially) determine the parameter settings, from the parameter space, under which the full-order system is solved and data are generated. In this talk, we present some preliminary results on the sequential experimental design for constructing reduced-order models that well capture the system behavior over the parameter space (needed in design, control, uncertainty quantification and real-time operations). This talk consists of two major parts. In the first part, we describe a Projected Gaussian Process (pGP) that establish the relationship between parameters and reduced-order models. In the second part, we share some early results on a sequential design framework that enables us to sequentially construct or update the pGP model by collecting data from the optimal experimental conditions (i.e., parameter settings).",2,120 | Cobb Galleria Centre,"Modeling, Optimization, and Learning for Complex Systems",13
46,6321.0,Personalized Private-Shared Federated Model with an Application to Distributed Additive Manufacturing,Academician,"Currently, many small and medium-sized organizations struggle with limited data availability and computational resources, leading to poor predictive capabilities due to data isolation. Federated Learning (FL) addresses this by enabling collaborative model training without data sharing, overcoming data silos. However, FL is challenged by feature space heterogeneity, where clients have missing or different but related features, complicating the development of a consistent, high-quality model for all participants. In this paper, we propose a personalized private-shared federated model for robust prediction. Each client adopts a personalized shared model, facilitated by FL, that guides their private model for inference. The shared model is trained by aggregating knowledge from all clients in the latent space using an encoder-decoder structure, while the private model distills knowledge from the trained shared model for enhanced prediction. This latent space sharing strategy and knowledge distillation can protect the clients’ data privacy while customizing their local models, respectively. We evaluated the proposed model through a simulation and a distributed L-PBF manufacturing scenario. Results show that clients with missing features can achieve significantly improved fatigue life predictions after participating in federated learning. The proposed model effectively enhances predictive capabilities in distributed environments by managing feature heterogeneity while preserving data privacy.",3,120 | Cobb Galleria Centre,"Modeling, Optimization, and Learning for Complex Systems",13
47,6623.0,An Optimal Incentive-Embedded Maintenance Policy for a Fleet of Self-Service Systems Subject to Imperfect Monitoring,Academician,"Self-service systems have experienced substantial growth in recent years due to their ability to operate without on-site staff, reducing labor costs and providing continuous access. However, without on-site personnel, operators must determine the optimal maintenance timing to balance high maintenance setup costs against service levels. Furthermore, as monitoring sensors cannot perfectly detect all failures, devising an effective maintenance policy becomes increasingly challenging. In this study, we propose an incentive mechanism that encourages customers to report failures encountered during use. By integrating failure information from both imperfect monitoring and customer reports, we optimize an incentive-embedded maintenance policy to maximize the long-term profit rate of a fleet of self-service systems. An iterative ternary search algorithm is developed based on the structural properties derived from this optimization problem. A real-world case study of six electric vehicle charging piles (EVCPs) in Hong Kong demonstrates the superiority of the proposed incentive-embedded maintenance policy over existing alternatives.",1,120 | Cobb Galleria Centre,"Monitoring, Prognosis, and Maintenance in Manufacturing and Electric Vehicle Systems",14
48,6575.0,Remaining Useful Life Prediction of Lithium-ion Batteries Using Monotone Decomposition,,"Accurately predicting the remaining useful life (RUL) of lithium-ion batteries is vital for efficient equipment health management. Throughout the aging process, the battery capacity exhibits nonlinear behavior, with intermittent capacity regeneration phenomena causing sudden increments between consecutive cycles, posing challenges for modelling and prediction. Despite the frequent use of empirical mode decomposition (EMD) to decompose capacity series, most EMD-based RUL prediction methods encounter limitations including end effects, information leakage issues, and a lack of uncertainty quantification. To address these challenges, we introduce a novel RUL prediction framework, MonoD-GPR-DeepAR, featuring a unique data decomposition algorithm, monotone decomposition (MonoD). MonoD alleviates end effects by decoupling the original capacity signal into a smooth, decreasing trend and a fluctuant capacity regeneration term. Gaussian process regression (GPR) and deep autoregressive (DeepAR) models are then applied to the subseries for prediction, including uncertainty intervals. Validation using simulations and three real lithium-ion battery datasets demonstrates MonoD’s superior performance in capturing the authentic aging trajectory characteristics. Compared to alternative methods, the MonoD-GPR-DeepAR model shows its effectiveness in addressing complexities introduced by capacity regeneration phenomena in lithium-ion battery RUL prediction.",2,120 | Cobb Galleria Centre,"Monitoring, Prognosis, and Maintenance in Manufacturing and Electric Vehicle Systems",14
49,6804.0,Spatio-temporal thermal history modeling and characterization for Wire Arc Additive Manufacturing processes,Academician,"Wire arc additive manufacturing (WAAM) is a large-area manufacturing process that joins wire feedstock material through directed energy deposition by leveraging a robotic arm. However, the broader adoption of WAAM in mission-critical environments is limited due to concerns with the fabricated components' geometric accuracy and mechanical properties. The thermal history of the WAAM process has been identified as one of the most informative data streams for final component geometric accuracy prediction. However, the thermal history of the WAAM process is highly dynamic and complex, making it very challenging to characterize its spatio-temporal structure during the fabrication. By integrating the WAAM process physics, a new temperature profile descriptor is established to characterize the thermal history of the WAAM process. Subsequently, a Gaussian process model is leveraged to characterize the spatio-temporal structure in the layer-wise thermal history and therefore characterize the dynamic evolution of the temperature profile descriptor. The proposed framework is validated based on a WAAM system comprised of a CMT welding torch and ABB robotic arm, where the thermal history was captured using an infrared thermal camera. The proposed framework can be extended for in-situ geometric accuracy prediction and certification for large-area WAAM components.",3,120 | Cobb Galleria Centre,"Monitoring, Prognosis, and Maintenance in Manufacturing and Electric Vehicle Systems",14
50,5761.0,Enhancing Operational Time for Sea Water Exchangers Utilizing the Tube Sleeve Insertion Method,Practitioner,"Introduction: Shell-and-tube heat exchangers are essential in marine vessels, as their failure leads to operational delays and costly maintenance. In regions like Pakistan’s coastal waters, high pH and salinity accelerate corrosion in marine equipment, drastically reducing the mean time between failures (MTBF) and raising costs. Background: One client, operating in Pakistan’s harbors, faced frequent shell-and-tube exchanger failures due to the material’s vulnerability to seawater. The tubes, made from a 90/10 copper-nickel (Cu-Ni) alloy, were less corrosion-resistant under harsh conditions than a 70/30 Cu-Ni alternative. Consequently, maintenance and operational interruptions became increasingly frequent. Solutions: After thorough inspection, two approaches were proposed: 1. Internal Tube Coating - Costly and time-intensive but offering strong corrosion resistance. 2. Tube Sleeve Insertion - Inserting sleeves at both tube ends, enhancing corrosion resistance at critical points. The client suggested to choose the tube sleeve option for its cost-effectiveness and shorter implementation time. Results: After one year of sleeve insertion, the exchanger showed no failures, confirmed by biannual eddy current inspections indicating excellent condition. The sleeve method proved effective in prolonging exchanger life while reducing maintenance needs. Conclusion: Tube sleeves offer a sustainable, economical solution for protecting marine heat exchangers in high-corrosion environments. This approach minimizes downtime, cuts maintenance costs, and supports self-sustainability, vital for operational efficiency in maritime applications.",1,120 | Cobb Galleria Centre,Failure and Maintenance Analysis for Complex Systems,15
51,6736.0,Application of Software Failure Modes and Effects Analysis to Improve Election Security,Academician,"In 2017, the U.S. Department of Homeland Security (DHS) recognized election systems as a subsecture of national critical infrastructure under the Government Facilities sector, underscoring the need to ensure its security and robustness against threats. Election systems are socio-technical systems involving the complex interactions of hardware, software, and humans (i.e., poll workers) and are susceptible to cyber, physical, and insider threats. Building off prior work by the U.S. Election Assistance Commission’s 2009 threat assessment of precinct count optical scanners (PCOS), the primary election machine used to scan and count ballots, this paper utilizes Software Failure Modes and Effects Analysis (SFMEA), a common safety analysis approach used to identify potential software failure modes and their causes and effects, to update the existing PCOS threat tree. Specifically, through literature review and the application of SFMEA, 60 new threats to the PCOS were identified and added to an updated threat tree. The use of a SFMEA in risk analysis, a bottom-up, forward analysis approach that initiates with a security risk to understand what security threats could arise, along with threat tree analysis, a top-down, backward analysis approach that stats with a threat to understand the risk(s) that cause a threat to occur, allows for a more complete, bi-directional threat analysis. This paper contributes to improving the security of election infrastructure through the application of SFMEA to update the PCOS threat tree. Specifically, this paper details the application of SFMEA for threat analysis as a demonstration to enable its application towards other securitcy-critical systems.",2,120 | Cobb Galleria Centre,Failure and Maintenance Analysis for Complex Systems,15
52,6971.0,Multistage MCN: Masked and Contrastive Learning for Continuous Multistage Manufacturing Processes with No Intermediate Labels and Few Final Labels,Academician,"The development of data-driven soft sensors for monitoring production quality has become common in manufacturing practices, but it typically requires extensive quality label data. In particular, continuous multistage manufacturing processes (MMPs) pose significant challenges due to their inherent characteristics, where inputs are sequentially transformed into final outputs across multiple stages without interruptions. In such processes, exemplified in oil refinery, petrochemical, food, and drug manufacturing domains, acquiring intermediate product labels is nearly impossible, and obtaining final product quality labels involves costly and time-intensive manual efforts. This scarcity of labels limits the application of conventional supervised learning or even semi-supervised learning methods. To address this challenge, we propose a novel self-supervised learning model tailored for continuous MMPs, called the Multistage Masked and Contrastive Learning Network (Multistage MCN). Our model is designed to capture the sequential process dynamics of continuous MMPs using minimal final output labels and no intermediate labels. Specifically, Multistage MCN involves two pretext tasks: one reconstructs masked patches to model sequential dependencies across stages, while the other contrasts samples to learn varied production patterns. These two tasks are conducted within a deep learning architecture that captures stage-wise sequential dependencies and lead-time differences, enabling effective representations of continuous MMPs with varied production lead times. Experimental evaluations for quality prediction on real-world datasets from petrochemical manufacturing factories demonstrate that Multistage MCN achieves performance comparable to existing supervised and semi-supervised models, significantly reducing reliance on costly labeling efforts while maintaining accuracy and reliability.",1,105 | Cobb Galleria Centre,Machine Learning and Statistical Techniques for Quality Assurance in Manufacturing,16
53,8790.0,A Statistical Modelling and Convex Relaxation Approach for Product Quality Analytics,Academician,"Massive continuous stream data in multi-stage processes provide a data foundation for product-oriented quality determination and intelligent decision-making. However, quality analytic models based on historical data are difficult to consistently stable in complex multi-stage manufacturing processes, so quantifying the model uncertainty becomes an important challenge. In this paper, we propose a multi-output conformal risk control method to realize robust interval estimation of product quality and adaptive calibration of the model. The proposed method first uses a multi-output regression model as the basic model. Then it minimizes the multi-output prediction region as the conformal risk control objective on the calibration set with a given global confidence level. The model adaptive calibration is realized by fine-tuning the prediction intervals through parameters, and finally, the fine-tuning coefficients for each quality indicator are solved by the convex relaxation method. Taking the hot rolling process as an example, the proposed method is used for the prediction of the mechanical properties of thick plates. The experimental results prove that the proposed method can provide interval estimation with a global coverage guarantee for multiple quality variables, and it can be adaptively calibrated by the new data, which can provide real-time and effective information for intelligent decision-making.",2,105 | Cobb Galleria Centre,Machine Learning and Statistical Techniques for Quality Assurance in Manufacturing,16
54,8846.0,A Convex Cluster and Regression Approach With functional data for Hot Rolled Product Quality Analytics,,"The data collected during the production process plays a crucial role for the quality analytics of products in the process industry. These data often include diverse and complex types such as compositional data and high-frequency time series data, which poses unprecedented challenges to the analytic of industrial data. This paper proposes a complex data modeling method for product quality analysis in process industries, which includes three modules: pre-processing of complex data, clustering, and regression. In the data preprocessing module, in response to the curse of dimensionality caused by high-dimensional data, it is proposed to model time series data as functional data and use spline basis expansion and IRL transformation methods to project functional and component data into Euclidean space; In the clustering module, considering the process characteristics of the process industry, we introduce a new distance measurement method and uses convex relaxation method to transform the clustering problem into a convex optimization problem, which is solved by alternating minimization algorithm; In the regression module, functional data analysis is used to establish relationship models between chemical composition, process, and quality for each type of data. The effectiveness of the proposed modeling method was evaluated by using historical data from actual hot rolling production, and compared with other methods to verify its performance.",3,105 | Cobb Galleria Centre,Machine Learning and Statistical Techniques for Quality Assurance in Manufacturing,16
55,5527.0,Sequential Learning for Integrated Safety and Quality Assurance in Human-Robot Collaborative Manufacturing Systems,Academician,"Safety concerns severely impede industrial adoption of emerging human-robot collaborative manufacturing systems. We propose a human-centric, near real-time anomaly detection framework rooted in sequential learning for integrated safety and quality assurance—a marked departure from earlier, quality- or safety-exclusive process control approaches. The framework first adapts deep learning and pose estimation to track fast robot motions via real-time images from a surveillance camera. By incorporating synchronized human physiological data through sequential learning, the framework provides uncertainty-based, risk-metered alerts for task anomalies. Application to a shared human-robot assembly line suggests that the framework can accurately track robot trajectories and outperform conventional statistical process control methods in anomaly detection, while reducing safety risks. The framework also allows for straightforward extensions to more involved manufacturing settings.",1,105 | Cobb Galleria Centre,Sequential Learning for Manufacturing,17
56,8936.0,Knowledge reinforced reinforcement learning for transferable selective annotation,Academician,"The increasing data volume in fields like personalized manufacturing, biomanufacturing, and medical diagnostics has amplified the critical role of human annotation, which is time-consuming and labor-intensive, directly impacting AI performance. While active learning aims to reduce annotation efforts by selectively querying data points, existing methods often fail to transfer their sampling strategies across similar-but-not-identical environments. This limitation leads to inefficiencies in data annotation when dealing with varying data distributions or evolving data domains where adaptability is crucial. To overcome these limitations, we propose a knowledge-reinforced reinforcement learning framework inspired by Human Information Processing (HIP) theory. Our hierarchical framework incorporates two cooperative agents: a long-term agent and a short-term agent . The long-term agent, reflecting top-down cognitive processes, strategically guides the learning process by leveraging historical knowledge and overarching objectives, influencing the short-term agent, which focuses on immediate data uncertainties, representing bottom-up processes. By coordinating these agents, the framework enables more effective knowledge transfer and enhances active learning. Our approach enhances the transferability and adaptability of active learning across different but related data domains by facilitating inter-agent cooperation. This hierarchical structure improves sampling efficiency and allows the active learner to generalize its querying strategy to similar-but-not-identical environments. By assigning cooperative goals to each agent, the framework mirrors human cognitive strategies in complex tasks, effectively addressing data annotation challenges in medical and other data-intensive fields, and improving the transferability and effectiveness of AI systems.",2,105 | Cobb Galleria Centre,Sequential Learning for Manufacturing,17
57,8829.0,Learning-based Optimization for Complex Physical Systems,Academician,"Learning-based optimization (i.e., reinforcement learning) is known for its strength in generalizability and scalability. Generalizability indicates the knowledge learned in optimizing the training data can be directly applied to optimize the unseen testing scenarios without retraining. Scalability represents the capability of handling giant design space. In this talk, we will discuss the recent research work on developing learning-based optimization for complex physical systems. We select two cases as examples. One is the fixture layout design for reducing shape deformation of large-scale sheet parts in the assembly process. The other is optimizing the control parameters in the Brookhaven Alternating Gradient Synchrotron (AGS) to improve intensity and emittance preservation after the RF bunch merge.",3,105 | Cobb Galleria Centre,Sequential Learning for Manufacturing,17
58,6055.0,Mutual Information Surprise: Rethinking Unexpectedness for Enhanced Autonomy in Engineering Systems,Academician,"Surprise, initially defined in psychology as a measure of unexpectedness, is also recognized as a mechanism for enhancing autonomy in eingeering systems. Common measures like Shannon Surprise and Bayesian Surprise quantify unexpectedness in anomaly detection by assessing belief deviations between old and new observations. However, these approaches may be limited by model inaccuracies, insensitivity to uncertainty variations, and an inability to detect over-exploitation of frequently observed data. This study reexamines surprise for engineering systems, introducing mutual information surprise to detect shifts in level of system understanding rather than understanding itself. A two-sided statistical framework is proposed, with results showing that mutual information surprise provides greater versatility and robustness compared to existing measures.",4,105 | Cobb Galleria Centre,Sequential Learning for Manufacturing,17
59,5062.0,From Static to Dynamic: Recent Advances in the Process Control of 4D Printing,,"Additive manufacturing (AM), widely known as 3D printing, has recently expanded with 4D printing—an innovative technique that combines AM with stimuli-responsive materials to introduce dynamic, programmable functionality. However, precise control over the shape-morphing behavior in 4D printed products remains challenging due to the influence of both printing conditions and external stimuli. This talk will explore new methodologies designed to enhance process control, enabling more accurate and predictable 3D shape transformations in 4D printing.",1,105 | Cobb Galleria Centre,Smart and Adaptive Process Control in Manufacturing and Food Systems,18
60,6914.0,Improving Bread Fermentation Through Temperature and Humidity Control in Proofers,Practitioner,"In response to waste challenges at Cidrines, Inc. in Puerto Rico, this research analyzed the Proofing Machine in their part-baked and fully-baked bread production line using Lean Six Sigma principles. The study applied the DMAIC framework to address recurring issues with maintaining control over temperature and humidity inside the Proofing Machine (Fermentation Accelerator). The research targeted the Sobao 10-count Sobao bread product batch to assess how these environmental factors influenced fermentation times. The company's operational limits were set at 100°F for temperature and 80.00% for humidity. However, findings revealed that the temperature at the machine's center averaged 105.81°F with moisture of 78.70%, while the machine's wings recorded an average temperature of 103.95°F and humidity of 78.89%. These deviations from the set parameters indicated significant process inconsistencies. The study recommended maintaining optimal conditions by decreasing temperature and increasing humidity to align with the product’s required standards. This research offered insights into process control improvements that could reduce waste and enhance product consistency. The proposed adjustments aimed to bring the machine's conditions closer to target limits, fostering greater efficiency in the fermentation stage and supporting better overall production outcomes.",2,105 | Cobb Galleria Centre,Smart and Adaptive Process Control in Manufacturing and Food Systems,18
61,6170.0,Implementing Quality and Lean Six Sigma in Solid-State Battery Manufacturing,Practitioner,"The solid-state battery manufacturing industry stands at the cutting edge of energy storage, particularly as demand for electric vehicles (EVs) accelerates. Achieving high standards in both quality and efficiency is essential. One of the major challenges in this process is maintaining strict contamination controls as even minuscule particles or debris can lead to defects that may affect battery quality, safety, and performance. Using Lean Six Sigma methodologies alongside ISO 14644-1 cleanroom standards, this research targets contamination issues with a focus on key performance indicators such as airborne particle concentration and airflow dynamics. Techniques like 5S, root cause analysis, and automated cleaning systems were implemented to systematically reduce contaminants, enhance cleanroom compliance, and increase operational efficiency. Through statistical approaches, including design of experiments and hypothesis testing, this study identified optimal cleaning strategies for different production phases. Quality Engineering tools—such as the 8D problem-solving method, Corrective and Preventive Actions, and Measurement System Analysis (MSA) Gage R&R—were also applied to assess and address contamination sources. The results demonstrated significant reductions in both debris levels and process variability, supporting higher product reliability and cleaner manufacturing processes. This structured approach highlights the potential for data-driven, quality-focused improvements in solid-state battery production, offering valuable insights for advancing EV technology through cleaner, more reliable manufacturing processes.",3,105 | Cobb Galleria Centre,Smart and Adaptive Process Control in Manufacturing and Food Systems,18
62,6937.0,Amortized Variational Inference of Interfacial Characteristics for Ultrasonic Weak-Bond Detection in Adhesive Lap Joints,Academician,"Nondestructive inspection (NDI) of damages (e.g., imperfect or degraded bondlines) or the remaining strength of adhesively bonded lap joints is critical for the manufacturing and operation of aircrafts, vehicles and wind turbine blades. It remains a challenge to use the conventional NDI to quantitatively infer the poor adhesion between the adherend and the adhesive. Essentially, the direct NDI measurements have to be related to the latent parameters indicating bond integrity. This work uses a statistical machine learning method, amortized variational inference, to build the mapping from the NDI measurements to posterior physical latent variables. In this paper, the smoothness of the neural network approximation is novelly constrained by the physical A-scan signals from NDI. In addition, multiple frequencies-based NDI measurements are fused into the input with appropriate data processing (e.g., image registration) for better variational inference results. The proposed physics-aware statistical machine learning method enables real-time and large-scale quality inspection in practice.",1,116 | Cobb Galleria Centre,"Physics-Informed Modeling, Monitoring, and Prediction",19
63,8805.0,MMoE: Enforcing Monotonicity in Mixture of Experts Models,Academician,"Shape constraints, such as monotonicity, arise in many engineering and physical applications, as motivated by some underlying physics or existing theory. As such, a growing trend in machine learning (ML) and data science calls for data-driven models that impose monotonicity constraints. Along those lines, this work is concerned with enforcing monotonicity in the context of mixture of experts (MoE) models. MoE is a prevalent ensemble modeling framework where multiple ML models (referred to as experts) are trained on different data subsets and then merged using input-dependent weights (referred to as a gating mechanism), thereby producing a mixture. We show that achieving monotonicity in MoE models is non-trivial since the mixture is not guaranteed to be monotonic even if the constituting experts are. Hence, we propose an MoE method that enforces monotonicity by leveraging the properties of a special class of polynomial functions as experts and then internalizing a constrained optimization model which directly acts on their coefficients. We mathematically show that, unlike the unconstrained MoE model, the proposed approach guarantees the mixture to be monotonic, and then test it on a synthetic dataset and two case studies from materials failure and wind energy generation, respectively. The case studies demonstrate that the proposed approach preserves the monotonicity of the mixture model, at a minimal compromise of its adequacy or predictive power.",2,116 | Cobb Galleria Centre,"Physics-Informed Modeling, Monitoring, and Prediction",19
64,6459.0,Enhancing Reduced-Order Models in High-Dimensional Engineering Applications through Gradient Boosting Tree,Academician,"Addressing forward problems with high-fidelity models like Computational Fluid Dynamics (CFD) and Finite Element Analysis (FEA) presents significant challenges due to their computational demands from high-dimensional spatial discretization. These models are expensive, requiring powerful solvers and extensive computational resources to achieve precise results. Additionally, their sensitivity to parameter variations complicates the accurate development of Reduced-Order Models (ROMs). Projection-based methods such as Proper Orthogonal Decomposition (POD) are typically employed to extract optimal basis modes from snapshot data for ROMs. However, these methods struggle to robustly quantify uncertainty, affecting their reliability and accuracy in practical applications. My research explores a novel integration of XGBoost, known for its predictive accuracy and capability to handle complex relationships, with an innovative mapping technique between a multi-dimensional Euclidean space and the Grassmann Manifold, which represents low-dimensional subspaces. This mapping includes an injectivity constraint to enhance optimization, making XGBoost suitable with minor modifications. The approach involves mapping the subspaces from the Grassmann Manifold back to the Euclidean space to formulate a multivariate XGBoost regression. When introduced to a new parameter, this method projects the corresponding vector onto the Grassmann Manifold, facilitating the identification of the optimal subspace or POD basis. This process allows for a detailed understanding of how variations in settings affect the optimal representation in ROMs and quantifies the confidence in these insights, addressing limitations in traditional projection-based methods.",1,116 | Cobb Galleria Centre,Spatio-temporal Analysis in Engineering Applications,20
65,6879.0,Time Series Domain Adaptation via Multi-view Contrastive Learning,Academician,"Adapting time series models across different domains is still a tough problem, mainly because of the complex temporal dependencies and shifts in data distributions between domains. Existing approaches have made progress, but they usually focus on just a single type of feature or variables from decomposition, which limits their ability to fully capture the temporal characteristics involved. In this work, we introduce a new self-supervised framework that combines different feature types from multiple domains using contrastive learning. Specifically, we leverage patterns from the time domain, dynamics from derivatives, and spectral features from the frequency domain. We use dedicated encoders for each of these domains and a hierarchical fusion mechanism to ensure that the final representation is both domain-invariant and keeps the temporal details intact. By optimizing domain-specific contrastive objectives alongside cross-domain alignment, our model learns features that are consistent over time and transferable across domains. Our experiments on several benchmark datasets in medical domain show that this approach outperforms other state-of-the-art methods in classification tasks. Overall, our method presents a promising direction for solving domain adaptation challenges in time series analysis.",2,116 | Cobb Galleria Centre,Spatio-temporal Analysis in Engineering Applications,20
66,8640.0,Advancing AI-Driven Road Safety and Infrastructure Resilience: A Call for Innovative Policy and Technological Integration,Academician,"This paper argues for the urgent adoption of AI technologies to bridge gaps in data translation, emergency response times, and risk mitigation across U.S. transportation systems. The proposed policy initiatives emphasize integrating AI with geospatial mapping, predictive analytics, and IoT-based systems, underpinned by cross-disciplinary collaboration, targeted workforce development, and equitable interventions for vulnerable communities. This paper outlines key objectives, critiques current approaches, and presents a vision for reshaping road safety to manage risks and reduce fatalities. The paper aims to spark a transformative shift toward a cohesive and data-driven policy framework for road safety in the U.S.",3,116 | Cobb Galleria Centre,Spatio-temporal Analysis in Engineering Applications,20
67,6815.0,Calibration Time Reduction for Tensiometers Models T-5 and T-60,Practitioner,"This project aims to reduce the calibration times for the tensiometers T-5 and T-60, which currently exceed the company’s target of twelve minutes per unit. To achieve this, the DMAIC (Define, Measure, Analyze, Improve, Control) problem-solving methodology was employed alongside robust statistical analysis tools to assess the calibration process thoroughly. Tensiometers are critical instruments to measure tension in aircraft cables, ensuring operational safety. While the T-5 and T-60 models serve the same purpose, the T-5 requires less weight than the T-60, impacting the calibration dynamics. The current process lacks standardization and is susceptible to workplace distractions and suboptimal ergonomics, contributing to prolonged calibration times. A comprehensive analysis was conducted to address these issues. Boxplot analyses and statistical analyses revealed that the T-5 model exhibited more significant variability in calibration times than the T-60. This insight was pivotal in shaping targeted improvements. In alignment with the United Nations Sustainable Development Goal 9—focused on building resilient infrastructure, fostering sustainable industrialization, and promoting innovation—practical solutions were proposed. These included the integration of a tablet to streamline workflow, providing an ergonomic chair to enhance operator comfort, and installing a curtain divider to minimize environmental distractions. The proposed solutions are designed to be cost-effective and straightforward, ensuring they can be implemented with minimal disruption. By prioritizing these measures, the project aims to standardize the calibration process, reduce time variability, and consistently meet the company’s calibration time goals, ultimately contributing to greater operational efficiency and worker well-being.",1,120 | Cobb Galleria Centre,Process Monitoring and Instrument Calibration,21
68,6340.0,Harnessing Real-Time Sensor Data for Fault Diagnosis in the Continuous Production of Copper Magnet Wires,Practitioner,"The production of defective products leads to substantial material and financial losses. It is critical to detect manufacturing defects as early as possible to reduce waste. To do so, many manufacturers have harnessed technology by installing sensors to monitor their production processes in real-time. Nonetheless, a significant portion of the data collected by the sensors remains unanalyzed due to processing and analytical constraints, resulting in missed opportunities for early fault diagnosis and quality improvement. This research leverages real-time sensor data from Rea Magnet Wire, one of the world’s largest manufacturers of magnet and nonferrous wire products, to identify the process variables responsible for deffects in the continuous manufacturing of aluminum wires. Due to the continuous nature of the manufacturing process, data alignment was done before model training. Three machine learning models (Random Forest, XGBoost, and Logistic Regression) were implemented to predict faults in the production of copper magnet wires using 42 process variables obtained from real-time sensor data from their Lafayette plant. We observe that interactions between the process variables are critical for fault diagnosis given that the non-linear methods have better performance. Furthermore, we study the effect of the process variables on the quality measures by using Accumulative Local Effects (ALE) The proposed methods lead to significant waste reduction at Rea’s plant and provide a foundation for developing automated fault diagnosis for continuous manufacturing.",2,120 | Cobb Galleria Centre,Process Monitoring and Instrument Calibration,21
69,8694.0,Within-layer In-situ Quality Monitoring of Additive Manufacturing Processes Along Production Paths,Academician,"In the rapidly evolving field of additive manufacturing (AM), existing layer-wise quality monitoring methods fall short in quickly detecting spatially localized anomalies within layers while the product is being produced. The layer-wise methods, which typically analyze vast amount of data post-production of each layer, result in significant delays in anomaly detection, thereby increasing material waste and reducing efficiency of AM processes. In this article, we propose to establish a novel in-situ quality monitoring framework of AM processes to quickly detect spatially localized anomalies in real time as they occur within layers along the predefined production paths before the collection of full layer data. In particular, we integrate neural network with Gaussian processes to develop a mixed-effect model for layer-wise spatial modeling that captures both the global spatial profile and localized spatial effects among measurement points of the layer. By leveraging layer-wise spatial relationships, we further develop a residual-based spatial Exponentially Weighted Moving Average (sp-EWMA) control chart to efficiently and recursively update monitoring statistics along the production path, where the weight of each measurement point decay exponentially with distance from the current observation. The proposed in-situ monitoring framework enhances the real-time detection capability of localized anomalies within layers, facilitating rapid responses and immediate corrective actions to reduce material wastage and enhance production efficiency of AM processes. A real-life laser powder bed fusion (L-PBF) dataset in a metal AM process is applied to validate the proposed method.",3,120 | Cobb Galleria Centre,Process Monitoring and Instrument Calibration,21
70,6111.0,Assessing User Retention of Connected Home Appliances: Survival Analysis,Academician,"The advancement of smart home appliance technology has led to the emergence of updatable features within these connected devices. This new data necessitates a change in traditional evaluation methods. While many marketing standards have traditionally relied on retention metrics, these metrics face significant limitations due to their lack of statistical validation and inability to capture the personalized interactions between users and smart home appliances. Our research emphasizes data derived from downloadable and updatable content, enabling the measurement of personalized user interactions. By analyzing this data, we introduce a metric based on survival analysis for engagement, addressing the gap in statistical validation found in traditional retention metrics. The approach suggests the groundwork for continuous measurement of user engagement and offer practical guidelines for improving accurate assessment and enhancement of user engagement in the development and marketing strategies of products.",1,120 | Cobb Galleria Centre,Human-Centered Efficiency in Smart Systems,22
71,8849.0,Hands-On approach towards Industrial engineering concepts through interactive Lego workstation assembly,Academician,"This project designs and implements a stimulated workstation assembly module using Lego components that teach the core Industrial Engineering (IE) techniques and tools to students in an interactive way to increase learning efficiency. The primary objective is to demonstrate the application of Six Sigma concepts, Statistical Quality Control (SQC), Inventory Analysis, and manufacturing systems and methods by having the students go through the Lego train assembly process and learn about systemized production. A picture-based instruction manual was developed to ensure ease of assembly and to test different concepts, and the process was tested by third-party users to validate clarity and effectiveness. We developed two different modules, unoptimized and optimized modules to show process improvement, defect reduction and IE concepts. The first initial module exhibits a higher defect rate than that in the second module. For our unoptimized module, we had high defects because of an unclear instruction manual and an unorganized inventory of Lego pieces. To overcome this, additional stations were set up to even out bottlenecks. Holding bins were refined with only pieces essential for each station left so students could understand how inventory affects production. The instruction manual was also updated and evaluated to validate efficiency. Various IE metrics were tracked, ensuring continuous improvement throughout the process. The objective of the project is to guide students toward achieving results in the optimized module when starting off with the unoptimized module. This project provides a hands-on approach to understanding the practical implications of industrial engineering principles.",2,120 | Cobb Galleria Centre,Human-Centered Efficiency in Smart Systems,22
72,4855.0,Understanding CBTC Safety Enhancement Through Fault-Tree Analysis of the 2014 CTA Track Overrun Accident at O’Hare Station,Academician,"This study analyzes the Chicago Transit Authority’s March 24, 2014, accident where a subway train overran the end of the tracks at O’Hare Station, a terminus in the CTA system. This was a serious accident with enormous service and safety impacts, so further investigation is needed to prevent future accidents of a similar nature. In the context of this incident, the merits of Communications-Based Train Control (CBTC) technology are considered from safety and operational perspectives. Additionally, a fault-tree analysis for the accident is conducted using information from the National Transportation Safety Board accident report and post-accident recommendations, to understand the root causes of the accident. Based on the fault-tree analysis, opportunities for CBTC to mitigate root causes of an accident of a similar nature are found, and a recommendation for CBTC implementation on rail mass transit systems to improve system safety is made.",3,120 | Cobb Galleria Centre,Human-Centered Efficiency in Smart Systems,22
73,5906.0,Intelligent Hierarchical Fault Diagnosis and Maintenance Recommendations for Text Mining of O&M Records,,"Textual Operations and Maintenance (O&M) records capture a wealth of failure information and maintenance activities, representing the basis for risk, reliability, and maintainability investigations. Effectively mining such data gives insights into hierarchical fault diagnosis and decision-making for emerging failure descriptions. However, this process requires annotation effort and diagnostic expertise, which are time-consuming and labor-intensive. Therefore, this paper proposes an intelligent hierarchical fault diagnosis and maintenance recommendation method specifically designed for textual O&M data. Firstly, a novel hierarchy-informed prompt-tuning method is proposed to identify failure mode at multiple granularities while mitigating the need for extensively labeled samples. Through feature fusion and probability propagation, hierarchy information is incorporated during the training phase. Subsequently, a knowledge base-enhanced large language model is proposed to provide online maintenance recommendations. Historical textual O&M records sharing the same failure modes are chosen based on semantic similarity to establish the knowledge base, facilitating maintenance recommendations by designing the prompt template with this base. The feasibility and superiority of the proposed method are validated using the dataset of 313 operating offshore wind turbines in mainland China. Overall, the proposed approach facilitates the failure knowledge mining for textual O&M records and contributes to the safe operation and maintenance of complex industrial systems.",1,120 | Cobb Galleria Centre,Text based Analysis with Large Models,23
74,6594.0,Beyond Text: Applying Large Language Models to Tabular Data for Anomaly Detection and Fault Classification,Academician,"This study examines the capabilities and limitations of using large language models (LLMs) to analyze tabular, non-textual data for anomaly detection and classification tasks. More specifically, this study will employ an LLM to try and detect anomalies and classify faults based on inputs of tabular data from a motor system. Data acquisitioned from the motor contains baseline operational data as well as data from serveral known-fault states, which were intentionally introduced to the motor. This goal of this study is to explore the potential of LLMs outside of conventional natural language processing tasks.",2,120 | Cobb Galleria Centre,Text based Analysis with Large Models,23
75,8677.0,Piecewise Deterministic Markov Decision Process for Opportunistic Maintenance Optimization in GPU Clusters Utilizing Direct-to-chip Liquid Cooling,Academician,"GPU clusters for training Large Language Models (LLMs) are increasingly recognized for their energy efficiency and advanced cooling solutions, particularly Direct-to-Chip Liquid Cooling (D2C-LC), which directly delivers coolant to heat-generating components. While failures of components like GPUs or switches can severely impact network performance and reliability, the malfunction of the GPU cold plate or Coolant Distribution Unit (CDU) affects performance over time rather than causing immediate disruptions. Effective inspection and maintenance strategies are crucial for preventing interruptions during training LLMs. This paper investigates opportunistic maintenance optimization problems in GPU clusters utilizing D2C-LC, proposing a Piecewise Deterministic Markov Decision Process (PDMDP) to manage maintenance strategies, including perfect, preventive, and opportunistic maintenance. The system’s natural deterioration is modeled by a Piecewise Deterministic Markov Process (PDMP), treating consecutive failures influenced by local load-sharing as random shocks. This paper formulates an optimization problem within a PDMDP framework to minimize maintenance costs and develop a simulation-integrated policy mirror descent algorithm to solve the model. The effectiveness and advantages of our proposed framework are demonstrated through a comprehensive case study that analyzes optimal maintenance strategies in a real-world context.",3,120 | Cobb Galleria Centre,Text based Analysis with Large Models,23
76,6209.0,Extended Block Diagram Method for Evaluating the Reliability of Multi-State  Systems Considering Dependent Components,,"In multi-state systems (MSS), the overall system's performance is influenced by its components' combined effects. These components may degrade over time due to factors such as fatigue, wear, or external stress, leading to changes in the system's overall performance. Several methods have been developed to evaluate the reliability of multi-state systems (MSS), including Markov processes, stochastic processes, and universal generating function (UGF) methods. Among these, the stochastic processes method has been particularly effective in handling repairable multi-state systems. The reliability evaluation of MSS often assumes component independence, a subject that has been extensively studied. However, accounting for component interdependence is crucial, as it better reflects real-world conditions. Recent studies have highlighted the effectiveness of the UGF method in analyzing the dynamic reliability of MSS. By using the Laplace transform and its inverse, the UGF method provides a powerful tool for assessing both transient and steady-state reliability, especially when dealing with complex dependencies among components. Traditional block diagram methods assess system reliability but overlook repairable multi-state components due to the computational complexity involved. While the universal generating function (UGF) method has been widely explored, it has not been applied to systems with interdependent components in existing literature. This paper introduces the use of the UGF method in conjunction with stochastic processes to analyze MSSs with interdependent components. A state-space diagram is constructed to define all possible state transitions to evaluate the reliability of an MSS using the stochastic processes. The results demonstrate significant improvements in the reliability assessment of multi-state systems.",1,120 | Cobb Galleria Centre,Time-Series Modeling and Intelligent System Analytics,24
77,6394.0,Changing-State Quasars Detection using a Neural Differential Equation Approach,Academician,"In this work, we present a novel deep learning-based approach for the detection of changing-state quasars (CSQSOs), which are of critical importance in quasars (QSOs) research due to their substantial temporal variations in optical properties. Detecting and monitoring CSQSOs presents considerable challenges, including data sparsity and irregularity, complex and diverse behavior, large data volume, and limited labeled training data. To address these challenges, we propose an unsupervised anomaly detection based on neural differential equations that incorporates the irregularities in time series data and captures the physical characteristics of QSOs. By utilizing neural differential equations as a generative model to learn the characteristics of normal QSOs, and employing reconstruction error-based anomaly detection to identify CSQSOs, the proposed method mitigates issues related to the scarcity of labeled data and irregular sampling. The effectiveness of the proposed method is validated through extensive simulation studies using three types of datasets (mean shift, variance shift, and exponential shift) derived from real-world physical models. Our method outperforms existing methods in CSQSO detection by simultaneously modeling the irregular and stochastic nature of QSOs.",2,120 | Cobb Galleria Centre,Time-Series Modeling and Intelligent System Analytics,24
78,8786.0,Enabling Tensor Decomposition for Time-Series Classification via A Simple Pseudo-Laplacian Contrast,,"Tensor decomposition has emerged as a prominent technique to learn low-dimensional representation under the supervision of reconstruction error, primarily benefiting data inference tasks like completion and imputation, but not classification task. We argue that the non-uniqueness and rotation invariance of tensor decomposition allow us to identify the directions with largest class-variability and simple graph Laplacian can effectively achieve this objective. Therefore we propose a novel Pseudo Laplacian Contrast (PLC) tensor decomposition framework, which integrates the data augmentation and cross-view Laplacian to enable the extraction of class-aware representations while effectively capturing the intrinsic low-rank structure within reconstruction constraint. An unsupervised alternative optimization algorithm is further developed to iteratively estimate the pseudo graph and minimize the loss using Alternating Least Square (ALS). Extensive experimental results on various datasets demonstrate the effectiveness of our approach.",3,120 | Cobb Galleria Centre,Time-Series Modeling and Intelligent System Analytics,24
79,5854.0,Analysis of Common Terminologies in Program Educational Objectives (PEOs) and Student Outcomes (SOs) for Top-ranked ABET-Accredited Industrial and Systems Engineering Programs,Academician,"With the growing number of schools and colleges offering Industrial and Systems Engineering programs in the United States, it is important to understand the core skills and competencies emphasized in Industrial and Systems Engineering education. This research focuses on identifying the most frequently used terminologies in Program Educational Objectives (PEOs) and similarities between the Student Outcomes (SOs) for ABET-accredited programs in Industrial and Systems Engineering. Data were collected using the U.S. News & World Report 2024 top-ranked programs in this field along with ABET-accredited programs after the 2024 ABET’s accreditation cycle found in ABET’s database. The data collected were categorized into three main categories: Industrial Engineering programs, accredited under ABET’s Industrial and Similarly Named Engineering Programs Criteria; Systems Engineering programs, accredited under ABET’s Systems and Similarly Named Engineering Programs Criteria; and Industrial and Systems Engineering programs, accredited upon meeting both preceding criteria. The main objective of this research is to analyze and identify common terminologies within the PEOs of the selected ABET-accredited programs using WordCloud and a text-based similarity analysis for each program’s SOs with ABET’s SOs. By examining the language and key phrases employed in these objectives and outcomes, the study aims to reveal trends and priorities in the field of Industrial and Systems Engineering education.",1,Stanhope | Renaissance Waverly Hotel,Closing Workforce Gaps,25
80,6399.0,Adapting Engineering Education for the Future: Exploring Generative AI Applications and Employer Expectations,Academician,"As artificial intelligence (AI) drives innovation, creates career opportunities, and shifts required skills across industries, it also brings challenges, particularly in aligning educational programs with industry demands. This study explores how generative AI technologies are currently being applied in organizations and the use of generative AI within the engineering industry. Using a cross-sectional research design adapted from the Technology Acceptance Model (TAM), the study aims to explore employer’s perspectives on the importance and impact of generative AI on the future of engineering practice and the competencies employers expect from engineering graduates. Employers’ data was collected through a comprehensive online survey. The survey assesses stakeholders’ perceptions of generative AI’s utility, ease of use, and their behavioral intentions and actual use. By examining generative AI applications and employer expectations, the study offers valuable insights into the skills engineering graduates will need in a technology-driven workforce. The findings are expected to guide curriculum adjustments, ensuring that engineering programs produce graduates equipped to meet the demands of a rapidly changing technological landscape. This research aims to bridge gaps between academic preparation and industry needs, reinforcing the importance of generative AI in fostering a workforce prepared for innovative roles in engineering. The insights gained will shape engineering education strategies that align with industry trends, positioning future professionals to excel in AI-enhanced work environments and make significant contributions to engineering.",2,Stanhope | Renaissance Waverly Hotel,Closing Workforce Gaps,25
81,6204.0,"Assessing needs for emergency, disaster, and crisis management skills for engineering students",Practitioner,"Growing threats of natural hazards, industrial incidents, and pandemics require engineers to possess skills and knowledge of emergency, disaster, and crisis management (EDCM). As the first step of a course development project, the current work provides findings from needs assessment of EDCM education among engineering students. The project team, consisting of three faculty members from engineering, curriculum and instructional design, and political science, conducted group discussions with 12 engineering students, interviews with four emergency professionals (e.g., firefighter, emergency manager), and one civil engineering faculty member who is well versed in disaster recovery. Group discussions and interviews were transcribed for a qualitative content analysis. Findings indicate that all of the participants viewed EDCM as an important gap to be addressed in engineering education. Regarding existing gaps of EDCM education in engineering, most of students stated that they participated in simple emergency drills such as fire evacuation. Emergency professionals provided several challenges arising while coordinating with engineers in industry. Main issues include poor emergency communication within the organization, inadequate transfer of relevant information to emergency responders, and limited adoption of incident action planning processes. The faculty member highlighted the educational needs for quantitative disaster modeling and taking advantage of financial aid programs, including flood damage assistance. Common recommendations for future EDCM education include the use of recent disaster case studies, hands-on learning experiences using simulation and live demonstration, and training on incident action planning processes. Findings from the current needs assessment inform the development and design of EDCM education courses for engineering students.",3,Stanhope | Renaissance Waverly Hotel,Closing Workforce Gaps,25
82,8878.0,Navy Engineering Analytics Program (NEAP): Inspiring and Motivating Undergraduates to Apply Their Skills to Defense,Academician,"Thanks to funding from the Office of Naval Research, Iowa State University (ISU) launched a Navy Engineering Analytics Program (NEAP) for undergraduate students in 2022. The objective of NEAP is to develop an innovative education and training program that teaches analytical skills to solve Navy and defense problems. The goal of NEAP is to provide undergraduate engineering students at ISU with the necessary analytical skills so that they can enter into professions in the Navy, the broader defense community, and industry that directly supports the Navy and the Department of Defense (DoD). NEAP is currently composed of four courses: (i) crisis decision making and risk management, (ii) design and evaluation of human-computer interaction, (iii) problem solving using R, and (iv) a project-based course in which students work on DoD-sponsored projects. These four courses align with the Navy's research and development objectives for augmenting the warfighter and sense-making. Students in these courses have analyzed situations such as: the COVID-19 pandemic breaking out on a naval aircraft carrier; a Navy crew unable to determine if an iidentifed aircraft is civilian or military; choosing the most cost-effective ship to acquire; and using regression to identify enlisted servicemen and women for officer candidacy. Since its inception, NEAP has awarded approximately 30 students with scholarships, supported student internships at DoD, and helped recruit two students for the Naval Nuclear Propulsion Office Candidate Program.",4,Stanhope | Renaissance Waverly Hotel,Closing Workforce Gaps,25
83,5917.0,The Impact of a Common First-Year Engineering Orientation Course  on Industrial and Systems Engineering Majors,Academician,"[Our De-identified] College of Engineering and Engineering Technology developed a common, 1-credit hour engineering orientation course, ENGR 1000, for all entering engineering and engineering technology students with a goal of helping students find the best-fit major earlier in their degree. This course is part of the college’s new first-year common core curriculum, aimed at addressing the challenges faced by students who enter the university unsure of their major. ENGR 1000 is co-taught by faculty across all engineering departments and introduces students to the various majors offered by the college and the resources available to support their academic journey and improve student success. Each department is allocated two weeks to introduce students to key concepts, career paths, and challenges specific to their discipline. Industrial and Systems Engineering uses one class period to discuss the major, career paths and student activities and the other class period to do a tinker-toy production line efficiency simulation with class discussion. To evaluate the impact of the ENGR 1000 course, more than 1600 students were administered brief surveys three times during the semester. We present results from the subset of students who were Industrial and Systems Engineering or Industrial Engineering Technology students. We share students' self-reported confidence in major choice (beginning and end of course), impact of the orientation course on major choice, confidence at the end of the semester, and reflections on the course content and structure. We also present results related to major changes for the full set of students.",1,Ansley | Renaissance Waverly Hotel,"Orientation, Mentoring and Career Transition",26
84,5961.0,Purdue Industrial Engineering Mentoring Programs,Academician,"The Edwardson School of Industrial Engineering at Purdue University has developed a series of formal mentoring initiatives over the past eight years aimed to foster professional development and alumni engagement through targeted mentoring opportunities. The IE Two-Way Mentoring Program is a semester-long program designed to foster professional networking opportunities between current Purdue IE students and alumni through one-to-one mentorship. It was established in partnership between the School and the Purdue IISE student chapter in 2017 and features bi-weekly meetings between alumni and students to discuss various topics. Before each two-week period, the IISE Mentorship Director distributes an email suggesting a theme for that meeting to help drive productive discussion, but if other topics emerge that are more relevant to the mentor/mentee pairings they are free to go wherever those lead. To date, we have nearly 250 mentor pairings. The IE 20000 Peer-Mentoring Program is a semester-long, peer-to-peer group mentoring initiative focused on helping sophomores through their transition into IE from First Year Engineering by connecting them with upperclassmen. Establish in the Spring 2021 semester, this program helps both new and seasoned IE students build skills, understand more about IE, and make connections within the School that expedites their academic and career development. Given the success of these programs, we would like to share best practices and lessons learned with the broader IE community.",2,Ansley | Renaissance Waverly Hotel,"Orientation, Mentoring and Career Transition",26
85,5378.0,Mentoring Matters: A Comprehensive Mentoring Program for Undergradutes,Academician,"Mentoring Matters is an annual departmental program that educates and engages undergraduate students through mentoring relationships with peers, alumni, and faculty. A variety of programs and events are offered throughout the year. Fall Quarter Alumni Connection All IEMS majors are invited to participate in a one-time mentoring conversation with IEMS alumni. Students and alumni are matched by indicating their interests and preferences via a short survey. Interview Coaching Students are invited to meet with a member of the faculty for a brief, focused coaching conversation. Winter Quarter Educational Session Students learn about the value of mentoring and how to become involved formally and informally. Peer Mentor Matches/Meetings Students are matched with Peer Mentors for focused conversations around courses, interests, and career interests. Spring Quarter Alumni Mentor Matches/Meetings Students are matched with Alumni Mentors for focused conversations around career choice, interview prep, and industry. Faculty Mentor Matches/Meetings Select students will be invited to submit three choices for Faculty Mentor matches. These students will be recommended by their Peer Mentor match. Students will be notified of their match and invited to arrange a mentoring conversation with their Faculty Mentor. Career Development Sessions Topics include resume development, interview preparation, and preparing for day-one in an internship or full-time role. End of the Year Celebration Luncheon A wonderful event to celebrate mentoring including an esteemed speaker, distribution of completion certificates, and IEMS merch!",3,Ansley | Renaissance Waverly Hotel,"Orientation, Mentoring and Career Transition",26
86,5202.0,Advice to Graduating Students Their First Year Working,Practitioner,"This presentation will offer some advice to graduating students applicable to their first year of working. It covers the following topics: Current Job, Volunteering, Project Management skills, Personal skills, In-house Training, Professional Societies, Career Planning, and Other Activities.",4,Ansley | Renaissance Waverly Hotel,"Orientation, Mentoring and Career Transition",26
87,9050.0,Every Drop Counts: A Water Waste Lean & Green Experiential Activity,Academician,"By implementing a water-use experiential exercise in operations or engineering management courses, student learning outcomes associated with traditional lean systems metrics can be linked to other challenges that are more often related to social and environmental impact. In this exercise, students are challenged to reflect on their own sustainability attitudes and behaviors by measuring their wasted water. The exercise is adaptable for online course delivery, can link to other key programmatic learning outcomes, and is flexible for a variety of high-level sustainability frameworks. It provides an approachable learning opportunity to discuss social and environmental waste challenges, directly discusses regional water cost and availability, and supports synergy to address multiple possible sustainability metrics from other holistic frameworks such as the UN 17 Sustainable Development Goals (e.g. Goal #6: Clean Water and Sanitation), ERS, CSR, ESG, etc. Additionally, students learn about local, regional, and global water challenges, and evaluate the impact of water waste, water-use choices, design tradeoffs, and long-term sustainability of water resources.",1,Ansley | Renaissance Waverly Hotel,ISE Education and Sustainability,27
88,8486.0,Integrating sustainability into an engineering entrepreneurship course,Academician,"The next generation of engineers must be aware of the environmental and social impact of their products and be trained with skills to handle sustainability issues in their design to achieve a sustainable future. Unfortunately, sustainability is usually introduced as a separate topic outside the technical training an engineering student will receive at a university which often results in engineering students considering sustainability in their design as an afterthought. This research aims to integrate sustainability into an engineering entrepreneurship course that prepare students to understand social and environmental impacts and constraints and are equipped with knowledge and green skills and tools that allow them to design and make products and solutions for a sustainable future. Working with a local hunger relief organization, students were presented with real world problems and developed innovative sustainable solutions for their business ideas. A pre- and post-survey was conducted to assess the project and a statistically significant improvement in students’ sustainability awareness and knowledge was achieved. The findings of this research can serve as an example for other engineering courses.",2,Ansley | Renaissance Waverly Hotel,ISE Education and Sustainability,27
89,6544.0,Driving Change from Within: Industrial Engineering Alumni Intrapreneurs Leading the Way to Sustainability,Practitioner,"This study explores how alumni from diverse industries are acting as intrapreneurs, spearheading sustainability initiatives within their respective companies. As organizations increasingly prioritize environmental responsibility, these alumni are leveraging their expertise in fields ranging from manufacturing and supply chain management to finance and technology to drive impactful, sustainable practices from within. Through structured surveys with alumni across various sectors, this research identifies key strategies, challenges, and innovations that companies are implementing to reduce their environmental footprint and promote long-term sustainability. Survey findings reveal a range of sustainability approaches, including waste reduction, energy efficiency, sustainable sourcing, and investments in green technologies. Alumni in logistics report optimizing routes to cut carbon emissions, while those in manufacturing highlight process improvements that reduce waste and conserve resources. Finance professionals observe a shift towards ESG (environmental, social, and governance) investment criteria, reflecting a broader commitment to sustainable growth. Meanwhile, alumni in tech roles are championing data-driven tools to track and enhance sustainability metrics. This study underscores the unique role of alumni as intrapreneurs—using their influence and innovation within their companies to champion environmental goals. Their efforts demonstrate the practical application of academic knowledge in advancing corporate sustainability objectives. This conference presentation will provide insights into the evolving practices across industries, emphasizing how alumni are bridging green goals with real-world challenges, driving meaningful change from within their organizations.",3,Ansley | Renaissance Waverly Hotel,ISE Education and Sustainability,27
90,5707.0,Matriculation & Retention Rate Improvement Study of Engineering & Engineering Technology students,Academician,"Background: Recent literature in engineering education has focused on many aspects of why students choose their majors. Little has explicitly focused on not only choosing an engineering major, but also how to retain students by identifying obstacles toward graduation. Purpose: There is a concern about a decrease in enrollment of engineering students at this university recently. The purpose of this paper is to discover how undergraduate students chose their engineering major, why they keep their major, why they switch their major, and why they leave the university. The cause of switching majors or leaving the university could be due to internal circumstances which could be avoided. For retention purposes, we prefer students to graduate with an undergraduate engineering or engineering technology degree at this institution. Methods: This is a quantitative study utilizing an online survey instrument on 4,477 undergraduate engineering students at one U.S. state university. The survey will ask for demographic information with specific questions about their choice of an engineering or engineering technology major. Freshmen through seniors will be the target audience of this study. The following research questions will be explored: RQ1) what factors influence students to study engineering, RQ2) what factors influence students to switch engineering majors, RQ3) what factors are perceived as obstacles toward graduation. Conclusion: To better understand the factors that help guide students toward graduation and discover how to keep students in engineering. We seek to find out what is entailed in this initial decision-making process, and perhaps serve to benefit future students.",1,Ansley | Renaissance Waverly Hotel,Broadening Horizons in Industrial and Systems Engineering,28
91,5524.0,Expanding the UNIVERSE: Understanding Industrial Engineering VERSatility for Educators – A Follow-Up,Academician,"According to the U.S. Bureau of Labor and Statistics, demand for industrial engineers will steadily increase over the next decade. However, many IE programs, similar to other engineering programs, are experiencing enrollment declines or reduced growth, meaning too few qualified IE professionals will be available to the workforce. At the same time, too few K-12 students understand industrial engineering as a career option. We know that K-12 teachers have an enormous impact on students, including student confidence in stem subjects and the career paths that students choose. To address this gap between supply and demand, we created a state-approved, hands-on continuing education course for K-12 educators that includes a full day in our teaching labs. The course raises teacher awareness of IE and helps them learn how to effectively teach IE principles to their own students. Knowledgeable K-12 teachers can incorporate IE content into their curriculum for 20-150 students every year, significantly increasing exposure to IE concepts before post-secondary school. Our inaugural K-12 course was held in May/June 2024. This paper describes course results, including content and format, pre/post survey results, lessons learned, teacher feedback, and changes for future offerings. After taking the course, K-12 teachers felt more confident about incorporating IE in their own classrooms and expressed their intent to do so. Offering this course and others like it could generate exponential influence, introducing IE to increased numbers of students every year, sparking interest, familiarity, and confidence that can lead to more students pursuing IE degrees and careers.",2,Ansley | Renaissance Waverly Hotel,Broadening Horizons in Industrial and Systems Engineering,28
92,6982.0,Educational Module for Demonstrating ISE Applications in Elections to K-12 Students,Academician,"This paper explores new education outreach modules developed through a partnership between the URI Navy STEM Coalition, the Engineering for Democracy Institute (EDI), and the Guiding Education in Math and Science Network (GEMS-Net). The URI Navy STEM Coalition has worked with GEMS-Net educators to create modules that translate current university research into lessons that can be easily adopted by K-12 teachers in the classroom, including links to national Next-Generation Science Standards (NGSS) for sciences and engineering as well as social studies standards for Rhode Island and Connecticut. The Engineering for Democracy Institute applies data-informed engineering methods to election science to improve the efficiency and performance of voting systems. The developed modules appeal to both STEM and social studies educators by including four primary tasks: (1) Wait Times at an In-Person Polling Location, (2) Facility Layout Design of an In-Person Polling Location, (3) Analyzing Vote Center Locations, and (4) Vote-by-Mail Ballot Processing Analysis. By scaffolding different topics, the worksheets break down the research applications, making the material more complex as students become more familiar with key terms and concepts. Each task is designed to get students to work through small-scale optimization techniques and principles of systematic layout planning. The project demonstrates the power of applying core research areas of industrial and systems engineering to election systems. The goal is for K-12 teachers and students to gain an expanded view of how industrial and systems engineering, as a discipline, can enhance a broad range of systems that impact communities and government.",3,Ansley | Renaissance Waverly Hotel,Broadening Horizons in Industrial and Systems Engineering,28
93,6538.0,Bringing Finance and Cost Accounting to Life: Real-World Project Implementation,Practitioner,"This case study examines the application of financial and cost accounting tools in a service-based business, specifically focusing on an ice cream business. The project, undertaken by students in a Finance and Cost Accounting course, required them to analyze financial transactions, assess budgets and variances, and develop cost-reduction strategies. The case identified two primary areas for potential savings: the cost of logo stickers and the use of liquid nitrogen, both of which were central to the business’s operations. The students explored two cost-saving alternatives. First, they proposed increasing the volume of liquid nitrogen purchased in order to qualify for volume discounts, thus reducing the per-unit cost. Second, they examined supplier options for logo stickers, successfully negotiating a reduced price with a preferred supplier. Both alternatives were evaluated based on their potential financial impact, and each was justified with respect to its contribution to cost reduction and long-term sustainability. The students presented their findings and recommendations to the business owner, providing a financial justification for each proposed strategy. The project not only demonstrated the practical application of cost accounting principles but also highlighted the importance of strategic decision-making in managing a small service business. Ultimately, the case study provided valuable insights into how cost accounting techniques can be employed to optimize operational efficiency and profitability in a real-world business context.",4,Ansley | Renaissance Waverly Hotel,Broadening Horizons in Industrial and Systems Engineering,28
94,6975.0,Linking Culture and Engineering Identity Development: A Conceptual Model,,"In the area of engineering education academic and professional persistence are linked to a strong identity. Research in this area, for several years, has focused on a particular aspect of engineering education focusing on professional identity formation. Identity formation is influenced by culture because societal beliefs, norms and rituals shape an individual’s self and way of thinking. Therefore, it is important to continuously improve engineering education by equipping students with the best ways of identity formation in their chosen profession. This goal can only be achieved if a clear understanding is established on how cultural factors influence engineering identity development. It is difficult to measure both engineering identity and culture, because they are both complex and multifaceted processes. The different characteristics that comprise culture can be analyzed with the help of theoretical cultural frameworks. This paper provides a summary of various cultural frameworks, such as Edward T. Hall’s three cultural dimensions and Geert Hofstede’s six cultural dimensions. Additionally, existing concepts of engineering identity and its formation are examined to initiate the integration of these cultural frameworks and cultural survey methodologies to develop a conceptual model for culturally centered engineering identity formation schemas. This conceptual model assists in linking culture and engineering identity formation. This paper provides an overview of the crucial cultural aspects that help to gain a better knowledge of professional identity development. This study is suited for both practitioners and researchers in industrial engineering, technical management, and engineering education",1,Ansley | Renaissance Waverly Hotel,Identity Formation and Communication in the ISE Curriculum,29
95,8643.0,"Systems Theory Archetypes in UX Journey Mapping for Identity Formation, Understanding, and Analysis",Academician,"Professional identity formation is a complex and developing field of study. Current research work in studying the formation of identity has been explored in a myriad of environments. Such areas as student development, workplace identity, identity within a profession, etc. have been the subject of study for some time now. More intricate inquiries such as how to develop a researcher identity in doctoral students are being explored. Documenting researcher identity development is critical for designing student-centered doctoral programs. A critical part of the development of doctoral students is learning how to become a researcher, which is an uneasy transition. Current studies to properly analyze identity development have employed user experience (UX) methods such as journey mapping, which are invaluable in capturing the complexity and richness in this multifaceted landscape. UX mapping methods allow for the development of identity “personas,” which are composite descriptors forming a fictional character representative of target users. Personas assist in framing a deeper understanding and empathy with the target audience. This UX approach creates several personas to capture the sample audience being serviced. Combining UX personas with Systems Theory Archetypes, expands research in this area to better reflect the repeating patterns of user needs, creating a clear reference point which assists user-focused decisions in the educational curricular design process and mentorship development of the emerging scholar. This research presents the developing work on systemic archetypes for researcher identity personas to assist in doctoral scholars’ formation. Examples are provided to illustrate the archetypical characteristics of the personas.",2,Ansley | Renaissance Waverly Hotel,Identity Formation and Communication in the ISE Curriculum,29
96,5406.0,Improving Student Communication Skills: Coaching and Connections,Academician,"The senior design capstone course at the H. Milton Stewart School of Industrial and Systems Engineering requires students to develop business/technical written and oral reports that update the faculty and clients throughout the semester on the progress students are making with their design projects. Through these reports and presentations, students must explain, justify, and substantiate their projects, including such information as the systems involved, the problems being solved, the solutions, the solution methodologies, and the value to the client. While many engineering schools require students to take technical communication courses, Georgia Tech’s Stewart School does not. This presentation will outline how we approach the development of communication skills, focusing specifically on team coaching sessions, required video content, and storytelling.",3,Ansley | Renaissance Waverly Hotel,Identity Formation and Communication in the ISE Curriculum,29
97,6565.0,Undergraduate Engineering Students Find Meaningfulness in Statistics Technical Writing Project,Academician,"Technical writing is a vital aspect of engineering education and practice. Technical writing can serve a dual purpose in the pedagogy of complex topics by facilitating students’ understanding and deepening comprehension through communication. This study analyzed a technical writing project designed to teach basic statistical literacy and technical writing skills in an undergraduate engineering probability and statistics course. A proctored student survey was conducted at the end of the semester and subsequently analyzed to determine the project’s meaningfulness and influencing factors to students. Meaningful writing can be experienced when an assignment goes beyond academic specifics and takes an ongoing significance for a student in their future careers or passions. Over 87% of students indicated that they found the project meaningful, as it provided an opportunity to learn the course material (p=0.018). Students also indicated the project was applicable to their fields as well as increased confidence for future writing projects in their professional careers (p=0.012). These findings support technical writing as a meaningful dimension and productive learning experience for students in engineering pedagogy and specifically in statistical literacy.",4,Ansley | Renaissance Waverly Hotel,Identity Formation and Communication in the ISE Curriculum,29
98,8938.0,"""Engineering Education in the Age of Generative AI: A Literature Review""",Academician,"As GenAI technologies become more prevalent in education-related environments, their effects on how subjects are taught/discussed (teaching approaches), the engagement level (interest) from students to learn the content/domain material and skills needed require systematic exploration. This literature review examines the potential of Generative Artificial Intelligence (GenAI) as a transformational force in engineering education while also addressing its challenges by examining trends, legitimacy of GenAI in learning pedagogy, its influence on thought processes concerning cognitive and creative domains. The review highlights important gaps in the literature such as (1) research supporting claims that GenAI will significantly impact student learning outcomes and skill acquisition in any beneficial way; (2) the absence of frameworks for implementing GenAI into engineering curricula in ways aligning with both accreditation standards while also addressing industry needs; (3) an urgent need to pursue strategies for addressing ethical, equity, and academic integrity challenges raised by GenAI adoption; and (4) only limited reporting on activities aimed at improving faculty readiness to leverage appropriate use of GenAI tools. In filling these gaps, this study aims to provide recommendations for educators,and researchers who seek to address the challenges of exploiting GenAI to improve engineering education. These results will inform dialogue around the future of engineering education in an age of artificially intelligent tools, along pathways whereby creative output and innovation can thrive within common standards of rigor and inclusivity as academic practice.",1,Ansley | Renaissance Waverly Hotel,AI in Engineering Education,30
99,6240.0,Faculty Perceptions on the Use of Generative AI in Engineering Education,Academician,"Generative AI is a type of artificial intelligence capable of creating content like text, images, or music based on user prompts or inputs. Generative AI models are trained on large amounts of data and use smart algorithms to create things that appear to be made by humans. This study aims to develop a comprehensive understanding of Generative AI’s current and potential impact on engineering education from the perspective of university faculty and to compare these findings with the findings of two related studies – from the perspective of students and employers. In particular, we aim to explore faculty perceptions of Generative AI’s usefulness, faculty’s intent to use vs. actual use, how it has been integrated into the classroom (i.e., types of assignments or activities), faculty’s attitudes towards its use by student for assignment (whether it’s considered as cheating), and any concerns or challenges with adopting Generative AI in engineering education. We will employ mixed methods statistical analysis techniques such as descriptive and inferential statistics, as well as comparative analysis, to identify patterns, trends, relationships, and contrasts with respect to faculty perceptions of using Generative AI in teaching engineering courses. The survey results will be tabulated to provide insights of faculty perceptions on the use of Generative AI.",2,Ansley | Renaissance Waverly Hotel,AI in Engineering Education,30
100,6702.0,Enhancing Student Engagement with AI-Driven Icebreaking Techniques in Engineering Courses,Academician,"Icebreaking activities are crucial in engineering courses to foster a collaborative and engaging learning environment. This study explores innovative ice-breaking techniques utilizing artificial intelligence (AI) designed to boost student engagement across both online and in-person course formats. The first technique involves students creating avatars that reflect their personalities and hobbies, which are then shared online. This activity promotes a sense of community and belonging, encouraging creative self-expression. The second technique is also conducted on the first day of class after introducing the course content. In groups, students create a song about the course using a free online AI tool based on what they learned from the introduction. These approaches help students become familiar with AI while engaging with the material in a unique and imaginative way, leading to discussions on AI usage and ethics. Feedback from students and instructors indicates that these methods effectively enhance participation and enjoyment. Findings suggest that incorporating AI-driven activities can make the learning experience more interactive and enjoyable, enriching student engagement.",3,Ansley | Renaissance Waverly Hotel,AI in Engineering Education,30
101,6620.0,The Impact of Generative AI on Engineering Education: Insights into Student Experiences and Perceptions,Academician,"The rapid evolution of artificial intelligence (AI), particularly generative AI, is reshaping engineering education. This study, part of a broader investigation into stakeholder perceptions of generative AI in engineering education, examines engineering students’ experiences with generative AI, focusing on their perceptions, usage patterns, benefits, challenges, and career expectations. Using a cross-sectional survey based on the Technology Acceptance Model (TAM) and supplemented with critical questions, we assess students' perceived usefulness, ease of use, intentions, and actual utilization of AI. Preliminary findings reveal a generally positive attitude among students, with many acknowledging that generative AI supports learning, enhances creativity, and aids in academic tasks like writing. Common applications include brainstorming and writing assistance. However, ethical concerns are significant, with over half of respondents 56% viewing AI usage as a form of cheating, while the remaining students are either undecided or do not consider it cheating. In terms of professional development, students perceive AI as a tool that can enhance adaptability, technical skills, and digital literacy, competencies they believe are increasingly valuable in the engineering field. However, they also report challenges with AI integration, citing issues around the reliability and accuracy of AI tools, ethical concerns, and the complexities of incorporating AI seamlessly within the structure of traditional engineering curricula. By tailoring strategies for generative AI integration and addressing students' unique needs and concerns, educational institutions can better align the role of generative AI with students' academic and career goals, preparing them for a workforce increasingly shaped by AI.",4,Ansley | Renaissance Waverly Hotel,AI in Engineering Education,30
102,8966.0,"Work Design, Time Studies, and Category Theory: The case for MTM-2 as a Category Theory Teaching Tool",Academician,"Category Theory is an abstract branch of mathematics that facilitates the interaction between multiple areas of mathematics without requiring a deep understanding of the specific details of each mathematical field. Recently, there has been a growing interest in using category theory as a rigorous mathematical method for practitioners to analyze the state space and dynamics of complex socio-technical systems. However, there is currently no readily graspable way for undergraduate students to learn category theory, as it is primarily taught in graduate-level mathematics programs. This research aims to enhance the category theory education of undergraduate industrial engineering students by employing the fundamental work design method MTM-2, alongside the string diagram modeling tool from category theory. By utilizing these essential industrial engineering disciplinary tools, undergraduate industrial engineering students will be able to: 1) Apply fundamental educational outcomes to create mathematically rigorous systemic representations, 2) Develop representations of both concrete and conceptual components required for successful work execution, and 3) Analyze the necessary resource flows that either facilitate or impede successful work execution. Future efforts in this curriculum development will apply these techniques and principles within work design curricula to identify potential barriers to learning and execution.",1,Ansley | Renaissance Waverly Hotel,Innovations in Engineering Education,31
103,7088.0,Teaching the Entrepreneurial Product Engineering Process with an emphasis on student led startups,Academician,"Many of the dominant technologies that we use every day were developed by technically minded college students with a curiousity and hands-on drive to learn. Entrepreneurial Product Development involves the experimental evaluation and validation of how new technologies can solve existing problems. Many of these new technologies are reasonably low cost and readily available at engineering schools. Therefore, producing the next generation of new technology companies means that we need to introduce, educate and instill an entrepreneurial skillset and mindset among a qualified group of students who have the willingness and drive to create valuable solutions. Success is ultimately achieved when a skilled and driven student is paired with a viable value proposition involving a “real” problem that can be solved by a “viable” technology with a low cost of implementation. This course design objective is to introduce the value of entrepreneurial and intrapreneurial work and life experiences as contrasted to traditional problem solving and engineering career paths. Students are provided an introduction to recent new low cost technologies which have been the foundation of recent college invention startups. Next, students are required to evaluate a list of real problems provided by industry partners as well as consider their personal and COOP/Intern challenges. Finally, students select the technology approach for the problem and systematically develop and pitch their Minimum Viable Product to their Smallest Viable Market.",2,Ansley | Renaissance Waverly Hotel,Innovations in Engineering Education,31
104,6833.0,A Structured Flowchart Approach to Teaching Inventory Models,Academician,"Inventory Analysis and Modeling is an important prerequisite knowledge for many other IE courses. Inventory models are usually presented to students from deterministic to more complicated stochastic models, and build on the skills and concepts from previous simpler models. Based on course assessments from the past few years, once all models were covered, students have difficulty selecting the correct model and understanding the inputs needed to build the correct inventory model for a given situation. To bridge this gap, an overarching framework on how to build inventory models is needed. We have developed a structured approach that presents a framework for presenting the concepts on inventory models such that students gain a systemic view of the models – the eventual goal being better retention and proper use of these models by IE students. In this paper, we will present our preliminary results from implementing our approach across two institutions and four semesters.",3,Ansley | Renaissance Waverly Hotel,Innovations in Engineering Education,31
105,7067.0,Reimagining Supply Chain Engineering and Management Learning: An Interactive Simulation Tool,Academician,"As supply chains grow more complex and interconnected, there is an increasing need for educational tools that prepare students for the realities of managing global networks. Traditional education tools like the Beer Game have illustrated fundamental supply chain dynamics, particularly the bullwhip effect, and highlighted the importance of coordination among supply chain actors. However, today’s supply chains face far more intricate challenges, including multi-level fulfillment networks, fluctuating lead times, growing customer expectations, and frequent disruptions. To bridge this gap, we developed a comprehensive, interactive supply chain simulator that integrates contemporary complexities of global supply chains. Our simulator advances beyond previous tools by offering a multi-facility network that allows users to explore interactions across assembly factories, distribution centers, and fulfillment centers within a highly configurable and realistic framework. It incorporates autonomous operational decisions, demand-driven replenishment, and the ability to dynamically adjust planning parameters, giving students a hands-on experience with modern supply chain engineering and management concepts. With capabilities for descriptive, predictive, and prescriptive analytics, students are empowered to experiment with diverse configurations, test replenishment strategies, and observe the impact of decisions on critical metrics such as market penetration, demand fulfillment, and profit margins. Our contribution fills a critical need in supply chain education by providing a simulation that reflects the envisioned complexities of future networks, and facilitates data-driven decision-making. Preliminary results from user testing indicate that this simulator significantly enhances students' understanding and analytical skills, better preparing them to tackle real-world supply chain challenges with confidence.",4,Ansley | Renaissance Waverly Hotel,Innovations in Engineering Education,31
106,5099.0,Measuring Distributed Situation Awareness In Sociotechnical Systems to Enhance Layout Design and Performance,Practitioner,"Many systems involve dynamic interactions between human and non-human elements to achieve shared objectives, a concept known as Distributed Situation Awareness (DSA). These sociotechnical systems emphasize the collaborative interplay between social and technical elements. Network analysis can be used to represent communication and information exchanges among system components, making it easier to visualize interconnections within the system. Using the Event Analysis of Systemic Teamwork (EAST) Framework, this study constructs three networks—social, knowledge, and task networks—to represent the relationships within the system. From a design standpoint, organizing the layout based on these relationships between the agents of the system can improve DSA and information transactions. Specifically, utilizing social network analysis measures such as closeness, modularity, and others, can provide useful context regarding the level of DSA within the system. This can further help in designing a layout to locate key elements or information units with stronger links closer together. This study will utilize simulation modeling to design and assess an optimized layout, comparing performance between current and redesigned layouts. While the research focuses on a Dermatology Department, the approach can be generalized to other sociotechnical systems. Ultimately, this research aims to quantify and improve DSA through spatial layout optimization based on communication dynamics.",1,Fulton | Renaissance Waverly Hotel,"Computational Approaches to Layout, Scheduling, and Strategic Infrastructure Planning",32
107,5764.0,Solving the Unequal Area Dynamic Facility Layout Problem Using a Zone-Based Structure with a Two-Phase Matheuristic Approach,Academician,"The Dynamic Facility Layout Problem (DFLP) is designing a facility over a multi-period planning horizon where the interdepartmental material flows change over the planning periods due to frequent changes in product demands. The DFLP in the continuous plane is a very challenging nonlinear optimization problem. This paper considers solving the unequal area Dynamic Facility Layout Problem (DFLP) using a zone-based structure. Zone-based layouts have significant advantages, such as being easily transferable to a detailed layout with innately included possible aisle structures; therefore, they can be fitted to the unique needs of the layout designers. The unequal area DFLP is modeled and solved using a zone-based structure, which is referred to as ZDFLP, where the dimensions of the departments and material handling system input/output (I/O) points are decision variables. A two-phase matheuristic, which directly operates on Problem ZDFLP without requiring an encoding scheme of the problem, is proposed to solve the ZDFLP with promising results.",2,Fulton | Renaissance Waverly Hotel,"Computational Approaches to Layout, Scheduling, and Strategic Infrastructure Planning",32
108,5968.0,Dynamic Graph-Based Reinforcement Learning for Efficient Scheduling in Large-Scale Flexible Job Shops,Academician,"The Flexible Job Shop Scheduling Problem (FJSP) is an NP-hard optimization challenge with significant industrial applications, especially for large-scale instances. Traditional approaches, such as Priority Dispatching Rules (PDRs), often struggle with time-intensive design processes and suboptimal performance as problem size increases. This paper introduces a scalable, dynamic graph-based reinforcement learning framework designed to address large-scale FJSP efficiently. By modeling FJSP as a dynamic graph and filtering out irrelevant operations, our approach reformulates the scheduling task as a Markov decision process (MDP). To capture complex task dependencies and machine constraints, we utilize a Heterogeneous Graph Attention Network (HGAT), while Proximal Policy Optimization (PPO) is employed to drive the reinforcement learning process, dynamically selecting actions that optimize makespan. Additionally, we introduce a multi-action selection strategy to further improve computational efficiency, enabling faster scheduling without compromising solution quality. Preliminary evaluations demonstrate that our method surpasses traditional heuristics and leading algorithms, achieving competitive makespan results and proving its robustness for large-scale scheduling needs. This approach offers a practical, scalable solution for real-world FJSP challenges.",3,Fulton | Renaissance Waverly Hotel,"Computational Approaches to Layout, Scheduling, and Strategic Infrastructure Planning",32
109,8913.0,Spaceport Location Planning in the Continental United States,Practitioner,"The burgeoning commercial space transportation industry necessitates an expansion of launch infrastructure to meet rising demands. To rigorously reason about where such future spaceports might be located and what their impacts might be, we introduce a facility location planning model for future US spaceports (SPFLP). Central considerations for the SPFLP include population density, space launch trajectories, and potential impacts to air traffic. The SPFLP outputs a cost-optimal set of candidate locations for future spaceports while satisfying a range of operational constraints. By conducting sensitivity analyses on the SPFLP, we are able to examine differences in flight rerouting costs and optimal launch mission allocations. Our model and numerical experiments offer valuable insights for future spaceport site selection, contributing to the strategic development of commercial space transportation while keeping in mind the need to integrate these operations within the NAS. Additionally, as commercial and governmental space transportation become more widespread, the effect of natural disasters on space launch infrastructure also grows more pronounced. Although we began with a deterministic facility location planning model, we then implement a chance-constrained (CC) version to address the stochastic nature of natural disasters. We base our probability distributions for natural disaster occurrences on annual frequency data, which supports the formulation of our Chance-Constrained Spaceport Facility Location Planning (CC-SPFLP) model.",4,Fulton | Renaissance Waverly Hotel,"Computational Approaches to Layout, Scheduling, and Strategic Infrastructure Planning",32
110,5523.0,Applying Lean approaches to improve warehouse space utilization   ensuring employee ergonomics.,Academician,"Lean tool 5S combined with ABC part analysis, have been proven effective for organizing workspaces resulting productivity improvement in assembly and other production lines. Such applications also improve warehousing of parts, when number of parts are very high, and storage space is limited. Since proper warehousing claims space for all parts to meet production goals, facilitate the ease of part kitting and delivery to the assembly line, creating an efficient and well-organized process to control the flow of parts from the receiving dock to the warehouse, and to the assembly line as required to meet customer build expectations. As such, this research integrates Lean tool 5S, PFEP (Plan For Every Part), and ABC based organizing the parts in the warehouse zones, for optimizing space utilization and warehousing performance (picking, kitting, and delivery to assembly lines). The Doosan-Bobcat factory in NC stocks a huge number of parts in their warehouse to support their more than sixteen assembly lines, building more than one hundred of products. With new products and lines planned for the immediate future, the warehouse space utilization is planned and designed by combing 5S, PFEP, and ABC parts assessment, resulting in a Lean based zoning system optimizing space utilization and warehousing performance for the future. This research seeks to develop a system for the warehousing beginning in the pilot area, which will then be mirrored in all other areas of the warehouse, and will be offered as a standard process for other Doosan-Bobcat facilities around the globe.",5,Fulton | Renaissance Waverly Hotel,"Computational Approaches to Layout, Scheduling, and Strategic Infrastructure Planning",32
111,5762.0,Nonparametric Input-Output Uncertainty Comparisons,Academician,"We consider the problem of inferring the system with the best simulation output mean among k systems when the simulation model is subject to input uncertainty caused by estimated common input models from finite data. The Input-Output Uncertainty Comparisons (IOU-C) procedure is designed to return a set of solutions that contains the best solution with an asymptotic probability guarantee when parametric input models are adopted. We extend this framework to nonparametric IOU-C (NIOU-C) when empirical distributions of the data are adopted as input models. Representing the simulation output mean of each system as a functional of the common empirical distributions via the functional Taylor series expansion, we propose two methods that rely on the nonparametric delta method and an ambiguity set formulation, respectively. We provide statistical analysis showing that our methods asymptotically achieve the desired probability guarantee. We provide numerical examples to test the performance of our methods and show that they outperform the IOU-C.",1,Brayton | Renaissance Waverly Hotel,Optimization of Stochastic Systems,33
112,6070.0,Communication Efficient Decentralization for Smoothed Online Convex Optimization,Academician,"We study the multi-agent Smoothed Online Convex Optimization (SOCO) problem, where N agents interact through a communication graph. In each round, each agent i receives a strongly convex hitting cost function f t i in an online fashion and selects an action x t i in R d . The objective is to minimize the global cumulative cost, which includes the sum of individual hitting costs f t i (x t i ) , a temporal “switching cost” for changing decisions, and a spatial “dissimilarity cost” that penalizes deviations in decisions among neighboring agents. We propose the first decentralized algorithm for multi-agent SOCO and prove its asymptotic optimality. Our approach allows each agent to operate using only local information from its immediate neighbors in the graph. For finite-time performance, we establish that the optimality gap in competitive ratio decreases with the time horizon T and can be conveniently tuned based on the per-round computation available to each agent. Moreover, our results hold even when the communication graph changes arbitrarily and adaptively over time. Finally, we establish that the computational complexity per round depends only logarithmically on the number of agents and almost linearly on their degree within the graph, ensuring scalability for large-system implementations.",2,Brayton | Renaissance Waverly Hotel,Optimization of Stochastic Systems,33
113,6165.0,Policy Gradient Optimization for Markov Decision Processes with Epistemic Uncertainty and General Loss Functions,,"Motivated by many application problems, we consider Markov decision processes (MDPs) with a general loss function and unknown parameters. To mitigate the epistemic uncertainty associated with unknown parameters, we take a Bayesian approach to estimate the parameters from data and impose a coherent risk functional (with respect to the Bayesian posterior distribution) on the general loss function. Since this formulation usually does not satisfy the interchangeability principle, it does not admit Bellman equations and cannot be solved by approaches based on dynamic programming. Therefore, we develop a policy gradient optimization approach to address this problem. We utilize the dual representation of the coherent risk measure and extend the envelope theorem to derive the policy gradient. Our extension of the envelope theorem from the discrete case to the continuous case may be of independent interest. We then show the convergence of the proposed algorithm with a convergence rate of O(1/t), where t is the number of policy gradient iterations. We further extend our algorithm to an episodic setting, and establish the consistency of the extended algorithm and provide bounds on the number of iterations needed to achieve a constant error bound in each episode.",3,Brayton | Renaissance Waverly Hotel,Optimization of Stochastic Systems,33
114,6136.0,Optimal Pricing With Impatient Customers,Academician,"We investigate the optimal pricing strategy in a service-providing framework, where customers can become impatient and leave the system prior to completing service. In this setting, a price is quoted to an incoming customer based on the current number of customers in the system. When the quoted price is lower than the price the incoming customer is willing to pay (which follows a fixed probability distribution), then the customer joins the system and a reward equal to the quoted price is earned. A cost is incurred upon abandonment and a holding cost is incurred for customers waiting to be served. Our goal is to determine the optimal pricing policy that maximizes the long-run average revenue. Our analysis shows that under reasonable assumptions, the optimal quoted price increases as the number of customers in the system grows. Using this insight, we propose a monotone policy iteration algorithm that efficiently computes the optimal pricing policy. Moreover, we introduce two heuristics that simplify the optimal dynamic pricing policy. The cutoff static policy charges a fixed price until the number of customers in the system reaches a certain threshold. By selecting the threshold that maximizes the long-run average gain, the heuristic achieves near optimality. The two price policy charges one price when the arriving customer can be served immediately and another price if the customer needs to wait. This heuristic is always near optimal and provides more robustness compare to the cutoff static policy.",4,Brayton | Renaissance Waverly Hotel,Optimization of Stochastic Systems,33
115,5802.0,Modeling Flex-Route Transit Systems With a Continuous Approximation Markovian Approach,Academician,"This presentation is concerned with performance evaluation and design optimization of a flex-route public transit system. The system in question combines a fixed-route with on-demand flexibility, allowing deviations from the route for on-demand passenger pickups or dropoffs. We develop a combination of Continuous Approximation and Markov Process models of the flex-route system that provide for the calculation of key performance metrics for operators, such as the probability of being late and the expected delay. We introduce more realism to this system by considering a backtracking policy and dependence between passenger arrivals and bus delays. We derive stability conditions to characterize system performance and perform long-run analysis. Then, we use this model to assess the performance of a flex-route service for different system parameters, such as schedule time, the width of the deviation area, and the distance between checkpoints, under both transient and steady-state conditions, and validate the results using simulation. Finally, we benchmark the proposed approach against using the assumptions typically considered in the literature. We conclude that the proposed model offers a more accurate alternative to approximate system parameters, and thus, it offers a valid means for determining system parameters to achieve a desired service level.",1,Brayton | Renaissance Waverly Hotel,Applications of Stochastic Systems,34
116,6487.0,Design and Operation of Pickup and Dropoff Facilities for Passengers and Packages,Practitioner,"As the use of ride-hailing services and package deliveries have increased, the legal and illegal use of curb sides for these operations has increased. The use of the curb side by just one vehicle results in a long stretch of the traffic lane being blocked for regular traffic use, thereby increasing congestion. Also, curb side pickups and dropoffs increase the chances of accidents. Thus, there is an increasing need for well designed and well operated pickup and dropoff facilities. Existing design guidelines ignore conflicts between vehicle movements and the effects of congestion inside the facility. We describe models that incorporate conflicts between vehicle movements and congestion, and show how the models can be used to optimize facility design and operation.",2,Brayton | Renaissance Waverly Hotel,Applications of Stochastic Systems,34
117,6347.0,Optimizing Healthcare Worker Availability During Infectious Disease Outbreaks with an MDP Model: Simulation-Based Validation and Sensitivity Analysis for Workforce Resilience,,"Healthcare workers (HCWs) face elevated risks during infectious disease outbreaks, which can undermine their ability to provide essential care. To mitigate these risks, we develop a Markov Decision Process (MDP) model designed to optimize HCW availability by reducing infection exposure through realistic staffing policies. Unlike traditional Susceptible-Exposed-Infected- Recovered (SEIR) models, our approach focuses on generating implementable policies that enhance workforce resilience during outbreaks. We validate the model through a comprehensive simulation study, first confirming its alignment with findings from existing literature to establish credibility, and then assessing the practical effectiveness of the optimal policy obtained from the MDP model in a detailed simulation environment. Additionally, we examine the model’s sensitivity to varying infection probabilities and reward function coefficients, revealing valuable insights on the resulting number of new infections and workforce availability. By extending the model and simulations to incorporate different number of HCW groups, we further assess the model’s adaptability under diverse outbreak scenarios. The proposed MDP framework provides a robust, implementable approach for managing HCW availability, offering a critical tool for healthcare system resilience during infectious disease outbreaks.",3,Brayton | Renaissance Waverly Hotel,Applications of Stochastic Systems,34
118,5223.0,Optimal Control of Queues with Demand-Driven Discharge,Academician,"We consider a Markovian queueing system with finite buffer space. Arriving customers belong to different classes and have class dependent service rates. At the time of an arrival, if the system is full, one of the existing customers has to be discharged prematurely, incurring a class dependent cost, whereas class dependent rewards are earned upon successful service completions. Our objective is to determine which customer class to discharge prematurely in order to maximize the long-run average profit.",4,Brayton | Renaissance Waverly Hotel,Applications of Stochastic Systems,34
119,6030.0,Redeveloping an Optimization Model of a Resource Sharing Platform Using Nonprofit Stakeholder Feedback,Academician,"A resource sharing platform for nonprofits, known as SWAP, is a newly developed system currently being piloted by two cohorts of nonprofits based in Maryland. Using an integer program, SWAP enables participating nonprofits to exchange resources with each other. SWAP offers a unique solution that considers two factors: resources offered and wanted by nonprofits vary greatly, and participants are motivated to participate out of a mission to collectively make an impact in the communities they serve. Thus, to aid in the design of SWAP's resource exchange optimization models, we adopt Value Sensitive Design (VSD), a common methodology used to design technologies that captures the interactions of human values and system design. Using the VSD methodology, we identify the values important to nonprofits through a number of activities with our nonprofit cohort members. We then develop different optimization model formulations that capture each of the nonprofit’s identified values into the resource exchange problem. A series of activities with nonprofit stakeholders and computational experiments generate feedback on the formulations' abilities to capture the identified values. Finally, we combine the previous models to create a final resource exchange optimization model that captures the need for the SWAP system to consider multiple values when making resource exchange decisions.",1,Brayton | Renaissance Waverly Hotel,OR for Societal Good I,35
120,8718.0,Active Learning for Detecting Human Trafficking,Academician,"Human trafficking investigators face challenges while processing the sheer volume of publicly available online data. Natural language processing (NLP) models can assist in identifying evidence of exploitation in text data such as business reviews. However, the potential for NLP-based detection algorithms is hindered by the scarcity of large and accurately labeled training datasets. Labeling datasets related to human trafficking is difficult because identifying indicators of trafficking requires domain expertise, trafficking cases comprise a small portion of the data, and reviewing the disturbing content is emotionally demanding for individuals. Active learning optimizes model training by strategically querying the most informative data points, thereby achieving high accuracy with minimal annotations. We formulate active learning as a decision model and learn a policy through deep reinforcement learning. We evaluate this approach for the imbalanced classification task of detecting Yelp reviews of massage businesses that contain human trafficking risk factors. The active learning policy outperforms benchmarks according to the scoring metric used in training the classifier. Furthermore, it is more effective than uncertainty sampling in large batch query settings. The proposed approach can incorporate other scoring metrics and holds promise for addressing imbalanced NLP tasks where labeling requires significant time, domain expertise, and emotional effort.",2,Brayton | Renaissance Waverly Hotel,OR for Societal Good I,35
121,8679.0,"Strategic placement of drone platforms in out-of-hospital-cardiac-arrest: A case study for Jefferson County, Louisville, KY",,"Each year, over 350,000 out-of-hospital cardiac arrests (OHCAs) occur in the U.S., with survival rates as low as 8–10% due to delayed ambulance arrivals and limited use of publicly available Automated External Defibrillators (AEDs). Unmanned Aerial Vehicles (UAVs), or drones, present a transformative solution for rapid AED delivery to OHCA scenes, overcoming barriers like AED accessibility and bystander hesitation. Using Louisville’s EMS data, this study identifies priority areas for drone delivery and potential launch sites in Jefferson County, Louisville, KY. These inputs inform an optimization model to determine the minimum number of launch platforms needed to cover a specified percentage of intervention calls within a desired service time. We present results of the optimization model under different scenarios, by varying the priority areas, the number and location of the platforms, and the drone flight parameters.",3,Brayton | Renaissance Waverly Hotel,OR for Societal Good I,35
122,8768.0,A Robust Bi-level Network interdiction Problem with Applications in Human Trafficking Disruption,Academician,"We investigate a robust bi-level network interdiction problem motivated by applications in human trafficking disruption. In this problem, the follower, who operates the network, solves a minimum cost flow problem. The leader, who interdicts the network, minimizes the number of arcs from a special set with flow on them in the minimum cost flow solution obtained by the follower. The problem includes data uncertainty as the leader does not know the follower's cost vector but only that it belongs to a given uncertainty set. The follower has complete knowledge about its own parameters and therefore makes decisions in a ``wait-and-see'' fashion.",4,Brayton | Renaissance Waverly Hotel,OR for Societal Good I,35
123,5977.0,A Learning-based Queueing Approach to Measure Surgical Workload and Inform Case Prioritization Policies,Academician,"Surgical care delivery is a dynamic and uncertain environment where surgical cases exhibit heterogeneity regarding arrival patterns, duration, type, and complexity. Surgeons are critical resources with limited workload capacity, necessitating department-level practices to ensure that surgical case related workload is accurately measured using data. Currently, no objective methods exist to quantify surgical task-related workload. Additionally, extended pre-surgery waiting times show significant variation and impacts quality and safety of care. This study addresses the need to holistically quantify surgical task-related workload and address the tradeoff between pre-surgery waiting times and surgeon utilization by developing a novel workload scoring system that quantifies workload using surgery- and patient-specific attributes. We categorize surgeries into distinct workload types to capture variations in resource demands. Using clustering, we input unsupervised learning-based results into a non-preemptive priority-based queueing model to evaluate key performance metrics, including surgery waiting times (Wq), surgeon utilization (p), and queue length. We perform sensitivity analysis to evaluate the impact of hiring additional surgeons and adjusting surgery priority orders, focusing on prioritization policies for the lower-priority surgeries as an indicator of resource constraints. Our sensitivity analysis reveals two key insights: First, hiring an extra surgeon reduces Wq for the lowest-priority surgery by up to 82.5%. Second, maintaining p but adjusting the priority order reduces Wq by up to 42.5%. Our findings offer a structured framework for hospital administrators to develop data-driven policies that balance patient care with surgical operational demands. We also highlight the need for future research in workload management within healthcare settings.",1,Brayton | Renaissance Waverly Hotel,OR for Societal Good II,36
124,5882.0,Who Gets to Ride? Comparing Transportation Eligibility Policies for Saanich Public Schools,Academician,"In public school transportation, eligibility policies determine which students get to ride the bus, and which do not. Determining the operational cost of individual eligibility policies is an intractable problem and school districts must often make decisions with incomplete information. We collaborate with the Saanich School District in British Columbia, Canada, to develop methodologies to evaluate the costs and benefits of various eligibility policies. Our work involves both analyzing the current costs and benefits of the current policy as well as designing optimization models that maximize utility while keeping costs below a given threshold.",2,Brayton | Renaissance Waverly Hotel,OR for Societal Good II,36
125,8929.0,Impact of Collaborative Food Rescue Initiatives on Food Waste Reduction,,"In the current food systems landscape, the concept of food rescue has become a powerful force due to concerns about waste and its effects on the environment and society. This paper explores the collaborative efforts of various stakeholders, such as retail donors, food banks, for-profit organizations, and waste management entities, in addressing the complex challenges of food security, sustainability, and waste reduction. Through a comprehensive framework, surplus food is rescued from various stages of the supply chain and redirected towards meaningful purposes, which helps mitigate the environmental impact of food waste. However, despite these efforts, the global paradox of food waste alongside increasing food insecurity continues to exist. This study examines the dynamics of how food is allocated among stakeholders through Monte Carlo simulation in order to identify disparities and optimize the use of resources. By addressing disparities and improving collaboration, the paper aims to enhance the efficiency and sustainability of food rescue initiatives, ultimately reducing food waste and improving societal well-being. Key research questions explore the impact of for-profit food rescue on non-profit organizations, the influence of optimized food allocation on sustainability, and the role of contracts in fostering collaborative efforts to combat food waste.",3,Brayton | Renaissance Waverly Hotel,OR for Societal Good II,36
126,6698.0,A Stochastic Service Network Design Model for Disaster Logistics Planning: A Case Study for the State of South Carolina,Academician,"In this talk, we present a logistics planning problem focused on prepositioning essential relief commodities in anticipation of a hurricane landfall. We model the problem as a service network design model under the framework of two-stage stochastic programming with recourse. In the first stage, the model works to optimize the prepositioning of relief commodities for all periods, and in the second stage, it focuses on demand fulfillment, demand shortage levels, and transportation flows decisions. We assume that the hurricane’s evolution over time can be approximated as a Markov chain, where each Markovian state is characterized by the hurricane’s attributes (location and intensity). Demand quantities at each point are calculated based on these evolving attributes, allowing for more accurate scenario generation. To solve the model efficiently, we apply Benders decomposition for improved computational performance. Additionally, we implement a rolling horizon approach for adaptive decision-making as the hurricane’s forecasted path and intensity are updated over time. Our numerical results and sensitivity analyses based on a case study for the state of South Carolina demonstrate the effectiveness of this adaptive, scenario-based approach compared to a deterministic service network design model.",1,Brayton | Renaissance Waverly Hotel,Optimization in Transportation,37
127,8703.0,Optimizing Commodity Allocation and Stock Prepositioning for Army Disaster Readiness,Practitioner,"Ensuring rapid and efficient deployment of critical resources in the wake of a disaster is a paramount challenge for the U.S. Army, especially in contested environments like the Pacific Theater. This research focuses on optimizing Army Prepositioned Stock (APS) locations, commodity allocation, and sustainment planning to enhance disaster response capabilities. We develop a Mixed-integer Programming (MIP) model as the first step in a broader effort to develop agile and adaptable logistics solutions and analyses. The model considers hypothetical disaster scenarios to simulate the effects of varying APS locations, types of stored commodities, and demand fluctuations on operational readiness. Using a graph representation of the area of interest, our MIP seeks to optimize the placement and inventory levels at various APS locations to minimize response time, logistics costs, and risks while maintaining resilience across a disjoint environment. This research adopts an agile approach, allowing for continuous refinement of the MIP model to align with evolving Army logistics needs. The outcomes will provide decision-makers with actionable insights into the efficient allocation of resources, ultimately improving preparedness and response times in disaster scenarios.",2,Brayton | Renaissance Waverly Hotel,Optimization in Transportation,37
128,6486.0,Advance Shipment Reservation Problem,Practitioner,"This project addresses the challenges faced by companies that rely on outsourced carriers to ship goods to their customer: securing cost-effective, reliable capacity for on-time pickups. Without their own delivery fleets, these companies depend on contracted carriers to ensure competitive shipping rates for specific lanes. However, these contracts lack guaranteed capacity, as carriers serve multiple shippers and allocate capacity on a first-come, first-served basis. When capacity is fully utilized, shippers face two costly choices: sourcing higher-priced services from alternative carriers or the spot market, or risking delays that impact operational schedules and customer satisfaction. To reduce the likelihood of last-minute spot purchases, the shipper has to make advance capacity reservations. However, this strategy is complicated by demand uncertainty, as shippers must reserve space for orders that may not yet be finalized. This study develops a data-driven optimization model to establish an effective reservation policy, addressing uncertainties in carrier capacity and shipment demand. By leveraging historical data on demand patterns, carrier capacity, and rate fluctuations, the model suggests reservation quantities based on available information. It is designed to adapt as new information on demand and capacity becomes available, allowing the reservation policy to dynamically respond to changing conditions. This approach minimizes expected shipping costs while reducing the risk of capacity shortages and delays in scheduled pickups, providing companies with a flexible and responsive framework for managing outsourced shipping effectively.",3,Brayton | Renaissance Waverly Hotel,Optimization in Transportation,37
129,5170.0,Recursive Partitioning and Batching for Massive-scale Network Design with Service Time Guarantees,Academician,"Motivated by small package carriers in the United States, we study a service network design problem with service time guarantees at countrywide scale. The goal is to determine paths and schedules for commodities to minimize transportation and handling costs while ensuring committed service times. To solve an industrial-scale instance, which involves over 1,000 nodes, 1 million arcs, and 40,000 commodities, we propose a recursive graph partitioning and batching method. This method partitions the network into smaller regions and then solve the problem for each region, considering only commodities whose origin and destination are in the same region. For commodities crossing between regions, we first determine appropriate subnetworks and then solve integer programming models on these subnetworks. To handle the complexity from a large number of commodities, we divide the commodities into batches based on each commodity's flexibility and solve the problem iteratively. We demonstrate the scalability and efficiency of our approach through computational studies on real-world instances from an industry partner. Notably, our method found solutions in 13.2 hours under realistic scenarios, while a commercial solver was unable to even build a model for a much smaller instance.",4,Brayton | Renaissance Waverly Hotel,Optimization in Transportation,37
130,5649.0,Continuous Hub Location Problems with Random Service Provider and Receiver Positions,Academician,"We consider the problem of locating service hubs in a continuous area to minimize the expected minimum travel distance from a randomly located service provider to a randomly located customer or patient via exactly one hub facility. We discuss a two-phase simulation-optimization-based approximation method for solving this problem. In the first phase, we discretize the space and solve a large-scale -median problem using Benders’ decomposition. We then apply a simulation-optimization method to improve the solution quality over the continuous space. We extend the analysis to the case of multiple service providers to reflect real-life service settings. Finally, we illustrate our methodology using publicly available data for determining AED locations in Virginia Beach. The findings suggest that while increasing the number of AEDs is beneficial, we should emphasize public education to recruit a sufficient number of volunteer service providers.",1,Andover | Renaissance Waverly Hotel,Optimization Models for Logistics,38
131,6932.0,Genetic Algorithms in Order Picking Route Optimization: A Review of Advances and Implications for Logistics,Academician,"Efficient order picking route optimization is essential in warehousing and distribution, where reducing travel time and costs directly enhances productivity. Genetic Algorithm (GA), a robust metaheuristic tool, has shown significant promise in optimizing complex routing challenges within dynamic warehouse environments. This paper presents a systematic review of GA applications in order picking route optimization, aiming to consolidate existing knowledge and highlight areas for future exploration. By following the PRISMA approach for systematic literature review, we conducted a structured search across three major databases: ProQuest, Web of Science, and ScienceDirect. Keywords were refined through preliminary searches to ensure relevance, and the review process encompassed an initial search, Title and Abstract Screening, and Full-text review. Out of the 30 selected papers that underwent full-text analysis, the 10 most cited studies were examined in-depth to identify the latest advancements and trends in GA applications for route optimization. Our findings are expected to demonstrate GA’s effectiveness and flexibility in addressing diverse routing conditions and in outperforming traditional methods. By reviewing the practical implications and performance of GA in different scenarios, this paper aims to provide a resource for researchers and industry practitioners seeking efficient solutions for order picking. The results will also contribute to informed decision-making in logistics and supply chain management by offering insights into GA’s capability to optimize routing strategies, potentially driving both academic interest and industry adoption of GA in route planning and optimization.",2,Andover | Renaissance Waverly Hotel,Optimization Models for Logistics,38
132,6998.0,Privacy-preserving location sharing in crowd-sourced shipping,Academician,"In applications involving volunteer delivery of food and other supplies to resource-limited locations as well as crowd-shipping of deliveries from a depot, a central dispatcher relies on location sharing from crowd-shippers to design optimal matching of deliveries to potential crowd-shippers and volunteers. However, this poses privacy concerns in sharing sensitive location and destination information between the dispatcher and shippers that can disrupt the dispatch operations in the long run if crowd-shippers are not afforded reliable privacy protections. We investigate privacy-preserving dispatch algorithms that protect destination location information shared by the shippers while maintaining the overall performance and the quality of matched deliveries. We investigate the cost of privacy in both local and central privacy regimes, where the destination information is protected either at the user-level locally or the matched deliveries are randomized centrally at the dispatch to protect the location information. We investigate managerial insights and policy implications that determine who bears the cost of added privacy protections and how the compensations to crowd shippers should vary accordingly.",3,Andover | Renaissance Waverly Hotel,Optimization Models for Logistics,38
133,6597.0,"A Game-Theoretic and MCDA Framework for Sustainable Arctic Development: Balancing Economic Interests, Security, and Indigenous Rights",Academician,"The rapid economic development in the Arctic, driven by its natural resources and strategic significance, poses substantial challenges. This study integrates game theory and multi-criteria decision analysis (MCDA) to scientifically examine the interplay between economic growth and indigenous interests, enhancing the robustness and analytical rigor of decision-making. The research employs sequential and non-cooperative game theory to model strategic interactions among governments, corporations, and indigenous communities, while MCDA evaluates and prioritizes diverse criteria to assess policy impacts comprehensively. This combined approach aids policymakers in balancing economic objectives with the protection of indigenous rights, sustainability, and regional stability. Indigenous concerns regarding cultural heritage, traditional lifestyle disruption, resource overexploitation, and environmental risks are addressed through MCDA-driven, data-informed analysis. Recommendations include promoting indigenous participation in policy-making, ensuring fair distribution of economic benefits, enforcing strict environmental safeguards, and supporting long-term community development. The findings demonstrate strategies that respect indigenous sovereignty and promote sustainable development. Integrating game theory with MCDA equips policymakers with scientifically grounded tools for informed, collaborative decision-making, balancing economic goals with indigenous rights and environmental stewardship.",4,Andover | Renaissance Waverly Hotel,Optimization Models for Logistics,38
134,5695.0,An Adaptive Dynamic Programming Heuristic for Train Assembly in a Railyard,Academician,"Efficient railcar movement within freight railyards is essential for minimizing operational costs and enhancing service efficiency. In typical manufacturing facilities that use rail transportation, loaded railcars are stored on classification tracks without prior knowledge of their outbound destinations. This leads to the challenge of retrieving subsets of railcars scattered randomly throughout the yard to assemble outbound trains, incurring significant repositioning costs. We address this problem by formulating a mixed-integer programming model that permits the simultaneous movement of multiple railcars, and we establish the NP-hardness of the problem through a conflict graph analysis. We develop an Adaptive Dynamic Programming Heuristic (ADP-H) that groups adjacent railcars with identical destinations and maintains these groups throughout graph construction and movement. By ensuring that destination-specific car groups are never split, the ADP-H significantly reduces the required number of states, thereby enhancing the efficiency and scalability of solution via dynamic programming. Numerical results show that the ADP-H produces quality feasible solutions in reasonably short computing time.",1,Andover | Renaissance Waverly Hotel,Applications of Combinatorial Optimization,39
135,6505.0,"An Adaptive Model for Real-Time, Dynamic Disaster Response",Academician,"Traditionally, vehicle routing problems (VRPs) have been applied to minimize the travel time or distance traveled by vehicles from their starting points to their destinations. However, in emergency situations, the urgency of controlling the situation, saving lives, and providing immediate medical attention to those injured requires not only a rapid response from emergency fleet vehicles but also a strategic approach. In addition to minimizing the time it takes for emergency vehicles to reach their destinations, other crucial factors, such as road congestion and closures, limited availability of response vehicles, and disaster-induced hazards, must be considered. This paper introduces a VRP-based mathematical model for diversified emergency fleet routing in disaster management, addressing key challenges such as resource limitations, aid prioritization, traffic congestion, and rerouting due to road closures or hazards. The main contribution of the proposed model is its ability to enable dynamic rerouting of vehicles in real time, ensuring that emergency vehicles can adjust their paths in response to evolving road conditions. The objective of this study is to determine the optimal routing and scheduling strategy for a fleet of emergency vehicles, each with varying priorities and availability, to ensure they arrive at emergency sites in the shortest possible time while maintaining a time gap between vehicles. This time gap is essential for reducing congestion among emergency vehicles and facilitating dynamic route adjustments when road hazards are encountered. Results demonstrate that the proposed model offers a robust solution for disaster management, improving response times and adaptability in unpredictable emergency scenarios.",2,Andover | Renaissance Waverly Hotel,Applications of Combinatorial Optimization,39
136,6044.0,Operational Decision Making in Shared Truckload Transportation,Practitioner,"The increasing number of gasoline-powered trucks on the road, projected to continue rising, contributes significantly to traffic congestion and carbon emissions. With many trucks partially empty, load consolidation offers a promising solution to these challenges. Shared truckload transportation, an emerging model in long-haul freight, allows freight brokers to reduce costs, lower emissions, and decrease the number of trucks on the road by consolidating shipments. However, this model adds complexity to decision-making tasks like driver assignment and load bundling. This study introduces frameworks that support decision-making for carriers and brokers, aimed at reducing current and future operational costs. We evaluate these frameworks through a computational study based on a Southeast U.S. freight carrier, demonstrating substantial cost reductions and improved operational efficiency.",3,Andover | Renaissance Waverly Hotel,Applications of Combinatorial Optimization,39
137,8864.0,Optimal Component Allocation on Pallet to Minimize Curing Costs in Precast Manufacturing,Practitioner,"Purpose: This research focuses on optimizing pallet capacity utilization in the curing process of precast concrete (PC) components, a critical aspect of minimizing curing costs in precast manufacturing. In order to improve the efficiency of precast component production, the specific objective is to minimize the curing cost by maximizing the average utilization rate of pallets used during the curing process. Methodology: To ensure the efficient utilization of curing space, an optimization model is developed to strategically arrange PC components of varying mixtures, shapes and sizes on pallets. The model incorporates several constraints such as fixed pallet areas, delivery deadlines, and demand fluctuations to dynamically allocate pallet capacity while fulfilling specific component requirements. Findings: The model demonstrates significant reductions in curing costs while maintaining production efficiency. It provides a strategy for arranging components on pallets to maximize curing capacity and minimize delays caused by inefficient curing processes. By analyzing the computational performance of the model, it is observed that higher demand for PC components increases curing costs and computational time. Larger dimensions of components also raise these metrics but at a slower rate. Assigning more components to a pallet reduces the number of pallets and batch sizes, leading to lower costs. Research Limitations: The study focuses specifically on the curing process in PC manufacturing and may require modifications to adapt to other production stages or industries with similar logistical challenges. Practical Implications: The findings provide actionable insights for optimized resource utilization in curing processes, ensuring production efficiency for precast manufacturing companies.",4,Andover | Renaissance Waverly Hotel,Applications of Combinatorial Optimization,39
138,6320.0,Distributed Speed Scaling in Large-Scale Service Systems,Academician,"In this talk, we will consider a large-scale parallel-server system, where each server independently adjusts its processing speed in a decentralized manner. The objective is to minimize the overall cost, which comprises the average cost of maintaining the servers' processing speeds and a non-decreasing function of the tasks' sojourn times. The problem is compounded by the lack of knowledge of the task arrival rate and the absence of a centralized control or communication among the servers. We draw on ideas from stochastic approximation and present a novel rate scaling algorithm that ensures convergence of all server processing speeds to the globally asymptotically optimum value as the system size increases. Apart from the algorithm design, a key contribution of our approach lies in demonstrating how concepts from the stochastic approximation literature can be leveraged to effectively tackle learning problems in large-scale, distributed systems. En route, we will also discuss the performance of a fully heterogeneous parallel-server system, where each server has a distinct processing speed, which might be of independent interest.",1,Andover | Renaissance Waverly Hotel,Analysis of Stochastic Systems,40
139,5643.0,Finite Time Convergence to Stationarity of M/M/N Queues: A Lyapunov-Poincaré Approach,Academician,"Service systems like data centers and ride-hailing are popularly modeled as queueing systems in the literature. Such systems are primarily studied in the steady state due to its analytical traceability. However, almost all applications in real life do not operate in a steady state, and so, there is a clear discrepancy in translating theoretical queueing results to practical applications. To this end, we provide a finite time convergence for a simple M/M/n queue, providing a stepping stone toward understanding the transient behavior of more general queueing systems. M/M/n queue exhibits a phase-transition at the so-called Halfin-Whitt regime. We obtain a tight characterization of the finite time queue length distribution in the super-Halfin-Whitt regime, all the way until the phase transition at Halfin-Whitt. We also obtain a finite-time bound for the distance to stationarity for the sub-Halfin-Whitt regime as well, along with other finite-time statistics such as mean queue length and tail bound. To prove such a result, we employ the Lyapunov-Poincaré approach, where we first carefully design a Lyapunov function to obtain a negative drift outside a bounded set. Within the bounded set, we get a handle on the behavior using a local version of the canonical path method to obtain a local Poincaré inequality. A key aspect of our methodological contribution is in obtaining tight guarantees in these two regions, which when combined give us tight mixing time bounds. Moreover, our approach has the potential to be generalized to other settings beyond M/M/n systems.",2,Andover | Renaissance Waverly Hotel,Analysis of Stochastic Systems,40
140,5945.0,Optimal Admission Control in Queues with Multiple Customer Classes and Abandonments,Academician,"We consider a Markovian, finite capacity queueing system with multiple customer classes and multiple servers where customers waiting in line may get impatient and leave without being served. There is a cost associated with abandonments and a holding cost associated with customers in the system. Admitted customers pay a class dependent reward at the time of arrival. Under these assumptions, our objective is to characterize the optimal admission control policy that maximizes the long-run average reward. We formulate the problem as a Markov decision process problem and prove that the optimal policy is a DST (Double Set of Thresholds) policy where there is a pair of thresholds for each class, such that customers of that class are admitted only if the total number of customers in the system is between the two thresholds. We also identify sufficient conditions under which the optimal policy reduces to an SST (Single Set of Thresholds) policy where each customer class is admitted only if the total number of customers in the system is less than a certain threshold. After investigating how the optimal long-run average reward changes with respect to system parameters, we conclude with a comparison of the performance of the optimal SST policy to the optimal policy.",3,Andover | Renaissance Waverly Hotel,Analysis of Stochastic Systems,40
141,5930.0,Dynamic Server Assignment in Queueing Systems with Synergistic Servers and Abandonments,Academician,"We study tandem queueing systems in which servers work more efficiently in teams than on their own and customers are impatient in that they may leave the system while waiting for service. Our goal is to determine the server assignment policy that maximizes the long-run average throughput. We show that when each server is equally skilled at all tasks, the optimal policy has all the servers working together at all times. We also provide a complete characterization of the optimal policy for Markovian systems with two stations and two servers when each server's efficiency may be task dependent. We show that the throughput is maximized under the policy which assigns one server to each station (based on their relative skill at that station) unless station 2 has no work (in which case both servers work at station 1) or the number of jobs in the buffer reaches a threshold (in which case both servers work at station 2). We study how the optimal policy varies with the level of server synergy (including no synergy) and also compare the optimal policy for systems with different customer abandonment rates (including no abandonments). Finally, we investigate the case where the synergy among collaborating servers can be task-dependent and provide numerical results.",4,Andover | Renaissance Waverly Hotel,Analysis of Stochastic Systems,40
142,5953.0,Generalized Isotonic Optimization with Fixed Costs,Academician,"This paper addresses a new mixed-integer nonlinear optimization problem that extends the generalized isotonic optimization problem by incorporating markup cost whenever a change occurs. More specifically, this formulation considers a directed path graph and puts isotonic constraints on each arc. The costs incurred at each node consist of a markup cost and a convex cost, and our objective is to minimize the overall costs. The formulation covers various applications, such as reduced isotonic regression, stochastic lot-sizing, and joint inventory control and pricing with costly price markdown. Upon proving an optimality condition, we introduce a shortest path algorithm achieving $O(n^2 \log n)$ complexity with arc information pre-computed to optimally solve this problem. Besides, after assuming the minima of those convex functions follow a non-decreasing order, we can leverage Monge conditions to improve computational efficiency to $O(n)$. We then extend this model by making the graph from a directed path to an arborescence and show that the current shortest path algorithm fails generally. Hence, we consider three special yet general enough cases: (i) piecewise linear convex functions, (ii) dominating markup costs on parent nodes, (iii) balanced and several unbalanced arborescences, and show that they are all able to be solved in polynomial time using more involved techniques. We also construct an instance that those special cases cannot cover, leaving a gap in solving the general arborescence-based problem in polynomial time.",1,Andover | Renaissance Waverly Hotel,Integer Programming,41
143,6612.0,Construction of Value Functions of Integer Programs with Finite Domain,Academician,"Value functions play a central role in integer programming duality, and they are also used to develop solution methods for stochastic integer programs, bilevel integer programs and robust optimization problems. In this paper, we propose a column-by-column algorithm for constructing the value functions of integer programs with finite domain over the set of so-called level-set minimal vectors. The proposed algorithm starts with the first column and sequentially adds the rest columns one by one. Each time a column is added, a new set of level-set minimal vectors is generated based on the previous set and the optimal objective values over the level-set minimal vectors are also computed. The advantage of the proposed algorithm is that no integer program needs to be solved in the algorithm and no dominated vectors in level sets are generated (only level-set minimal vectors are generated). Computational results on benchmark instances show that the proposed algorithm can reduce the computational time by up to three orders of magnitude compared with a state-of-the-art algorithm. We also extend the proposed algorithm to build value functions of integer programs with negative elements in the constraint matrix.",2,Andover | Renaissance Waverly Hotel,Integer Programming,41
144,8683.0,Recovering Dantzig-Wolfe Bounds Using Cutting Planes,Academician,"Dantzig-Wolfe (DW) decomposition is a well-known technique in mixed-integer programming (MIP) for decomposing and convexifying constraints to obtain potentially strong dual bounds. We investigate cutting planes that can be derived using the DW decomposition algorithm and show that these cuts can provide the same dual bounds as DW decomposition. More precisely, we generate one cut for each DW block, and when combined with the constraints in the original formulation, these cuts imply the objective function cut one can simply write using the DW bound. This approach typically leads to a formulation with lower dual degeneracy that consequently has a better computational performance when solved by standard MIP solvers in the original space. We also discuss how to strengthen these cuts to improve the computational performance further. We test our approach on the Multiple Knapsack Assignment Problem and the Temporal Knapsack Problem, and show that the proposed cuts are helpful in accelerating the solution time without the need to implement branch and price. (joint work with Rui Chen and Andrea Lodi)",3,Andover | Renaissance Waverly Hotel,Integer Programming,41
145,5790.0,A Hierarchy of SOCP-based relaxations for 0-1 Quadratic Programs,Academician,"Solving a 0-1 quadratic program (a combinatorially NP-Hard problem) by converting it into an equivalent 0-1 linear integer program has been well articulated in the literature. Moreover, several methodologies such as the Reformulation-Linearization Technique (RLT), produce a hierarchy of ever-tightening relaxations for constructing the convex hull of 0-1 integer programs. In this research, we add to the body of literature on hierarchical approaches by employing a hierarchy of Second Order Cone Programming (SOCP) -based relaxations to improve the solvability of 0-1 QPs. As with other techniques, this SOCP relaxation procedure also generates a sequence of relaxations that ultimately converge to the convex hull of the underlying 0-1 IP, but it offers many computational advantages as the number of constraints required at the k -th step of this procedure is significantly lower as compared to existing methodologies, while yet retaining the strength of the underlying lower bound. We prove that this SOCP-based relaxation procedure converges to the convex hull of the 0-1 QP in n -steps (where n is the number of variables in the problem) and furthermore, detailed computations on several well-known instances from QPLIB are used to demonstrate the efficiency of the proposed methodology.",4,Andover | Renaissance Waverly Hotel,Integer Programming,41
146,6206.0,Cooperation in the Design of Public Goods,Academician,"We consider the cooperative elements that arise in the design of public goods, such as transportation policies and infrastructure. These design processes involve a wide variety of stakeholders: government, busineses, advocates, and at the heart of it all transportation users. As such, their eventual deployment (or lack thereof) is critically dependent on the decision-maker's ability to garner sufficient support from each of these groups. We formalize these strategic requirements from the perspective of cooperative game theory. Specifically, we study non-transfearable utility (NTU) linear programming (LP) games, which combine the game-theoretic tensions inherent in public decision-making with the modeling flexibility of mathematical programming. We derive general conditions under which social cooperation is possible (i.e., conditions under which the core of NTU LP games is non-empty), results on the complexity of associated computational tasks, and devise computational solution methods. To illustrate the practical merits of this theory, we perform a numerical, data-driven implementation in the area of public transit network design. In particular, we demonstrate how the (often) conflicting design objectives of maximizing ridership and maximizing coverage fuse with the distribution of quality of service, ultimately giving rise to power dynamics that can significantly constrain the design space of a transit planner.",1,Andover | Renaissance Waverly Hotel,OR for Societal Good III,42
147,8492.0,Does dockless bike-sharing create a competition for losers?,Academician,"We model the oligopoly competition in a dockless bike-sharing (DLB) market as a dynamic game. Each DLB operator is first committed to an action tied to a specific objective, such as maximizing profit. Then, the operators play a lower-level game to reach a subgame perfect Nash equilibrium, by making tactic decisions (e.g., pricing and fleet sizing). We define a Nash equilibrium under either weak or strong preference to characterize the likely outcomes of the dynamic game, and formulate the demand-supply equilibrium of a DLB market that accounts for key operational features and mode choice. Using the oligopoly game model calibrated with empirical data, we show that, if an operator seeks to maximize its market share with a budget constraint, all other operators must either respond in kind or be driven out of the market. When all operators compete for market dominance, even a slight efficiency edge gained by one operator can significantly shift the outcome, which signals high volatility. Moreover, even if all operators agree to focus on making money rather than ruinously seeking dominance, profitability still plunges quickly with the number of operators. Taken together, the results explain why an unregulated DLB market is often oversupplied and prone to collapse under competition. We also show this market failure may be prevented by a fleet cap regulation, which sets an upper limit on each operator's fleet size.",2,Andover | Renaissance Waverly Hotel,OR for Societal Good III,42
148,6516.0,Integrated Dynamic Rebalancing and Charging for Electric Scooter Sharing System,Academician,"Electric-Scooter Sharing System (ESS) is a popular dock-less micro-mobility option for first and last mile trips in cities. ESSs can reduce traffic congestion and lower greenhouse gas emissions by replacing private cars and enhancing public transit usage. Because of spatiotemporally imbalanced customer demands and limited battery ranges, ESSs face operational challenges in sustaining their functionality throughout the day. Customers unable to find available, sufficiently charged e-scooters are likely to leave the system, resulting in unsatisfactory service experiences. To address this issue, providers must regularly rebalance e-scooters across zones and return them to depots for charging. However, prior studies on rebalancing and charging of ESS are limited. Research has primarily focused on isolated strategies for either rebalancing or charging, or on overnight operations that do not account for dynamic daytime customer demands. In this study, we proposed an integrated dynamic rebalancing and charging model based on mixed integer programming. This model considers both the real-time battery levels of e-scooters and their distribution across service zones and depots. To efficiently address real-world, large-scale instances, we applied Lagrangian Dual Decomposition and Local Search methods to approximate solutions, breaking the problem into three subproblems: rebalancing vehicle routing, e-scooter allocation, and charging restoration. Our numerical experiments demonstrate that our model generates high-quality solutions for large-scale instances. Our model offers an approach for dynamically maintaining the service of shared mobility systems with electrified fleets.",3,Andover | Renaissance Waverly Hotel,OR for Societal Good III,42
149,6356.0,An Initial Condition-Dependent Deep Learning Approach for Optimal Control Problems,Academician,"In this work, we investigate an indirect approach for the numerical solution of optimal control problems of the Lagrange type via deep learning. A customized neural network is constructed, where optimal state, co-state and control trajectories are approximated by minimizing the underlying parameterized Hamiltonian, relying on Pontryagin's Minimum Principle. Departing from previous results reported in the literature, we propose a novel, augmented network with both time and trajectory initial condition as inputs, suggesting an optimal feedback controller therein. Finally, we compare our initial conditional-dependent approach with other numerical schemes, including direct methods making use of other function approximators, such as polynomial basis functions and truncated Fourier series.",1,Waverly | Renaissance Waverly Hotel,Intersection of Operations Research and Machine Learning,43
150,6948.0,Machine Learning Optimization and Applications,Academician,"Optimization is a critical research area with increasing relevance to machine learning. For a range of machine learning tasks—such as hyperparameter tuning, knowledge transfer, and feature selection—optimization techniques are essential. Hyperparameter tuning, for instance, relies on optimization to identify the ideal set of parameters to maximize model performance, while feature selection uses optimization to isolate the most relevant data attributes, improving accuracy and reducing model complexity. Knowledge transfer between domains also benefits from robust optimization methods, ensuring that models trained in one setting adapt effectively to new contexts. This talk will draw on current research in our lab to provide an overview of optimization in machine learning, highlighting two primary integrations: (1) using optimization to enhance machine learning, and (2) applying machine learning to support optimization. We’ll illustrate these approaches with two examples from our work. The first example demonstrates how distributionally robust optimization can improve knowledge transfer across different domains by developing a new loss function to account for shifts in data distributions, thereby enhancing model robustness. The second example leverages predictive machine learning models to provide critical parameter information in a planogram optimization problem, a task commonly encountered in retail and logistics to optimize product placements. We hope this talk offers valuable insights into the potential of these approaches, sparking discussions and inspiring new directions to further advance research in both optimization and machine learning.",2,Waverly | Renaissance Waverly Hotel,Intersection of Operations Research and Machine Learning,43
151,4853.0,Proactive UAV Routing with Optical Flow and Reinforcement Learning for Adaptive Surveillance,Academician,"The rapid advancement of AI and sensor technologies has brought unprecedented capabilities while introducing new vulnerabilities, particularly in critical resources and defense systems, causing significant challenges to national security. Adversaries may exploit these technologies to undermine surveillance systems, disrupt defense operations, and take advantage of vulnerable areas. Addressing these challenges requires comprehensive, data-driven modeling approaches that enable proactive surveillance and robust defense strategies. This study presents an optical flow (OF)-based routing policy for unmanned aerial vehicles (UAVs), enabled by Markov Decision Processes (MDPs) and leveraging real-time sensor data within a game-theoretic framework. By integrating OF values—a measure of dynamic motion patterns in the environment—into reinforcement learning framed by MDPs, the proposed approach anticipates threats and enhances adaptability to rapidly changing conditions while accounting for adversarial behaviors. This methodology enables UAVs to respond intelligently to dynamic scenarios, such as shifting threats or unforeseen intruders, while maintaining comprehensive and effective surveillance coverage. The framework accounts for various adversarial behaviors, including dive bombers (high-speed, targeted threats), random movers (unpredictable, erratic patterns), and strategic adversaries (deliberate, calculated interference). Validation is conducted using a high-fidelity physics-based simulation designed to emulate realistic operational scenarios. These scenarios incorporate diverse environmental conditions and adversarial interference, ensuring comprehensive testing of the methodology. The findings demonstrate the potential of integrating optical flow analysis with reinforcement learning, enhancing situational awareness, and improving the effectiveness of surveillance operations against diverse and evolving threats.",3,Waverly | Renaissance Waverly Hotel,Intersection of Operations Research and Machine Learning,43
152,6359.0,"Convergence, Bias, and Fairness in Federated Learning: A Non-Stationary Multi-Armed Bandit Approach for Heterogeneous Client Selection",Practitioner,"In practical federated learning (FL) environments, clients often possess non-IID data, which can degrade model performance and extend convergence times. Effective client selection strategies have emerged as a promising approach to mitigate the challenges posed by statistical heterogeneity across clients. This paper proposes two novel non-stationary multi-armed bandit (MAB) approaches for dynamic client selection, addressing the inherent challenges of time-varying client data distributions and their impact on model performance. Unlike traditional methods that assume stationary reward distributions, our approach adapts to evolving client behaviors and data characteristics, ensuring that selection decisions reflect current relevance rather than outdated information. We evaluate client selection as a trade-off among three critical objectives: 1) Maximizing convergence rate to expedite training, 2) Minimizing solution bias to enhance model generalizability, 3) Promoting fairness to ensure consistently high performance across clients. Leveraging non-stationary MAB techniques, Discounted Upper Confidence Bound (D-UCB) and Sliding Window UCB (SW-UCB), we effectively balance exploration and exploitation to address client variability. Additionally, our method evaluates the potential contributions of each client by dynamically weighting recent data trends, providing an opportunity for under-represented clients to participate. Through comprehensive experiments, we examine the trade-offs between accelerating convergence and reducing bias while maintaining fair performance distribution across clients. The proposed approach provides a robust solution to the nuanced trade-offs inherent in federated client selection.",4,Waverly | Renaissance Waverly Hotel,Intersection of Operations Research and Machine Learning,43
153,6162.0,Application of Bin Packing for Effective Coalition Formation in Emergency Response,Academician,"Effective emergency response to natural disasters and disease outbreaks requires swift coordination among diverse entities, including healthcare providers, government agencies, and non-governmental organizations. Coordination can improve outcomes by capturing economies of scale, but not all partnerships are beneficial. Finding optimal coalitions among these parties is crucial but challenging due to varying resource needs, capacities, and logistical constraints. This research leverages similarities between the coalition structure problem and the well-known bin packing problem to identify good coalitions. Bin packing is used to model coordination by grouping parties into ""bins"" that represent potential coalitions. Since the bin packing problem is NP-hard, finding efficient, scalable computational approaches is vital for these coalition decisions. We analyze the effectiveness of different heuristics for bin packing in this context, where the heuristics differ based on packing sequence and bin capacity. Heuristic performance is assessed based on the total coalition structure cost and computation time. Through this approach, bin packing offers a systematic way to enhance decision-making in coalition formation, enabling faster and more coordinated responses. This scalable framework can help improve supply chain resilience, ultimately strengthening emergency response operations.",1,Waverly | Renaissance Waverly Hotel,Models for Disaster Relief and Resilience,44
154,8954.0,A Two-Stage Robust Optimization Framework for Community Resilience Planning,Academician,"Community resilience planning is challenging not only because the community is composed of many interdependent infrastructures, a diverse population, and numerous stakeholders but also because the hazards the community is vulnerable to are uncertain, and the availability of resources for mitigation actions is limited. Moreover, communities need to plan for hazards that are very rare but highly consequential, and the level of conservativeness in decision-making significantly impacts the resulting resilience metrics. This work presents a two-stage robust optimization framework that can assist communities in developing mitigation strategies for the community’s built environment and investigate the effect of varying the level of conservativeness in decision-making in terms of several resilience metrics. The first stage model of the proposed framework identifies pre-hazard mitigation actions for the community’s built environment to maximize the resilience of the least favorable hazard scenario among the scenarios under consideration. On the other hand, the second stage model generates the worst hazard scenario in terms of the resilience metric for the current set of actions and the uncertainty sets representing the stochastic nature of the hazard. A case study is presented to showcase the framework's applicability in developing alternative mitigation strategies under varied levels of conservativeness.",2,Waverly | Renaissance Waverly Hotel,Models for Disaster Relief and Resilience,44
155,5391.0,Cost-Effective Relief Network Design on Time-Phased Networks,Academician,"A well-designed relief network can be crucial in saving lives and helping people recover after a disaster. However, designing such a network involves considering various factors such as cost and time constraints. To address this, we proposed a mixed integer model in our paper that minimizes costs and satisfies evacuee demand by determining the best distribution center (DC) to open and the most efficient route and flow to connect DCs to shelters within a specific time frame. We devise a Benders Decomposition (BD) based algorithm to solve our model. To address the slow convergence rate of BD, we also provide acceleration approaches to enhance its performance. Finally, we test the model on randomly generated instances to examine the impact of algorithmic improvements and further perform an extensive analysis of results using real case study data from Texas Gulf Coast facilitated by Geographical Information System (GIS) .",3,Waverly | Renaissance Waverly Hotel,Models for Disaster Relief and Resilience,44
156,7040.0,Strategic Patrol and Surveillance Allocation under Resource  Constraints: A Game-Theoretic Framework,Academician,"Despite extensive research on resource allocation problems, the critical challenge of optimizing limited budgets for allocating patrol agents, technological tools (e.g., surveillance vehicles or towers), and sensors to minimize overall risk in open areas, such as land borders between countries, remains underexplored. This question inherently involves managing significant uncertainties, including predicting high-risk areas that may attract adversarial attention at any given time and dynamically allocating resources to minimize expected overall risk. The strategic interaction between adversaries and a defensive agency makes game theory particularly useful for modeling these interactions. This study addresses this challenge by developing a novel game-theoretic resource allocation model that optimizes patrol routes with technological tool placement under budget constraints and uncertainties.",4,Waverly | Renaissance Waverly Hotel,Models for Disaster Relief and Resilience,44
157,8480.0,Adaptive Optimization with Highly Corrupted Inputs: A Unified Framework for High-Probability Iteration Complexity Analysis,Academician,"We consider an unconstrained continuous optimization problem in which gradient estimates may be arbitrarily corrupted in each iteration with a probability greater than 1/2 both in the deterministic and stochastic settings. Additionally, function value estimates may be noisy or adversarially corrupted throughout the algorithm’s execution. This framework is applicable to many real-world problems, particularly relevant to stochastic and derivative-free optimization. We introduce an algorithmic and analytical framework that provides high-probability bounds on iteration complexity for this highly corrupted setting. The analysis offers a unified approach, accommodating noisy or corrupted inputs and encompassing methods such as line search and trust region.",1,Brayton | Renaissance Waverly Hotel,Machine Learning and Optimization,45
158,5986.0,RL Simplex: A Proof of Concept for Reinforcement Learning-Enhanced Pivot Selection,Academician,"Pivot operation significantly impacts the overall computational efficiency of Linear Program (LP) solvers. This research advances the application of Reinforcement Learning (RL) techniques within LP theory, focusing on the strategic selection of pivot variables. The method is particularly beneficial when multiple candidate variables are available for pivoting. We use the LP relaxation of the traveling salesman problem and demonstrate our RL Simplex algorithm's superiority over commercial solvers in terms of scalability, solution time, and stability.",2,Brayton | Renaissance Waverly Hotel,Machine Learning and Optimization,45
159,5956.0,Distributionally Robust Universal Classification,Academician,"We study a distributionally robust optimization (DRO) formulation of a stochastic multi-class classification problem (SP) under the Wasserstein distance. The DRO formulation addresses the poor out-of-sample performance and overfitting issues of the sample average approximation (SAA) approach, which relies on the empirical distribution rather than an uncertainty set. To manage the infinite-dimensional nature of the DRO formulation, we develop an in-sample counterpart, referred to as in-sample DRO, which enables a tractable solution while maintaining robust properties. We establish asymptotic relationships between the original SP, the DRO formulation, and the in- sample DRO formulation, and we propose sample size guarantees to ensure reliable performance. Additionally, we derive a mixed-integer linear programming (MILP) representation to solve the in-sample DRO formulation efficiently, mitigating the curse of dimensionality over feature space. Numerical experiments highlight the improved effectiveness and resilience of the distributionally robust model compared to traditional classification techniques.",3,Brayton | Renaissance Waverly Hotel,Machine Learning and Optimization,45
160,5567.0,A Robust Twin Support Vector Machine Approach for Classification Problems with Uncertainty,Academician,"In this presentation, we introduce a distributionally robust Twin Support Vector Machine (TSVM) model designed to tackle bi-class and multi-class classification challenges under uncertainty. Our model leverages first and second-order information to enhance robustness. By solving two interconnected chance-constrained SVM models, we identify two nonparallel linear separation hyperplanes. To ensure efficient computation, we derive tractable SDP and SOCP reformulations. Tests on synthetic data and real-world benchmarks reveal that our model outperforms established classification models.",4,Brayton | Renaissance Waverly Hotel,Machine Learning and Optimization,45
161,5156.0,A Machine Learning-Based Warm Start Approach to Improve the Solvability of Optimization Problems,Academician,"In this research, we explore the use of machine learning (ML) techniques, notably logistic regression and decision tree algorithms, for improving the solvability of optimization problems. Recognizing that the computational time required to solve a linear programming problem can be heavily influenced by the choice of a starting basis (referred to as ""warm start""), we apply classification techniques to apriori decide the decision variables that are most likely to be part of the optimal basis. Problem parameters and their combinations (including interactions) that yield the most effective warm start basis are identified. We also explore more advanced ML methods such as neural networks that can aid in improving the identification of advanced bases. Our detailed computations show that the use of such advanced ML techniques can significantly improve the solvability of optimization problems.",1,Brayton | Renaissance Waverly Hotel,Enhancing Optimization Algorithms using Artificial Intelligence and Machine Learning,46
162,5615.0,Enhancing Column Generation for Parallel Machine Scheduling via Transformers,Academician,"We present a neural network-enhanced column generation (CG) approach for a parallel machine scheduling problem. The proposed approach utilizes an encoder-decoder attention model, namely the transformer and pointer architectures, to develop job sequences with negative reduced cost and thus generate columns to add to the master problem. By training the neural network offline and using it in inference mode to predict negative reduced costs columns, we achieve significant computational time savings compared to dynamic programming (DP). Since the exact DP procedure is used to verify that no further columns with negative reduced cost can be identified at termination, the optimality guarantee of the original CG procedure is preserved. For small to medium-sized instances, our approach achieves an average 45% reduction in computation time compared to solving the subproblems with DP. Furthermore, the model generalizes not only to unseen, larger problem instances from the same probability distribution but also to instances from different probability distributions than those presented at training time. For large-sized instances, the proposed approach achieves an 80% improvement in the objective value in under 500 seconds, demonstrating both its scalability and efficiency.",2,Brayton | Renaissance Waverly Hotel,Enhancing Optimization Algorithms using Artificial Intelligence and Machine Learning,46
163,5623.0,ML-Assisted Parallel Branch-and-Price Framework for Community-Based Service Delivery,Academician,"Increasing demand for location-based personal services poses challenges to planning logistics, for example in the industry of parcel delivery and medical specimen collection. The complexities stem from providing time-constrained services, balancing customer priorities with cost, and managing travel and service time uncertainties. As demand networks grow, traditional mixed-integer programming (MIP) solvers struggle to produce quality solutions within available computing resources. Nonetheless, leveraging the modular and community-based structures, often found in customer geospatial layouts, we propose a novel decomposition-based exact solution approach. This method integrates parallel computation within a modified branch-and-price framework and accelerates optimization routines using machine learning techniques. Our solver demonstrates comparable solution quality to traditional methods for small-scale problems while significantly outperforming them as problem size increases. This approach offers a promising solution to the growing complexities in this service industry. We illustrate the solver performance on generated instances as well as with real-life parcel delivery case studies.",3,Brayton | Renaissance Waverly Hotel,Enhancing Optimization Algorithms using Artificial Intelligence and Machine Learning,46
164,6831.0,A Hybrid Machine Learning-Optimization Framework for Enhanced TSP Solutions,Academician,"This work proposes a hybrid framework combining machine learning and traditional optimization techniques to solve the Traveling Salesman Problem (TSP) efficiently. Using an encoder-decoder neural network with local attention, the framework learns to predict near-optimal city sequences by training on smaller TSP instances. Its design includes a generalization method, enabling the framework to adapt these learned patterns to larger problems by leveraging the temporal structure of city sequences. A feasibility-check loop iteratively refines the initial solution to ensure the predicted solutions are feasible. It also removes infeasible segments using an optimization solver to produce a valid, near-optimal tour. The framework is particularly well-suited for logistics and route planning applications, where efficiency and accuracy are critical issues. Experimental results are presented comparing the approach to traditional TSP-solving techniques.",4,Brayton | Renaissance Waverly Hotel,Enhancing Optimization Algorithms using Artificial Intelligence and Machine Learning,46
165,6520.0,Strategic Coordination of Heterogeneous Resources for Attack Detection,Academician,"We study a two-player network inspection game in which a defender coordinates sensors with potentially heterogenous detection capabilities to detect attacks from a strategic attacker. We assume that there are multiple sensor types, each determined by its detected area coverage when placed at a location. The objective of the defender (resp. attacker) is to minimize (resp. maximize) the expected number of undetected attacks. We analytically characterize Nash equilibria of this zero-sum game in scenarios where there is no overlapping between sensors’ detection areas, and both players possibly perform randomized strategies. Building on this analysis, we introduce a new solution approach based on an efficient polynomial-time algorithm.",1,Andover | Renaissance Waverly Hotel,Infrastructure Network Resilience,47
166,8977.0,The Price of Flexibility in Electricity Markets,Academician,"This paper develops a game-theoretic model to explore forward premiums in electricity markets, demonstrating that the forward premium can be interpreted as a flexibility premium. This premium rewards market participants for bearing uncertainty and maintaining flexibility in managing future market conditions. The model examines two scenarios: one without arbitrageurs, where participants pay a premium for flexibility in the day-ahead market to manage risk, and another with arbitrageurs, who seek to exploit price discrepancies between day-ahead and real-time markets. Although arbitrageurs are expected to drive the forward premium to zero by reducing price differences, our model shows that the premium persists even with arbitrage activity. This persistence reflects the value of flexibility in uncertain, volatile conditions, which arbitrageurs cannot fully eliminate, particularly in markets with unpredictable events like weather disruptions or fuel price fluctuations. As a result, the forward premium remains as compensation for participants’ exposure to these uncertainties.",2,Andover | Renaissance Waverly Hotel,Infrastructure Network Resilience,47
167,6492.0,Stochastic approach for enhancing system resilience under budget constraints,Academician,"This study introduces a generalized optimization model incorporating chance constraints to enhance system resilience under a budget constraint. We establish desired levels of robustness and recovery time to ensure that system resilience remains within the desired range following an adverse event. This model accounts for the uncertainty in functional degradation and recovery times of the components, reflecting the unpredictable severity of adverse events in real-life scenarios. The goal is to optimally allocate the budget across components by maximizing the confidence levels associated with the desired robustness and recovery times, resulting in enhanced system resilience. Economic utility functions, both linear and nonlinear, are incorporated to facilitate optimal budget allocation. A key innovation of this model is its flexible, generalized framework, which can be applied across various systems to strengthen resilience under budget constraints. We show that the proposed approach performs well under various distributional assumptions. The chance constraints are formulated as second-order cone programming representations under both normal and uniform distributions. Furthermore, the proposed model is validated on a real network design, offering an efficient budget allocation plan to improve system resilience.",3,Andover | Renaissance Waverly Hotel,Infrastructure Network Resilience,47
168,6707.0,Fortifying Distribution Network Nodes Subject to Different Disruption Levels at Each Node,,"We consider a distribution network for delivering a natural resource or physical good to a set of nodes, each of which serves a set of customers, in which flow disruptions may occur at one or more nodes. Each node receives flow via a path from one or more source nodes, implying that a node's service level depends on flow from upstream nodes. We consider the effects of disturbances of an uncertain degree of severity that can lead to reduced flow at individual nodes in the network, as well as fortification measures for protecting nodes against these disturbances. Our proposed model determines node fortification levels that maximize service availability under a limited budget and various assumptions on network structure, the nature of potential disturbances, and their associated probability distributions.",4,Andover | Renaissance Waverly Hotel,Infrastructure Network Resilience,47
169,6104.0,A Computational Study on the Impact of Incrementally Changing Stochastic Probability Distributions to Optimization Model Outputs,Academician,"Stochastic programming is used to optimize decisions when there exist uncertain parameters, represented as probability distributions. In this work we are interested in stochastic programming problems where additional information about these uncertain parameters is revealed incrementally. In such cases, the probability distributions for the stochastic input parameters gets updated and we are interested in quantifying the impact of such probability distribution updates on the outputs of stochastic optimization models, i.e., on the optimal decision variable values and the associated optimal objective function values. Several methods have been established to measure the statistical distance or divergence between different probability distributions, with some popular measures being the total variation distance, and the Kullback-Leibler (KL) divergence measure. In this work we conduct a computational study that changes the uncertain parameters’ distribution functions incrementally and explores whether different established probability distance and divergence measures can be used to estimate the impact of such changes on a stochastic optimization model’s optimal decision variable and objective function values.",1,Andover | Renaissance Waverly Hotel,Stochastic Optimization Methodology and Applications,48
170,5746.0,Two-stage stochastic optimization formulation for the police staffing problem,Academician,"We present a two-stage stochastic optimization formulation of the police staffing problem, where staffing decisions are made in the first stage and dispatch decisions in the second stage. Noticing the special structure of our problem, we provide a customized integer L-shaped method for our second-stage problem and show its computational advantage compared to the existing methods using 911 call data from the Atlanta Police Department. We also develop a new formulation for the second-stage problem, which adopts ideas from interval scheduling theory.",2,Andover | Renaissance Waverly Hotel,Stochastic Optimization Methodology and Applications,48
171,6415.0,On the ReLU Lagrangian Cuts for Stochastic Mixed Integer Programming,Academician,"We study stochastic mixed integer programs with both first-stage and recourse decisions involving mixed integer variables. A new family of Lagrangian cuts, termed ``ReLU Lagrangian cuts,"" is introduced by reformulating the nonanticipativity constraints using ReLU functions. These cuts can be integrated into scenario decomposition methods. We show that including ReLU Lagrangian cuts is sufficient to achieve optimality in the original stochastic mixed integer programs. Without solving the Lagrangian dual problems, we derive closed-form expressions for these cuts. Furthermore, to speed up the cut-generating procedures, we introduce linear programming-based methods to enhance the cut coefficients. Numerical studies demonstrate the effectiveness of the proposed cuts compared to existing cut families.",3,Andover | Renaissance Waverly Hotel,Stochastic Optimization Methodology and Applications,48
172,5638.0,Contextual Stochastic Optimization for Omnichannel Multi-Courier Order Fulfillment Under Delivery Time Uncertainty,,"The paper studies a large-scale order fulfillment problem for a leading e-commerce company in the United States. The challenge involves selecting fulfillment centers and shipping carriers with observational data only to efficiently process orders from a vast network of physical stores and warehouses. The company's current practice relies on heuristic rules that choose the cheapest fulfillment and shipping options for each unit, without considering opportunities for batching items or the reliability of carriers in meeting expected delivery dates. The paper develops a data-driven Contextual Stochastic Optimization (CSO) framework that integrates distributional forecasts of delivery time deviations with stochastic and robust order fulfillment optimization models. The framework optimizes the selection of fulfillment centers and carriers, accounting for item consolidation and delivery time uncertainty. Validated on a real-world data set containing tens of thousands of products, each with hundreds of fulfillment options, the proposed CSO framework significantly enhances the accuracy of meeting customer-expected delivery dates compared to current practices. It provides a flexible balance between reducing fulfillment costs and managing delivery time deviation risks, emphasizing the importance of contextual information and distributional forecasts in order fulfillment. This is the first paper that studies the omnichannel multi-courier order fulfillment problem with delivery time uncertainty through the lens of contextual optimization, fusing machine learning and optimization. The results provide actionable insights for e-commerce companies and online retailers to enhance service quality and customer satisfaction through efficient order fulfillment strategies that accounts for delivery time uncertainty.",4,Andover | Renaissance Waverly Hotel,Stochastic Optimization Methodology and Applications,48
173,6372.0,An Efficient Branch-and-Cut Approach for Routing of Heterogeneous Fleet of Drones and Ground Vehicles to Deliver Time-Sensitive Products,Academician,"The rapid growth in e-commerce and demand for timely delivery of perishable or time-sensitive products, such as pharmaceuticals, and fresh food, have underscored the need for faster last-mile delivery solutions. This paper presents a mixed-integer linear programming model for unsynchronized multimodal routing of a heterogeneous fleet of vehicles, including ground vehicles operating in circular delivery mode and drones following direct delivery, to deliver time-sensitive products within a pre-determined time window defined by both release and due times. We considered different drone types, with distinct costs, energy consumption, battery capacity, payload capacity, and speed. We also applied machine learning to estimate the energy consumption of drones influenced by key factors, such as drone type, distance, and package weight based on real data from drone flight time and power consumption. The realistic feature of replacing the battery of drones when required by tracking their energy consumption is also studied in this problem. We proposed an efficient branch-and-cut (B&C) method and introduced a clustering-based heuristic technique for efficiently solving large-scale delivery networks. Numerical results based on real data from a prepared food delivery industry provide insights into the effect of different practical operating conditions on the required fleet size, the required number of battery replacements, and energy consumption. Our B&C method performed 30 times faster than Gurobi, while the clustering-based heuristic approach further improves performance, achieving over 100 times the speed of Gurobi with a minor reduction in solution quality.",1,Andover | Renaissance Waverly Hotel,"OR Applications in Drones, Electrified Transportation, and Power Distribution",49
174,6032.0,Queueing Strategic Planning for Electric Aircraft Battery Swap and Megacharging with Time Constraints,Academician,"Aviation industry is undergoing a paradigm change driven by the electrification for low-carbon flight. This work addresses two research questions on interoperable charging infrastructure for electric aircraft (EA): (i) How to design an airport charging infrastructure to meet on-time departures under variable EA arrivals? and (ii) What is the optimal number of spare batteries, regular chargers, and megachargers to meet the turn-around time goal? We propose a queueing model to design airport charging infrastructure by allocating spare batteries, regular chargers, and megachargers to minimize the cost. Based on EA arrival rates, we design a battery-swapping system using M/M/c/s queueing model with guaranteed service time. Regular chargers recharge swapped batteries using lower power, reducing grid stress. When spare batteries are out of stock, an EA is redirected to a megacharging system with higher charging power and unlimited queue capacity (M/M/m/∞). We show that reducing waiting time can improve battery service, yet it significantly drives up the cost and influences the affordability of infrastructure for different arrival rates. For a shorter EA turn-around time (e.g., 0.30 hours), the dependence on regular chargers and spare batteries dominates megachargers, but the cost increases by up to 40%. Megacharging leads to longer waiting time (0.63 hours) and higher cost but results in less inventory resource cost. It is found that keeping the balance between megachargers and regular chargers is essential for a cost-effective and robust airport charging infrastructure. Reducing waiting time in the megacharging queue improves system efficiency, ensuring EA turn-around time and supporting sustainable aviation.",2,Andover | Renaissance Waverly Hotel,"OR Applications in Drones, Electrified Transportation, and Power Distribution",49
175,6016.0,Infrastructure Guided Self-driving Vehicles Flow Optimization in Street Network with Batch Algorithm and Access Control,Academician,"Metro cities all around the world are facing increasing congestion problems. In congested city traffic, most of the delay and fuel consumption occur at the intersections. Infrastructure guided self-driving (IGSD) technology raised the potential to control the path and speed of IGSD vehicles in such a way that traffic flow efficiency is greatly improved. IGSD vehicles approaching the same intersection are grouped into batches based on non-conflict phases, each of which traverse the intersection without stopping. The greater challenge is to factor into the knock-on effect among intersections, and to create a coordinated solution for IGSD vehicle flow in the entire network. We propose a Batch Algorithm which is used in rolling horizon fashion in a 100% IGSD environment, that determines the velocities of vehicles in such a way that no vehicle has to stop while traversing the network, and that minimizes the total travel time. An Access-control (AC) method is also proposed to minimize the knock-on effect and consequent gridlock, which can be applied with most traffic management mechanisms with minor modification. The approaches are experimented in a dynamic and stochastic setting with the open-source simulator SUMO for Sioux Falls network and compared with other solutions proposed in the literature. The numerical results demonstrate that Batch algorithm with AC method significantly outperforms other traffic control methods and increases the throughput of the network by 45%.",3,Andover | Renaissance Waverly Hotel,"OR Applications in Drones, Electrified Transportation, and Power Distribution",49
176,8867.0,Strategic Switches and DG Placement for Microgrid Formation using Sample Average Approximation,Academician,"Severe blackout events due to extreme weather are increasing recently. These events highlight the need for resilient power distribution networks. In this work we present the switches and distributed generation placement problem to increase the resiliency of a power system distribution network through microgrid formation in case of disruptions. The problem is formulated as a two-stage stochastic mixed-integer program. The objective is to minimize the first-stage cost of operations and maintenance of resources, along with the expected cost of load shedding and power generation in second stage. we conducted a detailed computational study using the sample average approximation (SAA) method. Computational results on an IEEE test case network demonstrate the effectiveness of our proposed method.",4,Andover | Renaissance Waverly Hotel,"OR Applications in Drones, Electrified Transportation, and Power Distribution",49
177,7038.0,Online Resource Allocation with Predictions,Academician,"Online decision-makers often obtain predictions on future variables, such as arrivals, demands, inventories, and so on. These predictions can be generated from simple forecasting algorithms for univariate time-series, all the way to state-of-the-art machine learning models that leverage multiple time-series and additional feature information. However, the prediction accuracy is unknown to decisions-makers a priori, hence blindly following the predictions can be harmful. In this paper, we address this problem by developing algorithms that utilize predictions in a manner that is robust to the unknown prediction accuracy. We consider the Online Resource Allocation Problem, a generic model for online decision-making, in which a limited amount of resources may be used to satisfy a sequence of arriving requests. Prior work has characterized the best achievable performances when the arrivals are either generated stochastically (i.i.d.) or completely adversarially, and shown that algorithms exist which match these bounds under both arrival models, without ``knowing'' the underlying model. To this backdrop, we introduce predictions in the form of shadow prices on each type of resource. Prediction accuracy is naturally defined to be the distance between the predictions and the actual shadow prices. We tightly characterize the extent to which any algorithm can optimally leverage predictions (that is, to ``follow'' the predictions when accurate, and ``ignore'' them when inaccurate) without knowing the prediction accuracy or the underlying arrival model. Our main contribution is then an algorithm which achieves this lower bound. Finally, we empirically validate our algorithm with a large-scale experiment on data from the retailer H&M.",1,Andover | Renaissance Waverly Hotel,Methodology and Applications of Online and Dynamic Decision Making,50
178,6175.0,Adaptive Large Neighborhood Search for Online Rescheduling Semiconductor Assembly Systems,,"In semiconductor manufacturing assembly systems, a master schedule spanning several weeks is used to outline the production timeline. However, once the master schedule is implemented, it can be disrupted by unexpected events, such as random machine failures. These disruptions may cause sequencing delays and time window violations, rendering the master schedule suboptimal or even infeasible, leading to productivity losses. Remaking a new master schedule is time-consuming, and the delay during rescheduling may lead to even greater losses than those caused by the initial machine failure. To address this challenge, we propose a novel adaptive large neighborhood search method to quickly reschedule over a short period, considering the complex re-entrant flows and time window constraints. Unlike traditional methods that use disjunctive graph representations, the proposed adaptive large neighborhood search approach utilizes sequence representation to help mask infeasible solutions arising from machine failures. Additionally, we introduce a new set of destroy and repair operators specifically designed for this problem context. The objective function minimizes deviations from the original master schedule. Experimental results demonstrate that the proposed method significantly reduces delays and time window violations caused by disruptions.",2,Andover | Renaissance Waverly Hotel,Methodology and Applications of Online and Dynamic Decision Making,50
179,8984.0,New Online Algorithm for Single-Period Order Quotation with a Bottleneck Resource and Delayed Feedback,Academician,"In make-to-order (MTO) firms, feedback on whether customers accept or reject their quotations may be delayed, yet MTO manufacturers are liable to meet quoted lead times or face tardiness penalties. We consider the online setting where customer orders arrive in a single period, and they accept or reject their quotations at the end of this period while the MTO manufacturer schedules firm orders over a finite planning horizon. Given the limited capacity of a single bottleneck resource and the impatient nature of customers, the MTO manufacturer seeks to balance between quoting shorter lead times to attract customers and quoting longer lead times it can reliably meet. We propose an online primal-dual algorithm for this problem to maximize the net total profit, defined as the total profit penalized by the tardiness costs for overpromised quotations. Through simulation experiments, we compare the proposed algorithm's net profit with the hindsight optimum's net profit and show that the primal-dual algorithm outperforms naive benchmarks used in practice.",3,Andover | Renaissance Waverly Hotel,Methodology and Applications of Online and Dynamic Decision Making,50
180,6606.0,Optimizing Curb Space Management through Dynamic Pricing,Academician,"With the growth of new mobility services and increased needs for goods delivery, curb spaces, and their use have evolved very rapidly. Especially after COVID-19, active travel has increased and delivery rates have skyrocketed, creating a great need to ensure safe and efficient curb management. With delivery trucks, ride-sharing vehicles, private cars, and pedestrians all needing curb space, cities need a better way to manage these areas to keep them safe and efficient. One way to handle this is by using flexible pricing for various curb uses that adjust based on demand and available curb spaces. Hence, in this study, we developed a pricing model that sets curb prices dynamically for different uses by considering the uncertain demand, to maximize the revenue cities earn while supporting a fair and organized use of curb space for everyone. Our model aims to create straightforward pricing strategies that city planners can put into action quickly. This approach helps cities adapt to the fast-changing needs of curb spaces, supporting smoother, safer, and more organized city streets for everyone.",4,Andover | Renaissance Waverly Hotel,Methodology and Applications of Online and Dynamic Decision Making,50
181,8620.0,Inverse optimization in countable-state Markov decision processes,Academician,"Inverse optimization involves finding values of problem parameters that would render given values of decision variables optimal. This is in contrast with the usual forward optimization where decision variables are computed using given values of problem parameters. In the context of Markov decision processes (MDPs), the inverse optimization literature has focused on imputing two types of inputs that would make a given policy optimal: (i) cost parameters or (ii) transition probabilities. All existing work only considers the case of finite-state MDPs. We will present an extension to the case of countable-state MDPs. We will rely on conditions under which the inverse formulations can be written as mathematical programs with countably infinite numbers of variables and constraints. We will present computational methods to solve these mathematical programs. The methodology will be illustrated using countable-state (discrete-time) MDP formulations of continuous-time MDPs and semi-Markov decision processes via a technique called uniformization. Applications from queuing and manufacturing processes will be discussed.",1,Andover | Renaissance Waverly Hotel,Optimization with Uncertainty,51
182,8621.0,Landscape of Policy Optimization for Finite Horizon MDPs with General State and Action,Academician,"Policy gradient methods are widely used in reinforcement learning. Yet, the nonconvexity of policy optimization imposes significant challenges in understanding the global convergence of policy gradient methods. For a class of finite-horizon Markov Decision Processes (MDPs) with general state and action spaces, we develop a framework that provides a set of easily verifiable assumptions to ensure the Kurdyka-Łojasiewicz (KŁ) condition of the policy optimization. Leveraging the KŁ condition, policy gradient methods converge to the globally optimal policy with a non-asymptomatic rate despite nonconvexity. Our results find applications in various control and operations models, including entropy-regularized tabular MDPs, Linear Quadratic Regulator (LQR) problems, stochastic inventory models, and stochastic cash balance problems, for which we show an ϵ-optimal policy can be obtained using a sample size in $O(ϵ^{-1})$ and polynomial in terms of the planning horizon by stochastic policy gradient methods. Our result establishes the first sample complexity for multi-period inventory systems with Markov-modulated demands and stochastic cash balance problems in the literature.",2,Andover | Renaissance Waverly Hotel,Optimization with Uncertainty,51
183,5879.0,Risk set estimation for simulation optimization under input uncertainty,Academician,"Input uncertainty refers to the error in the stochastic simulation output caused by having to estimate the input models of the simulator from finite data. In this research, we propose a Bayesian framework for analyzing the risk of suboptimality of user-chosen solution x * when the simulator is subject to input uncertainty. We define the α-level risk set of x * as the set of solutions expected to outperform x * by a practically meaningful margin (> δ) with a significant probability (> α). An empty risk set implies a powerful statistical guarantee that there is no practically better solution than x * with a significant probability, despite uncertain inputs. To efficiently estimate the risk set, the conditional mean performance of solutions given a set of input distributions is modeled as a Gaussian process (GP) that takes solution-distribution pairs as inputs. We adopt a GP kernel that allows both nonparametric and parametric input models. To efficiently estimate the risk set, we propose a sequential sampling procedure based on a dynamic program formulation. We propose both one-step optimal and roll-out sampling policies and show that the estimated risk set is strongly consistent when the simulation budget increases to infinity.",3,Andover | Renaissance Waverly Hotel,Optimization with Uncertainty,51
184,7032.0,Forecast-driven Dual Sourcing Inventory Management using Simulation-Optimization,Practitioner,"Supply chain networks today are complex webs of siloed actors operating globally, functioning within a highly volatile, uncertain, and disruption-prone environment. The COVID-19 pandemic and its repercussions have severely impacted all major industries with unforeseen shifts in demand patterns, supply unavailability, shipment delays, and supplier unreliability. Ensuring customer satisfaction and growth in such an environment requires consistent product availability, coordination among the actors, and capabilities to proactively assess and smartly react to the upcoming uncertainties. Dynamic supplier management has proven effective in supply chain management, as recognized by both researchers and practitioners. Dual sourcing, in particular, has been extensively studied, considering various uncertainties, constraints, inventory control policies, and sourcing mechanisms. However, a gap exists due to varying assumptions about network, demand, supplier, and supply transportation characteristics. Furthermore, there exists an industry need for dynamic dual-sourcing and, consequently, multi-sourcing strategies that are proven and applicable in real-world scenarios. Our work addresses this gap, towards facilitating seamless and quasi-autonomous day-to-day decision-making. Our contribution builds on our data-intensive study of dual-mode forecast-driven inventory management to address challenges faced by a major North American e-commerce-based furniture manufacturer with whom we collaborate in action research mode. We first propose a simulation-optimization based dynamic dual sourcing framework, and then compare the performance of three dual-sourcing policies: single-index, dual-index, and tailored base-surge. Finally, we investigate the sensitivity of the policies and their performance to supplier characteristics and constraints.",4,Andover | Renaissance Waverly Hotel,Optimization with Uncertainty,51
185,8895.0,Mechanisms for Inter-Airline Collaborative Rescheduling under Disruptions,Practitioner,"Disruptions such as inclement weather may result in reduced flight capacities at airports. To maintain demand-capacity balance, airlines and air traffic managers (e.g., the US Federal Aviation Administration) collaboratively reschedule aircraft, resulting in potential delays. Due to a wide range of factors (e.g., flight crews timing out, passenger connections, etc.), a delayed flight may be more or less costly to an airline, even when compared to another similarly delayed flight. Currently, identifying optimal slot swaps between airlines requires sharing the airline-specific delay cost of each flight. However, this is not amenable as sharing these private delay costs could reveal sensitive business practices. We build upon previous work with a ledger-based mechanism that enables inter-airline collaborative rescheduling without revealing private delay costs. Specifically, two extending lines of work will be presented: (1) Introducing more realistic flight delay cost-per-unit-time functions that better model non-linear increases in private delay costs. We utilize a stochastic jump-linear model to derive and incorporate these improved private delay cost models; (2) Analyzing a case study of applying this ledger-based mechanisim for an entire year of historical operations. Previous work only examined the effectiveness of this mechanism across a 1-month period. We will focus this presentation on realistic implementation challenges for airlines and air traffic managers.",1,Brayton | Renaissance Waverly Hotel,OR in Transportation and Logistics,52
186,6518.0,Solving large-scale traveling salesman problems based on large language model,Academician,"The Traveling Salesman Problem (TSP) is an NP-hard yet fundamental combinatorial optimization problem with applications across numerous real-world fields, including transportation, logistics, and manufacturing. In recent years, artificial intelligence (AI)-based algorithms have gained popularity for solving TSP, particularly for accelerating computations on large-scale instances. In this work, we leverage the human-like understanding and reasoning capabilities of large language models (LLMs) to enhance TSP problem-solving in a more generalized manner. Our novel approach circumvents the need for distribution-specific AI training efforts by prompting LLMs directly to address TSP. Experiments using TSPLIB instances demonstrate promising results, underscoring the potential of LLMs as powerful tools for tackling combinatorial optimization problems.",2,Brayton | Renaissance Waverly Hotel,OR in Transportation and Logistics,52
187,8850.0,Exact Model and Solution Approach for the Service Agent Transport Problem,Academician,"This work proposes an alternative formulation for the Service Agent Transport Problem (SATP), a widely studied problem in autonomous transportation. SATP involves service agents performing tasks within polygonal service areas and coordinating with autonomous transport agents for high-speed movement between locations. Actions, including servicing, moving, docking, and deploying, each have known execution times to minimize the mission makespan. Our formulation eliminates dependency on predefined phases used in previous work. It incorporates acceleration techniques such as problem-specific valid inequalities, warm-start strategies, node reduction, and lower bounds from approximation algorithms. Results demonstrate superior performance across lower bounds, upper bounds, and computational time, and we illustrate the approach with a case study on scheduling autonomous underwater surveillance vehicles.",3,Brayton | Renaissance Waverly Hotel,OR in Transportation and Logistics,52
188,6595.0,Path routing and task assignment of multiple robots in a robotic mobile fulfillement system,Academician,"Robotic Mobile Fulfillment Systems (RMFS) have rapidly advanced in recent years, driven by the rise of e-commerce and the increasing adoption of robots in warehouses. Efficient path routing and task assignment are crucial for optimizing system performance. We propose a modified A* algorithm for robot path planning that accounts for potential collisions with other vehicles. Additionally, we introduce a novel priority rule aimed at reducing energy consumption. Our approach also includes task assignment algorithms designed to minimize both the risk of collisions and the total distance traveled. These advancements improve overall operational efficiency and safety in robotic warehouse environments.",4,Brayton | Renaissance Waverly Hotel,OR in Transportation and Logistics,52
189,8574.0,Intelligent Job Scheduling in Dynamic Environments: Multi-Agent Deep Q- Networks with Bidding-Based Coordination,Academician,"This paper presents a novel approach to job scheduling problems in dynamic manufacturing environments using multi-agent deep reinforcement learning with a bidding-based negotiation mechanism. The proposed framework centers on a multi-agent system that incorporates negotiation and reward mechanisms. In our framework, we adopt Deep Q- Networks for each agent to model the complex state-action value functions in the high- dimensional scheduling space. Job and machine agents engage in a bidding process for the allocation of jobs. Agents use global information for learning to extract historical information from the negotiation history to predict other agents offers and actions in the next round of negotiation and submit bids that correspond to their current states and estimated Q-values. We implement a sliding window approach for state updates, prioritized experience replay, and rapid re-bidding mechanisms to handle the dynamic nature of the environment. This mechanism facilitates agents to coordinate their actions and resolve conflicts to unexpected events such as machine breakdowns or sudden influxes of new jobs in real time. The scheduler utility agent handles the reward structure to optimize three critical metrics: resource utilization, completion time, and task waiting time. We evaluate our approach on both benchmark datasets and simulated dynamic environments. Simulated results demonstrate significant improvements in resource utilization, completion time, and task waiting time compared to traditional heuristic methods and single-agent reinforcement learning approaches.",1,Brayton | Renaissance Waverly Hotel,Applications of Game Theory and Learning,53
190,8963.0,From Rankings to Tiers: Enhancing Preference Aggregation,Academician,"Preference aggregation arises across various domains, from decision-making to recommendation systems. Traditional methods are designed to provide a collective representation of the given preferences, typically as a single fine-grained ranking over all of the alternatives. This detailed expression may overlook broader patterns in the data, namely hierarchies between the items being evaluated. This talk introduces a novel framework for creating tiers from aggregate rankings and their underlying preference graphs, by optimizing cut imbalance, a clustering metric defined as the ratio of in-degrees to out-degrees for directed edges crossing tier boundaries. We partition preferences into meaningful and distinct clusters while preserving relationa l structure. The proposed approach helps decision-makers better understand how groupings of the evaluated alternatives perform relative to one another, providing deeper insights than direct rankings in contexts where strict ordinal preferences may be impractical or overly restrictive. We provide a formal mathematical formulation of the problem, analyze its properties, and discuss algorithms for finding optimal or near-optimal tier structures. We present numerical experiments on real-world datasets, including an ISE faculty hiring network, to demonstrate the effectiveness of our approach. The results highlight the computational efficacy of our solution to the tiering problem, showcasing its scalability and practicality.",2,Brayton | Renaissance Waverly Hotel,Applications of Game Theory and Learning,53
191,8907.0,Multi-view Dynamic Graph Deep Learning for Surface Defect Identification using 3D Point Cloud,Practitioner,"Recent defect recognition methods for 3D point cloud data, mainly based on deep graph networks, can directly process large-scale object surfaces and have achieved state-of-the-art results. However, due to the imbalance in defect points and conforming points, the extracted defect features are prone to be diluted by heterologous and conforming points that are neighboring to the defects. To overcome this limitation, this work proposes a novel multi-view dynamic graph deep learning approach for unsupervised defect learning on 3D point clouds. The intrinsic characteristics of the local point set are firstly classified into defective and non-defective parts. Next, the designed feature purification operations obtain the shadow around the defects, realizing the function of ignoring unimportant information and extracting important information. The proposed structure is theoretically proven to be a general architecture that satisfies the invariance property of point clouds and can be applicable to other existing graph deep learning frameworks.",3,Brayton | Renaissance Waverly Hotel,Applications of Game Theory and Learning,53
192,7063.0,Enhancing Multi-Agent Decision-Making: Integrating Artificial Neural Networks with Game Theory in the Neural Game Equilibrium (NGE) Algorithm,Academician,"In complex and dynamic environments, the process of decision-making among multiple agents faces significant challenges, especially when it comes to swift adaptation, strategic planning, and maximizing results for all agents. This paper presents the Neural Game Equilibrium (NGE) algorithm, an novel approach that integrates the predictive capabilities of Artificial Neural Networks (ANNs) with the intricate strategies of game-theoretic concepts. The NGE algorithm is designed to transform multi-agent interaction, allowing agents to proactively predict and adapt to the strategies of their counterparts, with the goal of achieving a seamless equilibrium that enhances both personal and group objectives. The NGE algorithm utilizes sophisticated recurrent neural networks to provide real-time predictive insights, with Nash equilibrium-based strategy modification, enabling agents to achieve exceptional flexibility, stability, and alignment in uncertain situations. NGE exhibits a substantial enhancement over conventional methods via simulations in expected applications, including autonomous systems and resource distribution frameworks, establishing new benchmarks to evaluate efficiency, collaboration, and strategic coherence among agents. Our research indicates that the NGE algorithm is poised to revolutionize multi-agent intelligence, providing significant improvements in decision quality, robustness, and scalability in domains where immediate responses and coordinated strategy are essential. NGE advances the limits of intelligent synchronization and adaptive decision-making, serving as an innovative instrument for the next phase of multi-agent systems and representing a notable progression in the development of autonomous, strategically complex agent interactions.",4,Brayton | Renaissance Waverly Hotel,Applications of Game Theory and Learning,53
193,8739.0,Mixed-Integer Programming Models for Solving Semi-Cooperative Attack Graphs Problems,Academician,"In this work, we focus on the Semi-Cooperative Attack Graph Problem (SCAGP) where we try to optimize the attack strategies in interdependent infrastructures. Preliminary work proposed two mixed-integer programming (MIP) models - a continuous time MIP (MIP-C) and a period-based MIP (MIP-D) - to minimize the time required to complete attacks, considering shared resources and budget constraints among multiple attackers with valid inequalities and strategic parameter settings for improved computational efficiency. In our current work, we compare the computational efficacy of these two models over a diverse set of test instances. Additionally, we modify our MIPs to consider various objective functions such as minimize the maximum lateness and the weighted total tardiness. Furthermore, we will explore decomposition techniques such Lagrangian relaxation, branch and price, or novel decomposition methods that iteratively determine upper and lower bounds based on restricted and relaxed versions of the period-based model for eventual computational efficiency improvement. The SCAGP model can be applied to various domains, including wireless sensor network security, defense strategies for interdependent infrastructures, and spread of disinformation over a social network. By considering cooperative attack scenarios, our models contribute to a more comprehensive understanding of attack dynamics and optimization in real-world settings.",1,Waverly | Renaissance Waverly Hotel,Interdiction and Security,54
194,6818.0,Racial Bias in Automated Traffic Law Enforcement and the Price of Unjustness,Practitioner,"This paper investigates racial bias in automated traffic law enforcement (ATLE), focusing on the socioeconomic consequences of red-light cameras and their broader implications on social justice. Red-light cameras are notably prevalent in Chicago, IL, sporting a number of devices per capita multiple times larger than municipalities of similar size. People from historically marginalized groups tend to be impacted at higher rates, being overly represented in bankruptcy courts, both in relative proportion and amounts owed. Our preliminary research indicates a significant difference in ticketing rates between drivers whose vehicles are registered in mostly black neighborhoods compared to those from mostly white neighborhoods. However, it has been proven that both populations commit traffic violations at similar rates. While ATLE devices issue fines indiscriminately, their placement is not devoid of human bias. Through data-driven modeling and optimization methods, including side-constrained minimum-cost flow models and Lagrangian optimization, we are exploring how the location of ATLE devices impacts demographic groups when trying to reach different pieces of infrastructure relevant for upward social mobility, such as universities, hospitals, high-schools, etc. With statistical tools, we will analyze the significance and impact of different variables based on the demographic of the neighborhood of origin, such as the number of cameras encountered, time required to reach each type of infrastructure/destination, additional time required to take a route to avoid certain numbers or all cameras, while controlling for variables such as type of roads taken.",2,Waverly | Renaissance Waverly Hotel,Interdiction and Security,54
195,8996.0,Repair Crew Routing for Infrastructure Network Restoration under Incomplete Information,Academician,"n this paper, we consider a disrupted infrastructure network where the repair crew knows the locations of service outages but not the actual faults. Our goal is to determine the route for a single crew to visit and repair the disruptions to restore the service with minimum negative impact. We call this problem the Traveling Repairman Network Restoration (TRNRP). This problem imposes strong computational challenges due to the combinatorial nature of the decisions, inter-dependencies on the underlying infrastructure network, and incomplete information. Considering the dynamic nature of the decisions due to the change of information regarding the node status, we model this problem as a finite horizon Markov decision process. Our solution approach uses value approximation based on reinforcement learning, which is strengthened by structural results to identify a set of suboptimal moves. In addition, we propose state aggregation methods to reduce the size of the state space. We perform extensive computational studies to observe the performance of our solution methods under different parameter settings and compare them with benchmarks.",3,Waverly | Renaissance Waverly Hotel,Interdiction and Security,54
196,9025.0,Link Prediction Modelling in Illicit Networks,Academician,"Link prediction models play a key role in analyzing complex networks, forecasting missing or future connections, and revealing the dynamics of illicit trafficking networks. Despite numerous surveys on general link prediction modelling, there is a lack of comprehensive studies focusing on the applicability of these models to illicit networks. This study contributes a thorough literature review that addresses this deficiency by evaluating how effectively link prediction models can identify missing links in illicit networks. We aim to deliver a precise, well-organized overview that advances understanding of the potential of these models in disrupting illicit activities and to present preliminary results from the comparison of existing models for link prediction on a human trafficking dataset.",4,Waverly | Renaissance Waverly Hotel,Interdiction and Security,54
197,6630.0,A Bi-Objective Particle Swarm Optimization Algorithm in a two-stage flow shop with Batch Processing Machines and Limited Waiting Time Constraints,Academician,"This research explores a two-stage flow shop, where each stage has a batch processing machine capable of handling multiple jobs simultaneously as long as the machine's capacity is not exceeded. The objective is to minimize the makespan and maximum tardiness . The processing times and sizes of the jobs are given. In this context, the processing time of a batch is determined by the job with the longest processing time in the batch. Traditionally, flow shop problems incorporate a finite buffer between the two stages. . The buffer is defined in terms of the number of jobs that can wait between the two stages. However, this research focuses on the maximum waiting time between the two stages. The problem under study is NP-hard . This paper proposes a Particle Swarm Optimization (PSO) algorithm. The effectiveness of the PSO algorithm is evaluated using random instances, with results compared to those obtained from a commercial solver . Experimental results indicate that the PSO algorithm generally outperforms the commercial solver in terms of solution quality and computational time.",1,Waverly | Renaissance Waverly Hotel,Scheduling and Resource Allocation,55
198,6951.0,Sample Complexity of Risk-Neutral Optimal Control with Application to Vaccination Scheduling for Epidemic Control,Academician,"The SEIR model is a widely used framework for simulating the spread of infectious diseases, and optimal control problems can be formulated to design effective intervention strategies, such as vaccination schedules. However, the SEIR model's parameters are often uncertain. To account for this, we model the parameters as random variables and formulate a risk-neutral optimal control problem by the average over their possible values. Building on this idea, we consider a broad class of risk-neutral optimal control problems involving nonlinear ordinary differential equations with uncertain inputs. By sampling these uncertain inputs, we approximate the original problem using empirical risk minimization. Leveraging metric entropy techniques, we derive non-asymptotic sample complexity bounds for the sample-based optimal values and critical points. Numerical simulations for the vaccination scheduling problem confirm our theoretical findings.",2,Waverly | Renaissance Waverly Hotel,Scheduling and Resource Allocation,55
199,6523.0,Production Optimization considering Processing Time Uncertainty and Cell Viability in Tissue Engineered Medical Products Manufacturing,Academician,"Tissue-engineered medical products (TEMPs) are gaining significant attention for their potential to address organ shortages and improve public health. However, as the field grows, optimizing production to achieve appropriate biofunctionality (e.g., high cell viability) remains a critical challenge. Each TEMP has its unique process plan, precedence requirements, and critical constraints on cell viability, which dictate its overall success. The waiting time spent by a work-in-progress TEMP is a significant factor in dictating viability of cells at each production stage and in the final product. In order to achieve a better control while manufacturing TEMPs and maintain the desired level of cell viability, we investigated the problem of assigning and scheduling of operations required by these products on different machines. This problem is akin to a flexible job-shop scheduling problem. We present a mixed-integer linear programming (MILP) model formulation for the assigning and scheduling of TEMP operations in a deterministic environment that ensures both the stage-wise sequencing, and also, machine-wise sequencing of the operations of an order, while ensuring the desired viability of cells by controlling the waiting time of each order. The outcome of the model gives insights into planning the capacity of different facilities as well as their effective utilization by appropriately assigning orders to these facilities. We also extend our methodology to a stochastic environment where the processing times follow different distributions. Validation and effective performance of the proposed methodologies are demonstrated using discrete event simulation.",3,Waverly | Renaissance Waverly Hotel,Scheduling and Resource Allocation,55
200,5718.0,Scheduling Batch Processing Machines in a Flow Shop with Two Objectives and Limited Time Waiting Constraints,Academician,"This paper presents mathematical formulations to an application observed at an electronics manufacturing company. After assembly, the electronic products are subjected to various kinds of accelerated tests using Batch Processing Machine (BPMs) arranged in a flow shop environment. A BPM can process several jobs simultaneously. When forming batches, the total size of all the jobs contained in a batch cannot exceed the machine capacity. The processing time of the batch is equal to the longest processing time of the job contained in the batch. The configuration of the batch affects the batch processing time and hence the schedule. Due to the nature of the test conducted, the jobs cannot wait for more than a certain amount of time between two machines. The objective is to minimize the makespan (or the completion time of the last batch of jobs) and the maximum tardiness. The processing times and sizes of the jobs are given. The capacity of the machines at various stages of the flow shop is also known. This paper presents a weighted sum and a goal programming formulations of the problem under study. The experimental study highlights the usefulness of the formulations and their pros and cons. With the help of these formulations, the scheduler’s job is made easier and good quality solutions are obtained in a reasonable time.",4,Waverly | Renaissance Waverly Hotel,Scheduling and Resource Allocation,55
201,8752.0,Automated Distribution Center Palletization Simulation for Leading Pet Supply Company,Practitioner,"Design Conveyor Systems (DCS) was contracted to design an automated palletization subsystem inside a new state of the art distribution center for a leading pet supply company. DCS partnered with MOSIMTEC to develop a simulation of the palletization subsystem to ensure the system can perform at its contracted levels. The simulation model helped test the throughput of the design and identify potential weaknesses in the operational policies. Pallets arrive from manufacturers with single SKU bags of dog food, cat food, cat litter, and other bagged products. Since stores rarely need an entire single-SKU pallet, the automated subsystem builds multi-SKU pallets to ship to stores. The subsystem is a completely automated grid with eight independent zones, each zone with a dedicated overhead crane. Up to 60 manufacturer pallets are placed in each zone for the gantry crane to build mixed-SKU store pallets. All pallet moves within the grid are executed by Autonomous Mobile Robots (AMRs). Completed store pallets are plastic wrapped, labeled, and brought to an outgoing cell. The flexible AnyLogic model allowed DCS to change grid configuration, SKU definitions, pallet assignment locations, movement speeds, and AMR counts. Examples of questions answered by the engagement included time to complete daily work, number of AMRs needed, system bottlenecks, impact of failures, and impact of staff break times. The model analysis identified a potential imbalance of the eight zones driven by manufacturer SKU to zone assignments. The analysis also suggested the number of planned AMRs may be reduced without compromising throughput.",1,Kennesaw | Renaissance Waverly Hotel,Industry Case Studies I,56
202,9055.0,How is additive manufacturing disrupting the dental industry?,Practitioner,"A case study of how a 3D printing company is disrupting the dental industry. An inside look into how an additive manufacturing service bureau is leveraging their technology, material, and operational excellence to manufacturer dentures. Dentures can impact someones confidence by allowing them to smile, improve speech, and have a higher quality of life. Traditional denture production can take up to 3 weeks. Additive manufacturing has cut that time by more than half. Innovation in AM is impacting the number of times a dentist needs to see the customer for adjustments. The personalization is impacting the customers ability to choose the color, size, and shape providing the most natural look.",2,Kennesaw | Renaissance Waverly Hotel,Industry Case Studies I,56
203,8958.0,The Digital OpEx Solutions for Thermo Fisher Scientific and Eaton Corporation,Practitioner,"In this presentation, we would focus on two industry cases to develop digital solutions to improve the operational excellence at the facilities of Thermo Fisher Scientific (NYSE: TMO) and Eaton Corporation (ETN). Both Master’s level capstone projects were conducted and executed by the 3-4 team members simultaneously in a period of 9 months. At TMO, there was a lack of data visualization of their product quality and foam production issues. The main purpose of Case 1 was to standardize and automate their repair, rework, scrap data files and evaluate foam processing data of the ULT freezers and HPLRF refrigerator-freezer for the life science applications. A total of six digital dashboards was developed using Power BI to monitor their RMA, Red Tag, Scrap, and Foam Processing data in Asheville, NC. For the ETN electrical power systems division in S. Milwaukee, WI, Case 2 was to develop a Kanban system and centralized storage system to improve their cable production backlog, reduce excess inventory, and rationalize SKU cables. A supermarket storage system was designed using Solidworks to replenish the top 15 kanban cables. An excel-macro was further developed to reduce the SKU rationalization of the non-kanban cables. This engineering consulting-based industrial approach has enriched the IE graduate students with industrial hands-on experience and customer engagement skills that would well-prepare them to become the next generation ISE leaders. It has bridged the gap and enabled a sustainable partnership between industry and academia. Both industrial sponsors are currently implementing these digital OpEx solutions at their facilities.",3,Kennesaw | Renaissance Waverly Hotel,Industry Case Studies I,56
204,9365.0,A Distribution Industry Case Study on Building Resilience Through Business Continuity Strategies,,"ISEs are responsible for designing and optimizing critical business processes across industries. However, as processes become increasingly reliant on technology, they are more susceptible to IT outages and cyber-attacks. As part of process design and optimization, engineers should consider how to reduce technology risks and evaluate how their processes can be maintained during a disruption. This session will provide a case study of how a large distribution company identified risks to process continuity by conducting a business impact analysis, addressed risks through business continuity planning, and conducted tests of plans to enable organizational readiness. The case study will include practical actions businesses and ISEs can take to reduce business continuity risks.",4,Kennesaw | Renaissance Waverly Hotel,Industry Case Studies I,56
205,6897.0,Point of use tote population sizing and direct ship process evaluation,Practitioner,"Demand for an Automated Storage and Retrieval System (ASRS) is expected to increase over the next two years. Long lead capital projects may be necessary to expand capacity to meet demand. Therefore, evaluating proposed counter measure effectiveness early is necessary to ensure continuity of service. This presentation will evaluate the impact of two countermeasures: 1) Direct shipping from Goods-To-Person (GTP) stations to increase the number of kitting transactions performed at GTP stations in 24 hours. This model quantifies GTP capacity increase and evaluates various direct ship alternatives for manpower and space resource requirements. 2) Adjusting each job’s kit tote population to minimize tote storage space consumed. A kit tote contains a group of parts to perform a job and is delivered from the warehouse to the assembly line point of use. Unique foam shadow boxes are designed to identify, protect, and present the parts well to the end user. Therefore, each kit tote design is unique to a single job. This study evaluates the effectiveness of setting kit tote population using two methods: A) Same quantity for each job of a given production model (current state), B) Different quantities for each job based on how long customers keep kit totes before returning them. This model quantifies each approach’s likelihood of having a tote available when needed and the impact on tote storage size required to meet demand. A forecast for tote quantity and storage requirements will be generated for various demand profiles and used for capital planning purposes.",1,Stanhope | Renaissance Waverly Hotel,Industry Case Studies 2,57
207,5730.0,Impact of Global Warming and Flooding on Marine Heat Exchangers: Engineering Challenges and Adaptive Solutions,Practitioner,"Introduction: Heat exchangers are vital in marine machinery, transferring heat from oil and other fluids to prevent overheating. However, recent environmental challenges, including severe flooding attributed to global warming in Pakistan’s Sindh province, have heightened the clogging issues in marine heat exchangers due to debris-laden floodwaters. This has increased both maintenance costs and machinery downtime, emphasizing the need for innovative solutions. Case study: Our organization, specializing in the repair and servicing of marine heat exchangers, conducted flow analyses and experimental trials to address this challenge. We implemented several solutions: 1. Enhanced Sea Chest Strainers: By adding perforated plates (5mm holes in the sea chest strainer and 3mm holes before heat exchangers), we intercepted larger debris, effectively reducing clogging frequency. 2. Usage Optimization: We advised restricting machinery use during low tide, where debris intake is higher. 3 Optimal Sea Suction Positioning: Where possible, we advised utilizing machinery with sea suctions positioned farther from the jetty. This helps prevent the intake of floating debris that commonly accumulates near fender areas, further reducing clogging risk. Conclusion and Results: These measures significantly reduced clogging, maintenance frequency, and service costs, positively impacting operational efficiency and strengthening client relations. Our solutions have proven to be effective adaptive strategies for addressing environmental impacts on marine heat exchangers, reinforcing our commitment to sustainability and cost-effectiveness.",3,Stanhope | Renaissance Waverly Hotel,Industry Case Studies 2,57
208,5180.0,Some Retirement Planning Tips,Practitioner,"Retirement is very different for everyone, but this presentation will provide some tips & observations, from my Retirement. Several key areas to consider, when planning for your Retirement will be presented, including: when to retire, the initial transition, location issues, financial planning, healthcare, insurance, social security, home computing, volunteer activities, pursuing creative interests, and traveling differently in Retirement.",1,Stanhope | Renaissance Waverly Hotel,Professional Development I,58
209,5181.0,Some Job Search Advice,Practitioner,The presentation will offer some advice to ISE students doing a Job Search. It will cover several important topics: • Skills Hiring Companies look for in new ISEs • General Job Search tips • Industries ISEs work in • ISE-related Job Titles • A sample Resume format • Some key words to use in your Resume • Some reasons why Resumes are thrown out • Sample Job Interview questions,2,Stanhope | Renaissance Waverly Hotel,Professional Development I,58
210,6043.0,"Applications of Value Engineering’s Function Analysis to Overcome the Perfect Storm: Rising Costs, Global Competition, Supply Chain Disruptions, and Evolving Market Dynamics.",Practitioner,"US manufacturers grapple with complex challenges, including rising costs, intense global competition, supply chain vulnerabilities, and shifting consumer preferences. To maintain competitiveness, a strategic approach is imperative. Value engineering, a systematic methodology outlined in ASTM E1699, provides a powerful tool to address these issues. Central to Value Engineering is Function Analysis, a technique that breaks down a product or system into its essential functions. By scrutinizing these functions, engineers and managers frame problems and designs with a changed viewpoint that triggers opportunities for innovation, improved performance, and cost reduction. This approach encourages a fresh perspective, challenging conventional thinking and inspiring creative solutions. By prioritizing functions over physical form, Function Analysis enables exploring alternative designs, materials, and processes that can significantly enhance value. This presentation introduces the principles of function analysis and its applications in actual cases to reframe challenges, optimize product designs, improve manufacturing processes, and enhance product performance. It also shares how this tool will fit your existing improvement core tools set. Through value engineering’s function analysis, manufacturers can optimize product design, streamline production, and reduce costs without compromising quality or performance. By embracing this methodology, businesses can adapt to changing market conditions, strengthen their competitive position, and ensure long-term success.",1,Kennesaw | Renaissance Waverly Hotel,ISE Tools,59
211,5941.0,Mastering the Art and Politics of Building Models in the Real World,Practitioner,"Implementing an optimization model in a real-world setting is no small feat. Beyond the technical complexities of building the model and accessing the right data, there’s an intricate layer of organizational dynamics to navigate. Decision-makers often have varying levels of familiarity with modeling, and some may overestimate their understanding, creating unique challenges for the modeler. How can we bridge these gaps? Drawing from experiences as both a modeler and a manager, this session will explore some of the most significant challenges encountered and offer practical strategies to overcome them.",2,Kennesaw | Renaissance Waverly Hotel,ISE Tools,59
212,9216.0,Driving Organizational Results using the OKR Method,Practitioner,"Objectives and Key Results (OKR) are an effective goal-setting and leadership tool for communicating what organizations want to achieve, how the company will achieve success and how employees and teams contribute to the company’s objectives. My presentation will talk how to implement a OKR methodology into their organization as most organizations are familiar with Key Performance Indictors (KPI). KPIs can be great for measurement, but they are standalone metrics. The KPI are good measurement to tell you if your performance is good or bad, but they don’t communicate what direction you team needs to go in. OKR provide that direction. The Objective describes what you want to accomplish, and the Key Results describe what you want to accomplish, and the Key Results describe how you know you’re making progress. KPI make a great key result. OKR’s are used to lead that shows both what they want to accomplish and how they’re going to get there. The benefits include improved focus, transparency and alignment required to organize and move employees to achieving shared, bold corporate goals.",3,Kennesaw | Renaissance Waverly Hotel,ISE Tools,59
213,6022.0,The Big Benefits of Small Firms: Why Early-Career Industrial Engineers Should Consider Joining Smaller Companies,Practitioner,"Starting a career at a smaller firm presents unique and valuable opportunities for industrial and systems engineers, offering experiences that often differ from those found in large corporate settings. This presentation will share valuable insights for both early-career professionals and students by drawing on personal experiences transitioning from Fortune 500 companies to a Virginia Tech alumni-founded consulting firm of fewer than 25 employees. Attendees will explore how smaller firms foster a rapid acceleration of technical and managerial skill development, providing an immersive environment where new hires work on diverse projects, interact closely with leadership, and make a visible impact on outcomes. Unlike more segmented roles commonly found in larger organizations, this structure encourages broader learning and innovation. Industry professionals will gain practical perspectives on how lean methodologies can be effectively applied even in resource-constrained environments. Students will discover how the high visibility and hands-on nature of smaller firms fosters adaptability, problem-solving skills, and strong professional relationships, all while overcoming limitations on resources and fluctuating project demands. Practical strategies for overcoming these constraints and thriving within a small firm environment will be shared, highlighting how such experiences cultivate resilient engineering professionals. Whether attendees are professionals seeking to refine lean practices or students evaluating career paths, they will leave with a deeper understanding of the often-overlooked benefits of small firms and gain actionable insights to guide their own career decisions.",1,Kennesaw | Renaissance Waverly Hotel,Professional Development 2,60
214,7081.0,Rewards & Recognition to Enhance Performance and Engagement,Practitioner,One of the top motivators of employees is recognition for a job well done. Yet most managers don’t truly understand the power behind frequent rewards and recognition. Well-constructed recognition provides the single most important opportunity to parade and reinforce the specific behaviors you hope others will emulate. The three basics of recognition will be covered. The target audience for this presentation are Project Managers and Managers.,2,Kennesaw | Renaissance Waverly Hotel,Professional Development 2,60
215,6454.0,Process View Simulation Using the Kotlin Simulation Library,,"The Kotlin Simulation Library (KSL) provides an application programming framework and modeling constructs for developing and analyzing discrete-event dynamic systems. This presentation provides an overview of the library functionality for constructing discrete-event simulation models using the process-view. An overview of the process-view implementation based on Kotlin coroutines is provided. Then, an overview of the process view functionality is provided. Finally, the process-view modeling constructs are illustrated via an example. The capabilities of the library to perform experiments and statistical analysis are presented. Practitioners can benefit from the library due to its comprehensive functionality and open-source availability. Academicians can utilize the library as a low cost software platform for teaching students the basics of simulation.",1,Kennesaw | Renaissance Waverly Hotel,Academic Research Related to Industry I,61
217,5646.0,A Federated Distributionally Robust Support Vector Machine with Mixture of Wasserstein Balls Ambiguity Set for Distributed Fault Diagnosis,Academician,"Performing fault diagnosis tasks using geographically dispersed data is crucial for original equipment manufacturers seeking to provide long-term service contracts to their customers. Due to privacy and bandwidth constraints, the classification models must be trained in a federated fashion. Moreover, due to harsh industrial settings the data often suffers from feature and label uncertainty. Therefore, we study the problem of training a distributionally robust (DR) support vector machine (SVM) in a federated fashion over a network comprised of a central server and G clients without sharing data. We consider the setting where the local data of each client g is sampled from a unique true distribution Pg, and the clients can only communicate with the central server. We propose a novel Mixture of Wasserstein Balls (MoWB) ambiguity set that relies on local Wasserstein balls centered at the empirical distribution of the data at each client. We study theoretical aspects of the proposed ambiguity set, deriving its out-of-sample performance guarantees and demonstrating that it naturally allows for the separability of the problem. Subsequently, we propose two distributed algorithms for training the global FDR-SVM: i) a subgradient method-based algorithm, and ii) an alternating direction method of multipliers (ADMM)-based algorithm. We derive the optimization problems to be solved by each client and provide closed-form expressions for the computations performed by the central server during each iteration for both algorithms. Finally, we thoroughly examine the performance of the proposed algorithms in a series of numerical experiments utilizing both simulation data and popular real-world datasets.",1,106 | Cobb Galleria Centre,Advances In Machine Learning Methods I,62
218,6360.0,MissRASS-CPD: Robust Selection of Joint and Individual Features for Tensor Data Analysis,Academician,"Nowadays, multi-modal data collected from complex systems is ubiquitous. In such systems, each modality provides myopic information about the system. Therefore, fusing the data from all the modalities is critical to understanding the system as a whole. For example, to diagnose Inflammatory Bowel Diseases, ulcerative colitis (UC) and Crohn's disease (CD), different omics data are needed. Transcriptomic provides information about gene expressions, proteomic about protein levels, while metabolite about molecule concentrations produced by microorganisms. Using only one type of omics data results in limited diagnosis accuracy and inefficient treatment plans. Furthermore, to improve diagnosis and treatment, differences and similarities across groups (UC, CD, and healthy individuals) need to be incorporated into the analysis. In this work, we propose a framework that fuses multi-modal data while exploiting the similarities and differences across groups for accurate diagnosis. The framework finds the joint latent variables for each group and the individual latent variables for each modality, and identifies the key features in each modality. Furthermore, the framework is capable of imputing missing data and removing outliers. Our method decomposes the data into a low-rank component, that captures the true signal, and a sparse component, that captures the outliers. Then, using CP decomposition on the low-rank component with a sparse group Lasso penalty, we simultaneously learn the joint and individual latent variables. The performance of the proposed framework is illustrated through simulated and real data examples. In particular, we use the proposed framework to identify key biomarkers for UC and CD patients.",2,106 | Cobb Galleria Centre,Advances In Machine Learning Methods I,62
219,5739.0,Enhancing EO/IR Sensor Data Analytics with a Two-Stage Framework for Clutter Reduction and Signal Classification,Academician,"In recent years, there has been a significant surge in the use of sensors for data acquisition, with Electro-Optical/Infrared (EO/IR) sensors being a prominent example. These sensors are crucial for various applications, ranging from surveillance to security operations. As a result, EO/IR sensors generate vast volumes of high-dimensional data, necessitating efficient processing for effective decision-making and analysis. A key challenge with these sensors is the prevalence of clutter, which compromises data quality and highlights the critical need for robust filtration techniques to enhance the integrity and reliability of sensor data. This paper proposes a data-driven, two-stage framework to analyze and classify EO/IR sensor data. The first stage employs Dynamic Time Warping (DTW) to normalize trajectory data over time, while the second stage applies a decision tree algorithm to classify trajectory signals into categories such as small planes, birds, helicopters, and clutter. Real EO/IR data, including attributes such as longitude, latitude, and altitude, were used to evaluate the framework. The proposed approach is expected to improve clutter reduction, computational efficiency, and classification accuracy. This research presents a scalable and interpretable framework for implementing decision tree models in practical EO/IR sensor applications, offering new insights into sensor data analytics and enhancing automated detection systems.",1,106 | Cobb Galleria Centre,Advances In Machine Learning Methods II,63
220,6604.0,A Novel Probabilistic Masking-based Hybrid Random Projection for Dimensionality Reduction,Academician,"Dimensionality reduction plays a vital role in the handling of high-dimensional data in various fields including machine learning, bioinformatics, and image processing. In this study, we introduce a novel probabilistic masking-based hybrid random projection (PM-HRP) method that combines the strengths of dense and sparse random projections. The key innovation of PM-HRP is the masking probability, which controls the selection between dense and sparse projection matrices, and offers a flexible trade-off between computational efficiency and information retention. In addition, the PM-HRP introduces linear and nonlinear weighting schemes to further enhance its adaptability to complex data structures. We provide both theoretical and empirical analyses of the PM-HRP. The method ensures that the geometrical properties of the data are preserved through dimensionality reduction. Extensive simulations and real-world datasets using the period changer and toxicity datasets demonstrate that PM-HRP consistently outperforms classical random projection methods, including normal and Achioloptas random projections, in terms of minimising average distortion. In particular, PM-HRP excels in high-dimensional settings where maintaining distance preservation is crucial. Using nonlinear weights adds a unique capability to PM-HRP, enabling it to capture complex nonlinear relationships in datasets while maintaining computational feasibility. This method offers a robust and scalable solution for a wide range of high-dimensional data applications, with significant benefits over classical methods in terms of the average distortion.",2,106 | Cobb Galleria Centre,Advances In Machine Learning Methods II,63
221,5926.0,Game Theory in Disaster Management: Enhancing Collaborative Prescribed Fire Strategies for Public and Private Stakeholders,Practitioner,"Prescribed fires (Rx) reduce wildfire risks and support ecosystem health, but their management is complicated by conflicting objectives between public and private stakeholders. Public agencies focus on community safety and ecological preservation, while private landowners and businesses prioritize economic concerns and liability risks. Current Rx management frameworks fail to address these conflicting goals, often focusing on individual stakeholders rather than the trade-offs between public and private interests. This research aims to develop a game-theoretical framework to facilitate collaboration between stakeholders with differing priorities. The primary research question is: How can stakeholders collaborate to optimize Rx operations, reduce wildfire risks, and improve environmental and economic outcomes? To address this, the study will: 1) Identify and analyze conflicting objectives through surveys, interviews, and literature review, providing a comprehensive understanding of stakeholder perspectives; 2) Develop a game-theoretical model to quantify trade-offs and identify strategies for cooperation, focusing on cost-sharing, risk distribution, and ecological impacts; and 3) Design a multi-stakeholder engagement framework to improve communication, decision-making, and collaboration using simulation tools and decision-support systems. This research contributes to Rx management by enhancing stakeholder collaboration, leading to increased Rx use, reduced wildfire risks, and better ecosystem preservation. The decision-support tools developed will aid fire managers and policymakers in making data-driven decisions, with the findings shared through conference presentations, policy briefs, and academic publications",3,106 | Cobb Galleria Centre,Advances In Machine Learning Methods II,63
222,6567.0,BLAZE TRACK: Developing Dynamic Wildfire Risk Maps Leveraging MODIS Satellite Imagery and Advanced AI Algorithms,Academician,"Due to climate change, the frequency and intensity of wildfires are increasing globally. Wildfire poses a pervasive threat to communities, causing loss of lives, disrupting critical infrastructure services, and economic loss. Therefore, identifying accurate risk hotspots at granular spatiotemporal resolution and predicting the future risk for wildfire is crucial for preventive measures. To develop BLAZE TRACK, we used satellite imagery data from the Moderate Resolution Imaging Spectroradiometer (MODIS) to extract information on the historical burnt areas from 2015-2022 at a weekly temporal scale. Every pixel (spatial dimension 250m*250m) is labeled burned or unburned. This is integrated with indices like normalized difference vegetation index (NDVI), topography, elevation, and climate data. Leveraging advanced AI algorithms like attention-based transformers, Bayesian neural networks (BNN), spatial-temporal attention networks, and self-attention long short-term memory (LSTM), a dynamic risk mapping is developed for the Southwestern US, including the six states—California, Nevada, Utah, Arizona, Colorado, and New Mexico- most susceptible to wildfires due to their unique weather, topography, and vegetation cover. Our preliminary results show that the dynamic wildfire risk is predicted with ~95% accuracy. Our analysis also identifies the key predictors (e.g., climate, topographical, and vegetation factors) for the wildfire risk. The high, low, and moderate wildfire-risk zones and their seasonality can be detected, better informing emergency managers of proactive, risk-informed preparedness measures. The analysis can be further expanded to understand how the wildfire risk varies for socially vulnerable communities by integrating socio-demographic information of the regions into the modeling framework to formulate equitable policy directives.",1,106 | Cobb Galleria Centre,AI Methods for Wildfire and Disaster Management,64
223,5892.0,Desert Resilience: Effective Roadway Management to Combat Arizona’s Rising Wildfire Threats,,"Wildfires have become more frequent, larger, and intense in recent decades, with this trend likely to continue worsening. In Arizona, recent wildfires have significantly damaged roads, impacting infrastructure, public safety, and the economy. Climate change, with prolonged drought and extreme weather, combined with land development and past fire-suppression practices, has intensified wildfires, especially in vulnerable desert ecosystems. These fires destroy native plants, disrupt ecosystems, and heighten the risk of road and infrastructure damage. Poorly managed roadsides can also contribute to fire risk by altering native vegetation and introducing invasive species. Effective management of right of way near critical roadways is vital for aiding post-fire vegetation recovery and reducing infrastructure maintenance costs. This study examines Arizona wildfires from 2009 to 2021, focusing on areas where the state highway network has been affected. Eight study sites where wildfires caused roadway damage are selected based on a range of factors, including multiple biomes, wildfire history and severity, and the extent of transportation infrastructure damage. At these sites, we analyze the relationship between ADOT’s post-wildfire maintenance efforts and vegetation recovery and identify the key risk factors associated with effective roadside management strategies for wildfire prevention, control, and recovery. The research findings have the potential to identify successful management strategies that can help reduce costs and roadway damages from wildfires, while also supporting ecosystem recovery across the state. The study provides important insights for developing practical interventions to minimize wildfire impacts on transportation networks and promote the safe, efficient functioning of Arizona’s transportation network.",2,106 | Cobb Galleria Centre,AI Methods for Wildfire and Disaster Management,64
224,8979.0,All-Hazards Risk Analysis: Interrelated and Integrated Risk for Installation Planning,Practitioner,"Military installations need to modernize to mitigate current and future threats from increased complexity and potential risks. Central to future military installations is revolutionizing installation master planning, the process to plan for the development of an installation’s land, facilities, and infrastructure while ensuring sustainability, efficient use of resources, and risk management. PLANNER, an Army Installation Modernization Pilot Program (AIMP2) project, is a new platform to digitize and operationalize integrated installation planning, modernizing the process, and lowering the manpower required by master planners, energy managers, and environmental planners. Within PLANNER, a new process, All-Hazards Risk Analysis (AHRA), has been developed to analyze all hazards and risks related to military installations, such as climate-change, cyber, and human-induced risks. This paper will describe the methodology and development of the AHRA and feedback from stakeholders.",3,106 | Cobb Galleria Centre,AI Methods for Wildfire and Disaster Management,64
225,4900.0,Post-Disaster Damage Assessment from High-Resolution Satellite Imagery using Dilated LinkNet Hierarchical Radon Transformer,Academician,"Effective post-disaster damage assessment is critical for supporting timely disaster response and recovery efforts. To address the challenges of large-scale damage analysis, in this study, we propose a novel deep learning model, the Dilated LinkNet Hierarchical Radon Transformer (DL-HRT) network. The DL-HRT network integrates feature extraction, change detection, and damage classification within a unified framework, enabling accurate and efficient satellite imagery analysis. This model was trained on datasets including the Haiti Earthquake dataset and the xBD dataset from the xView2 Challenge, achieving precise classification of damage severity across four levels, from minor damage (level 1) to destroyed (level 4). Experimental results demonstrate that the DL-HRT network outperforms state-of-the-art models, such as DAHiTrA and BiTransUNet, in terms of accuracy, computational efficiency, and robustness. Such a key innovation lies in its incorporation of a dilation module within the U-Net architecture, preserving low-level features during down-sampling. This capability enhances the network’s ability to effectively capture fine-grained details and global contextual information. Additionally, the inclusion of hierarchically structured difference blocks, leveraging positional embeddings derived from Radon image transformations, improve both classification speed and accuracy. By providing a comprehensive and efficient solution for post-disaster damage assessment, the DL-HRT network has the potential to significantly enhance disaster management operations, aiding rapid and informed decision-making during response and recovery efforts.",4,106 | Cobb Galleria Centre,AI Methods for Wildfire and Disaster Management,64
226,5364.0,Classical mechanics meet deep learning: ship hazardous navigation detection system,Academician,"Vessel accidents at sea cause significant loss of life and property, which highlights the growing importance of early detection of hazardous ship behavior. While existing studies have primarily focused on analyzing hazardous navigation by tracking ships’ motion, this study aims to recognize and predict potential hazards in advance. For this, we propose a sophisticated hazard detection model using the concept of force in ship dynamics by applying Newton's laws of motion from classical mechanics. Specifically, if the magnitude of the total force generated during ship movement, excluding the engine’s propulsive force, exceeds a certain threshold, it means that external factors independent of the captain’s intention are influencing the ship’s movement. Thus, this study defines and analyzes the residual force as a function of forces generated by environmental factors and operational variables, after excluding internal engine forces from the ship’s total force. In this process, Newton's laws of motion are used to precisely capture these interactions, enabling interpretation of the ship’s motion and assessment of potential hazards. With this concept from classical mechanics, we propose an anomaly detection technique based on a transformer-based autoencoder, designed to automatically detect abnormal movements in the ship’s sailing pattern. Our approach offers a means of early identification of potential hazards that ships may face by combining classical mechanics with deep learning. We believe that our results can contribute to improving maritime safety by providing valuable information to proactively address safety challenges in this field.",1,113 | Cobb Galleria Centre,AI Methods for Complex Systems I,65
227,9057.0,Sequential Actuator Placement Selection and Optimization for Aircraft Fuselage Assembly via Reinforcement Learning,Academician,"Precise assembly of composite fuselages is critical for aircraft assembly to meet the ultra-high precision requirements. However, precise composite fuselage assembly is challenging due to compliant structure s, complex material properties, and dimensional variabilities of incoming fuselages. In practice, actuators are required to adjust fuselage dimensions by applying forces to specific points on fuselage edge through pulling or pushing force actions. The positioning and force settings of these actuators significantly influence the efficiency of the shape adjustments. The current literature usually predetermines the fixed number of actuators, which is not optimal in terms of overall quality and corresponding actuator costs. This paper introduces a reinforcement learning (RL) framework that enables sequential decision-making for actuator placement selection and optimal force computation. Specifically, our methodology employs the Dueling Double Deep Q-Learning (D3QN) Algorithm to refine the decision-making capabilities of sequential actuator placements. The environment is meticulously crafted to enable sequential and incremental selection of an actuator based on system states. With the consideration of current fuselage dimensional variabilities, actuator placements and fuselage structure properties as the state representation, the learning efficacy of D3QN is significantly improved. The proposed methodology has been comprehensively evaluated through numerical case studies and comparison studies, demonstrating its effectiveness and outstanding performance in enhancing assembly precision and minimizing the number of actuators employed.",2,113 | Cobb Galleria Centre,AI Methods for Complex Systems I,65
228,8838.0,Improving and Centralizing Communication of Environmental Data using Existing Technology Tools,Practitioner,"The South Florida Water Management District is responsible for flood control, water supply planning, water quality and ecosystem restoration in a 16-county area of South Florida. Following soon-to-be-resolved federal/state consent orders associated with construction and operation of over 60,000 acres of constructed stormwater treatment wetlands, the agency must meet extremely stringent water quality standards when discharging the treated water into the Everglades. This requires that information-sharing among the agency’s many organizational units be much more efficient, with a solution needed in the coming months. The solution must be sustainable and meld disparate types and sources of data into a single communications tool, without the need for a custom programming project from the IT department, but with the ability for the data providers and users to develop their modules using a variety of existing technology tools. Such a solution would be more nimble, flexible, and sustainable than a custom program that would require intensive up-front support from IT, and extensive IT assistance to adjust content. This case study describes the process(es) used for multiple groups within the agency to create a “tool” consolidating different types of data from multiple environmental-related databases, leveraging the agency’s existing technology, and accommodating the differing skill sets among the groups. The result provides a one-stop-shop for all information that factors into operating the stormwater treatment areas (STAs), and significantly reduce the need for dozens of emails and phone calls, and provide better information exchange during internal discussions and meetings with regulators.",3,113 | Cobb Galleria Centre,AI Methods for Complex Systems I,65
229,5871.0,Digital Twin-Enabled Emissions Management System for Real-Time Urban Health and Sustainability,Practitioner,"Urban health and sustainability have become critical concerns with the rapid urbanization and serious environmental challenges. The inherent complexity and uncertainty of urban systems make the measurement and control of emissions difficult. This research explores the potential of digital twins for real-time urban health and sustainability management. This paper proposes a digital twin-enabled urban health and sustainability system comprising three main modules: the data collection end, the back end, and the front end. The data collection end utilizes multi-sensor networks to gather real-time data on various urban parameters. The back end integrates dynamic and static datasets, emissions estimation algorithm, WebGL-based visualizations, and service manager. The front end provides user interfaces through desktop and mobile clients, enabling stakeholders to interact with the system seamlessly. This case study demonstrates the development and implementation of the designed digital twin system for New York, highlighting its effectiveness in enhancing the measurement and management of urban emissions.",4,113 | Cobb Galleria Centre,AI Methods for Complex Systems I,65
230,8580.0,Optimizing Multi-Lane Intersection Performance in Mixed Autonomy Environments,Academician,"One of the significant challenges in implementing autonomous driving at multilane intersections is the coexistence of human-driven vehicles (HDVs) and connected autonomous vehicles (CAVs). This paper introduces an innovative method for managing traffic by utilizing a combination of Graph Attention Networks (GAT) with soft actor-critic reinforcement learning. GAT modeling the dynamic and graph-structured nature of multi- lane intersections while Soft actor-critic is used for adaptive decision-making. GATs capture the complex spatial and temporal relationships between vehicles, lanes, and traffic signals across varying intersection topologies. Soft Actor-Critic support a balanced approach to exploration and exploitation by maximizing entropy. Our state space incorporates real-time traffic data including vehicle position, speed, and vehicle type (CAV or HDV). Our action space consists of a continuous action space for controlling connected and automated vehicles (CAVs) and a hybrid discrete-continuous model for managing traffic signals. This setup allows us to achieve precise control over CAVs and traffic signals thus focusing on multiple objectives such as reducing travel time, maximizing throughput, ensuring safety, and promoting fairness between CAVs and conventional vehicles (HDVs). Key performance metrics, including travel time reduction, throughput maximization, safety, and fairness between CAVs and HDVs are optimized. The model is evaluated using a SUMO simulation modeled a 4-way intersection with varying traffic densities and CAV peneration rates. Simulation results demonstrate our model outperformed traditional methods across diverse scenarios.",1,113 | Cobb Galleria Centre,AI Methods for Complex Systems II,66
231,6260.0,Resilience of Connected and Autonomous Vehicles to Cyber-Physical Disruptions,Academician,"The rise of Connected and Autonomous Vehicles (CAVs) has spurred research into their impact on traffic congestion, which affects travel time, fuel consumption, emissions, and throughput. Additionally, disruptions like lane closures and cyber infrastructure failures create significant obstacles for reliable and efficient travel. While strategies like high-occupancy vehicle lanes aim to reduce congestion, CAVs offer new possibilities to enhance road network performance. We developed and validated an agent-based simulation model to capture interactions between CAVs, non-CAVs, traffic signals, and road infrastructure in both urban and highway environments. Experiments conducted on selected study sites in Oklahoma showed that CAVs improved travel times, even with 30% communication failures and 20% random lane closures. CAVs effectively managed congestion through redundancy mechanisms during communication disruptions; however, CAVs with active communication performed 5% better than those relying solely on redundancy.",2,113 | Cobb Galleria Centre,AI Methods for Complex Systems II,66
232,7016.0,Dynamic sensor selections for remote prognostics,,"Precise prognostics of operational units is crucial to avoid catastrophic failures. Units in sectors like automotive and aviation, often operating remotely, are equipped with advanced computing systems and wireless sensors that collect operational data. While local computing systems can now autonomously evaluate maintenance needs using server-deployed algorithms, Original Equipment Manufacturers (OEMs) still desire continuous collection of the original sensor data to a central server for subsequent data analysis. However, high transmission costs and bandwidth limitations in practice often necessitate selective transmission of sensor data in real time. In addition, dynamic sensor selection must adapt to changing operating environments and accurately characterize the underlying failure evolution. In this study, we leverage the computational capability of local edge units to dynamically select the most informative sensors at the edge unit for transmission in order to achieve accurate prognostic performance. Specifically, we formulate the sensor selection problem as an optimization task that minimizes prediction accuracy loss due to sensor reduction while adhering to the bandwidth constraints. A key challenge in this optimization is that the predictive model is a complex neural network which is challenging to optimize. To address this issue, we propose to approximate the optimization problem within a small neighborhood using linear systems. In this way, the optimization problem can be solved using an efficient dynamic programming algorithm. Validation studies on an aircraft gas turbine engine dataset demonstrate the superiority of our method over other benchmarks.",3,113 | Cobb Galleria Centre,AI Methods for Complex Systems II,66
234,8674.0,Multi-objective optimization based on data analytics for product quality composition design in steel production process,Academician,"The steel composition design process aims to establish a reliable analytical model that links chemical composition with material performance. In this study, a neural network-based model is used to build the relationship between chemical composition and key mechanical properties, including tensile strength, hardness, and ductility. This model helps to better understand how alloying elements influence the desired steel properties. To optimize the steel composition, a multi-objective optimization algorithm with a fractal mutation strategy is employed. This approach identifies optimal compositions that meet specific performance criteria, allowing for a more efficient design process. The fractal mutation strategy enhances the algorithm's ability to explore a wider solution space, improving its convergence on the best possible compositions. By utilizing Pareto-based optimization, the multi-objective evolutionary algorithm (MOEA) identifies a set of trade-off solutions that balance conflicting objectives, such as maximizing strength while minimizing material waste or cost. This gives decision-makers a range of optimal compositions to choose from tailored to different steel grades and applications. The practicality and stability of the proposed algorithm are verified using real-world production data from steel manufacturing enterprises. The results demonstrate that this approach improves the ability to design high-performance steels with customized properties, reduces material waste, and increases process efficiency. Additionally, the algorithm’s generalization capability is tested on multi-objective optimization benchmark problems, confirming its robustness and potential for addressing complex challenges in modern steel manufacturing. This suggests that the developed technique is a powerful tool for optimizing steel composition and improving manufacturing processes.",2,113 | Cobb Galleria Centre,AI Methods for Complex Systems III,67
235,8949.0,Privacy-Preserving Porosity Prediction Using Spatiotemporal Graph Neural Networks in Laser-Based Additive Manufacturing Processes,,"Porosity control in the LENS (Laser Engineered Net Shaping) additive manufacturing process is critical for ensuring structural integrity and durability, especially in high-performance applications. Traditional predictive models, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), often struggle to accurately capture the complex layer-by-layer connectivity in in-situ monitoring. This leads to decreased accuracy in porosity predictions and impacts quality certifications. Moreover, centralized machine learning approaches raise significant privacy concerns due to the proprietary nature of manufacturing data and the lack of robust mechanisms to protect sensitive information during model training. To address these challenges, this study proposes a unified Spatiotemporal Graph Neural Network (ST-GNN) framework that integrates spatial-temporal modeling and differential privacy (DP). The architecture combines Graph Convolutional Networks (GCNs) and Recurrent Neural Networks (RNNs) to capture spatial and temporal dependencies while employing differentially private stochastic gradient descent (DP-SGD) to protect data confidentiality during training. Experiments demonstrate superior accuracy in predicting porosity labels and sizes compared to baseline models. Additionally, privacy impact is quantified by measuring the privacy budget under different configurations, validating robust data protection with minimal performance trade-offs. This approach offers a privacy-preserving solution for quality certification, advancing secure, data-driven innovations in additive manufacturing.",3,113 | Cobb Galleria Centre,AI Methods for Complex Systems III,67
236,6252.0,Utilizing Explainable Machine Learning for Robust and Accurate Fraud Detection in E-Commerce,Academician,"In the rapidly growing field of e-commerce, fraudulent actions are responsible for significant financial losses, and the challenge of detecting financial fraud remains critical. In this study, we utilize a real dataset, specifically the IEEE-CIS Fraud Detection dataset provided by Vesta Corporation, which contains a diverse mix of real-world e-commerce transactions. We implement supervised machine learning models to identify fraudulent transactions while addressing the common problem of class imbalance using SMOTE and Random Over Sampling methods. Our evaluation employs machine learning algorithms such as Random Forest, XGBoost, and Support Vector Machines (SVM) to assess how various models and oversampling techniques impact detection performance. We also apply explainable AI methods to enhance the clarity of model behaviors and feature significance. Local interpretable methods, including Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP), and global interpretation methods, such as permutation feature importance and univariate partial dependencies, are implemented to determine features that are crucial for robust prediction. Performance is evaluated using crucial metrics including accuracy, precision, recall, F1-score, and the area under the ROC curve (AUC-ROC). The results demonstrate that both the oversampling strategy and the machine learning model used significantly influence detection effectiveness and provide insights on effective combinations to enhance fraud detection. Overall, this research lays the foundation for designing more effective and interpretable fraud detection systems to help financial institutions mitigate losses and reduce the incidence of fraudulent transactions.",1,113 | Cobb Galleria Centre,AI Methods for Complex Systems IV,68
237,8934.0,Project Management Decision-Making Assistant - A Deep Learning based framework focused on Real-time project-oriented information,Practitioner,"This project delves into the multifaceted factors influencing project management decisions, focusing on critical elements such as scope changes, resource availability, and stakeholder engagement. By identifying and analyzing quantitative measures like cost, schedule, and quality, we aim to elucidate the underlying variables impacting project outcomes. Employing an automated market research approach, we extract relevant keywords and data sources to enrich our analysis. For each identified factor, a comprehensive cause-and-effect diagram maps the decisions made at various project stages. This visual representation highlights the intricate relationships between different project aspects and their resultant effects. In parallel, we explore advanced deep learning techniques, particularly neural networks, natural language processing (NLP), and machine learning algorithms, to enhance our project data analysis. These methods facilitate the creation of an adaptive threshold decision-making model, which dynamically adjusts to varying project conditions and constraints. The integration of deep learning approaches in project management data analysis offers a robust framework for predicting project outcomes and optimizing decision-making processes. This innovative model aims to support project managers in making informed decisions, ultimately leading to improved project performance and success",2,113 | Cobb Galleria Centre,AI Methods for Complex Systems IV,68
238,7013.0,ANALYSIS OF FANTASY FOOTBALL DEFENSIVE OUTPUT IN RELATION TO OFFENSIVE PERFORMANCE,Practitioner,"This study leverages machine learning to enhance fantasy football strategies by predicting team defensive season points based on offensive player performance. While fantasy football success often relies on specific player selection, traditional approaches overlook the potential influence of offensive metrics on defensive outcomes. To address this gap, this model utilizes data from Yahoo Sports (2017-2023), aggregating points for key offensive positions such as quarterbacks, wide receivers, running backs, tight ends, and kickers, into team-based position metrics for each season. The data was prepared by summing individual player points for each position to account for team-level fluctuations like injuries or lineup changes. The model uses linear regression to evaluate the relationship between offensive output and defensive scoring potential. Results reveal that offensive metrics can effectively predict team defensive points, suggesting that this approach to team composition may improve fantasy football outcomes by strategically drafting defensive teams based on offensive performance. The findings open new opportunities for drafting and trade strategies in fantasy leagues, indicating that focusing solely on high-scoring individual players may be less effective than considering team-level dynamics. Future work could involve expanding the predictive model to include variables such as player matchup analysis and injury risks, aiming to enhance the model’s prediction accuracy and provide fantasy football enthusiasts with even more refined team-building insights. This broader approach could help bridge gaps in current predictive methods and refine strategies for consistent fantasy league success.",3,113 | Cobb Galleria Centre,AI Methods for Complex Systems IV,68
239,5971.0,Social Determinants of Health and Sepsis: A Comparative Study of Sepsis Patients and the General Population,Academician,"Previous literature has highlighted the significant impact of Social Determinants of Health (SDOH) on sepsis outcomes, such as mortality, readmission, and length of stay (LOS). These critical metrics can impact the demands on healthcare delivery systems. However, there is a gap in research comparing SDOH factors between sepsis patients and other patient populations to identify potential differences. This study aims to address this gap by evaluating SDOH factors among sepsis patients compared to a general patient population. Using data collected from a local healthcare institution in Baton Rouge, Louisiana, between May 2022 and June 2024, this analysis examines differences in age, gender, health conditions, and the Social Vulnerability Index (SVI) as a measure of SDOH between these two populations. Statistical comparisons will be performed using the Chi-square test to determine whether significant differences exist in these populations. The findings of this study will provide insights into the unique social and demographic characteristics that may influence sepsis outcomes and contribute to the growing body of evidence on the role of SDOH in healthcare disparities. Understanding determinants of healthcare disparities provides critical information for designing healthcare systems that provide equitable access and care for the target populations while accounting for demands on healthcare resources.",4,113 | Cobb Galleria Centre,AI Methods for Complex Systems IV,68
240,6571.0,Machine Learning and Protein Language Models for Vaccine Candidate Prediction in Aquaculture,Academician,"I dentifying vaccine candidates is crucial for disease prevention, especially in aquaculture, where pathogen outbreaks impact food security and economic stability. Computational vaccine prediction accelerates discovery, cuts laboratory costs, and prioritizes promising targets. This study explores advanced machine and deep learning models using protein language models (PLMs) to identify vaccine candidates from protein sequences. Meta’s ESM-2 model was used to transform protein sequences into high-dimensional embeddings, capturing functional, structural, and evolutionary information. These embeddings, combined with physiological features, fed into multiple classification models: XGBoost, a fully connected neural network (DNN), a fine-tuned ESM-2, and a Graph Attention Network (GAT). A curated dataset of 526 sequences was compiled, including 92 known vaccine candidates from four aquaculture pathogens: Flavobacterium columnare, Flavobacterium covae, Edwardsiella ictaluri, and Aeromonas hydrophila. The fine-tuned ESM-2 model outperformed others with 97.17% accuracy and an F1-score of 0.9143. The GNN model closely followed the DNN, with 95.28% accuracy and an F1-score of 0.8485. These results suggest fine-tuned PLMs like ESM-2 offer strong predictive capabilities. Although the GNN shows potential for capturing complex relationships, performance may improve with larger, more detailed datasets. Future work will expand datasets, explore larger PLMs, and integrate multi-relational graphs in GNNs to enhance model generalization. This approach establishes a scalable, accurate foundation for vaccine candidate prediction using machine learning.",1,113 | Cobb Galleria Centre,Biological and Agricultural Data Analysis,69
241,8822.0,Integrating Topology and Function Information for Protein Association Network Clustering: A Mixed Integer Linear Programming Based Approach,Academician,"The clustering of protein association networks is crucial for understanding protein relationships and cellular functions. This research employs Mixed Integer Linear Programming (MILP) approach to cluster proteins in the Flavodiiron protein FprA1 network, containing 61 proteins and 230 connections. The first stage applies MILP to minimize the maximum diameter inside cluster, emphasizing only the topological characteristics of the network. A refined model follows, designed to maximize the functional similarity within each cluster. This is achieved using a Jaccard similarity matrix based on the molecular function aspect of the Gene Ontology (GO) terms, which emphasizes biological relevance in the clustering process. The number of clusters, a parameter of the MILP model, is learned using graph convolution network (GCN). GCN considers protein-protein interaction network topology and node and edge features. The integration of topological and functional criteria in the second MILP model enables effective clustering that captures both connectivity and biological context. Validation through gene sequence alignment supports the functional relevance of the clusters formed, revealing biologically meaningful groupings. Findings suggest that incorporating functional similarities into clustering improves the biological interpretability of gene groups, demonstrating the potential for refined gene function prediction. Future directions include incorporating additional GO aspects like biological processes and cellular components, as well as advanced metrics for sequence similarity, to further improve clustering precision.",2,113 | Cobb Galleria Centre,Biological and Agricultural Data Analysis,69
242,5260.0,Comparative Analysis of Analytical Methods for Predicting NDVI Using Multi-Dimensional Environmental Data,Academician,"In the past years, remote sensing has been used by scientists to estimate vegetation greenness due to advancements that have reduced accessibility and cost constraints. The Normalized Difference Vegetation Index (NDVI), a popular metric derived from satellite imagery’s reflectance in the red and near-infrared spectral bands, has been widely used in the estimation of the vegetation greenness. However, the accuracy of NDVI can be affected by various environmental factors, including wind speed, wind direction, precipitation, humidity, sea level pressure, and cloud cover. To address these influences, analytical techniques are essential for predicting NDVI based on multi-dimensional environmental data, which enhances forecast precision and provides a deeper understanding of vegetation health. The objective of this study is to compare the accuracy on predicting NDVI using various approaches with multi-dimensional data, including multiple linear regression, support vector regression, random forest, ordinary least squares, and long short-term memory. A dataset spanning six years and two months (January 2016 to February 2022) of NDVI satellite data with high spatial resolution was used. This research provides valuable insights into NDVI estimation, with findings revealing that long short-term memory models incorporating time-lag analysis on NDVI data significantly outperform traditional regression methods. The use of time-lag, particularly a 1-month delay in NDVI data, proved critical in capturing temporal dependencies and long-term patterns in greenness of areas. These insights offer valuable guidance for researchers and practitioners in coastal ecosystem management, emphasizing the role of time-lag in improving decision-making and enabling more effective conservation strategies.",3,113 | Cobb Galleria Centre,Biological and Agricultural Data Analysis,69
243,8974.0,Data-Driven Optimization and Monitoring of Ultrafiltration Water Treatment Systems,Academician,"Ultrafiltration (UF) systems play a critical role in water treatment and reuse by reducing particulate load and impurities before advanced treatment, like reverse osmosis. Operating these systems efficiently is complex due to variable fouling mechanisms driven by variability in incoming water quality caused by seasonality, environmental conditions, and geographical locations. To address these challenges, we introduce a data-driven physics-informed approach for real-time monitoring and optimization to ensure efficient operations of UF systems. First, we conducted a comprehensive data-driven analysis of UF operations—filtration, backwash (BW), and chemical-enhanced backwash (CEB)—to characterize fouling and cleaning dynamics. Multi-sensor historical data collected over six years were processed to reveal varying system behavior patterns, segment operational phases, and extract insights into system performance. In doing so, we leveraged physics-informed feature engineering and incorporated for the first time environmental variables—such as temperature, rainfall, cloud cover, and sunshine duration—to capture seasonal variations in water quality and their impact on UF systems performance. Then, an interactive dashboard was designed and implemented for real-time human-assisted monitoring and decision-making. Moreover, we developed advanced statistical and ML models, including optimized neural networks, to capture the system’s nonlinear behavior and predict net water production with high accuracy across varying operational and environmental conditions. This work presents a step forward towards autonomous and self-correcting decentralized water treatment and reuse systems that are efficient, reliable, and resilient. This is key to tackling one of the most critical challenges of our time—water scarcity, especially in arid regions like the southwestern US.",1,114 | Cobb Galleria Centre,AI and Analytics for Infrastructure Operations and Maintenance,70
244,6089.0,Combining Machine Learning and Discrete-Event Simulation for Optimized Workforce Planning in Airport Screening Facilities,Academician,"In this research, machine learning (ML) clustering techniques are studied with the goal of integrating them to a simulation-optimization framework developed to improve the operation of airport checkpoints. Simulation-optimization is a suitable framework for problems involving data uncertainties that evolve over time, requiring important system decisions to be made prior to observing the entire data stream. This is indeed the case in airport security checkpoints, where the passenger arrival times are difficult to predict and requirements for equipment and human resources must be scheduled in advance. However, simulation-optimization is computationally expensive limiting their ability to be helpful when making real time decisions. This work hypothesis is that ML clustering techniques can be incorporated in the simulation-optimization framework to reduce the computational time. In this research, a computational study is presented that compares the performance of multiple ML clustering techniques and identifies the best techniques for the goals of this research. The results of this research are expected to help improve the operation and safety of airport facilities by developing new decision-making models that will leverage technology and knowledge to enhance the operational effectiveness of airport security checkpoints. Equipping these facilities with better planning tools will allow for better-informed downstream distribution decisions. The resulting models and methods from this research will provide valuable information that will increase the potential of airport facilities to meet the organizational objectives multiple operational scenarios.",2,114 | Cobb Galleria Centre,AI and Analytics for Infrastructure Operations and Maintenance,70
245,8528.0,Automated Detection and Classification of Airport Pavement Cracks,Academician,"This research introduces a machine-learning algorithm to automate the identification and classification of airport pavement cracks. Utilizing drones equipped with high-definition cameras, the study implements a deep learning neural network based on the U-net framework, along with a specialized module for precise quantitative analysis of crack length and width. The algorithm categorizes cracks into distinct types and incorporates a color-coding system for quick identification. To overcome the lack of publicly available datasets on airport pavements, we conducted field investigations at several General Aviation Airports in Tennessee to develop a new dataset, CrackAirport, which is now accessible on IEEE DataPort . The proposed algorithm effectively distinguishes cracks from similar non-crack features at the pixel level, ensuring high accuracy in complex environments. This advancement enhances the Tennessee Department of Transportation's (TDOT) ability to monitor pavement conditions efficiently, reducing inspection times and supporting more timely, cost-effective maintenance planning.",3,114 | Cobb Galleria Centre,AI and Analytics for Infrastructure Operations and Maintenance,70
246,5653.0,High-dimensional (Group) Adversarial Training in Linear Regression,Academician,"Adversarial training can achieve robustness against adversarial perturbations and has been widely used in machine-learning models. This paper delivers a non-asymptotic consistency analysis of the adversarial training procedure under $\ell_\infty$-perturbation in high-dimensional linear regression. It will be shown that, under the restricted eigenvalue condition, the associated convergence rate of prediction error can achieve the minimax rate up to a logarithmic factor in the high-dimensional linear regression on the class of sparse parameters. Additionally, the group adversarial training procedure is analyzed. Compared with classic adversarial training, it will be proved that the group adversarial training procedure enjoys a better prediction error upper bound under certain group-sparsity patterns.",1,114 | Cobb Galleria Centre,Advances in Statistical and Physics-Informed Modeling,71
247,5572.0,Physics-Informed Gaussian Process with applications in ODE/PDE parameter estimation,Academician,"Parameter estimation for nonlinear dynamic system models, represented by ordinary differential equations (ODEs) or partial differential equations (PDEs), using noisy and sparse experimental data is a vital task in many fields. We propose a fast and accurate method, physics-informed Gaussian process, for this task. Our method uses a Gaussian process model over system components, explicitly conditioned on the physics information that gradients of the Gaussian process must satisfy the ODE/PDE system. By doing so, we completely bypass the need for numerical integration and achieve substantial savings in computational time. Our method is also suitable for inference with unobserved system components and provides uncertainty quantification. Our method is distinct from existing approaches as we provide a principled statistical construction under a Bayesian framework, which rigorously incorporates the ODE/PDE system through conditioning. Ref: PIGP for ODE: https://www.pnas.org/doi/abs/10.1073/pnas.2020397118 PIGP for PDE: https://epubs.siam.org/doi/10.1137/22M1514131",2,114 | Cobb Galleria Centre,Advances in Statistical and Physics-Informed Modeling,71
248,8721.0,Efficient Prediction of Thermal History in Wire Arc Additive Manufacturing Through Spatially Varying Functional Models,Academician,"Wire-arc additive manufacturing (WAAM) is a promising technology for fabrication of large-size structural components across various industrial sectors. WAAM offers relatively low equipment and material costs, high production rates, and a large fabrication area. However, high deposition temperatures can introduce significant thermal stresses, leading to substantial distortion and roughness in deposited layers. Precise thermal history prediction is essential for achieving the desired mechanical properties and geometric quality. While finite element methods (FEM) are commonly used, their reliance on solving large PDEs imposes significant computational demands, limiting application to small parts. In this paper, we develop a generalized functional regression model as a surrogate for thermal history prediction during single layer, single track deposition of Stainless Steel 316L in WAAM. Peak energy density, time after deposition, and location are chosen as predictors for the process. The interaction energy density and time after deposition is modeled using tensor-product basis. Thermal history variations between cross-section locations are modeled using Gaussian Process-based varying coefficients. FEM simulations are used to generate thermal history for two geometries at three energy density levels. We studied the effect of track geometry on the FEM results, the accuracy of the proposed surrogate methodology for predictions of the thermal history profile for each geometry, as well as the capability of the results to generalize to heterogeneous geometries.",3,114 | Cobb Galleria Centre,Advances in Statistical and Physics-Informed Modeling,71
249,7043.0,CLUMM: A Contrastive Learning Framework for Unobtrusive Motion Monitoring,Academician,"Motion monitoring provides an avenue for assessing workers, understanding worker behavior, identifying potential safety risks, and giving tailored ergonomic interventions such as further training and workplace/workpiece adjustment. Despite this potential, monitoring and analyzing human motion requires significant effort in sensing and analysis, especially in complex environments with many moving parts. This paper introduces a novel framework utilizing a contrastive self-supervised learning approach capable of learning rich representations from unlabeled camera data. The framework extracts information from critical human joint coordinates relevant to motion tasks, thus learning directly from human-specific data and removing the impact of the complex work environment. The proposed framework uses a contrastive loss to learn representations by maximizing the agreements between different augmented views of the same sample, allowing the identification of intrinsic motion categories in the input data. By fine-tuning the learned model on a downstream motion classification task, an accuracy of up to 91% is obtained, demonstrating the effectiveness of the proposed solution in real-time human motion monitoring and its potential for future generalization across different environments. Further evaluation demonstrates the robustness of the framework to outliers and its ability to detect anomalous motions. The proposed framework contributes to the methodology and application of unobtrusive human motion monitoring. Methodologically, it will contribute to a label-efficient machine-learning approach to recognize motion types from unobtrusive human sensor data. Practically, the ability of the framework to identify various motion categories accurately makes it suitable for task monitoring and optimization by monitoring repetitive motions for further ergonomic assessment.",1,114 | Cobb Galleria Centre,AI for System Analysis,72
250,4813.0,Discovering Puerto Rico Roads Profile Influencing Traffic Fatalities,Academician,"The infrastructure of Puerto Rico (PR) was granted a grade point average of ""D-"" in 2019. Transportation crashes may be influenced, among others, by environmental or infrastructure hazards. Before creating and implementing the first PR Strategic Highway Safety Plan (SHSP), the island reported more than 200,000 traffic crashes annually between 2007-2013. They resulted in a staggering 360 fatalities and 5,200 seriously injured individuals. Road fatalities dropped by 20.72%, and serious injuries decreased by 36.93% throughout the SHSP 2014-2018 implementation. Despite this progress, pedestrian traffic fatalities continue to be a major worry. Is it possible to predict injury severity or crash circumstances for a pedestrian vis-à-vis specific roads that support the Puerto Rico Highway and Transportation Authority (PRHTA) safety improvement efforts? Authors in Indonesia have used the Naive Bayes algorithm to predict crash severity by analyzing contributing factors. This research aims to establish a PR road profile with potential high-severity crashes. Data mining and predictive analytics helped to forecast crash severity and its associated road location. Outputs have shown tree (standing only), live animal, wall, utility pole/light support, fell/jumped from the vehicle, other non-collision, and ditch as the first crash injury-or-damage producing event. With 88.1% likelihood of suffering a fatal injury, especially in I-2 (Aguada and Aguadilla-Int. PR 459); CR-3 (Canóvanas, Carolina, Guayama (Int. Ancha de Blondet St. and Int. PR 54)); and I-52 (Caguas, Cayey, Juana Díaz, Salinas, Santa Isabel). PRHTA could consider the findings for focusing funds’ investments on safety improvement measures.",2,114 | Cobb Galleria Centre,AI for System Analysis,72
251,5936.0,Conversational AI and RAG for Precision Work Package Generation in Mission-Critical Facility Management,Practitioner,"Asset management is critical for efficiently and reliably operating mission-critical facilities. Asset management optimizes how assets and related maintenance tasks affect the production system. Optimal management of assets leads to minimal downtime, system impact, and high reliability in production processes. However, planning for the required maintenance can be complex and arduous. To ensure the safety and accuracy of maintenance work, documentation, approvals, materials, resources, work steps, hazard controls, and responsibilities must be collected and aggregated into documents called work packages. The packages can take months to compile due to the components involved. We propose leveraging large language models and AI-based tools to streamline this process from the initial request to identifying hazards and corresponding controls and permits. This work details a study at a high-security facility utilizing the AI Asset Management Assistants (AMAs) for the utility-specific scope of work generation and corresponding hazards, controls, and permits. This work shows that the AMAs can help planners compile the needed information from the initial request of the work to the generation of customized hazards, department custom controls, and permits. The results show crucial components can be accurately identified and generated using a set of fine-tuned, large language models (LLMs). Using conversational assistants for requesters, the users can provide complete requests, generating accurate scope of work descriptions based on historical data. Through the collaboration of LLMs and specialized domain selective retrieval augmented generation (RAG) methods, organizational domain knowledge can be infused into the assistants, ensuring the outputs match the facility's needs.",3,114 | Cobb Galleria Centre,AI for System Analysis,72
252,6177.0,A Data-Driven Predictive and Optimization Framework for Fire Resource Relocation,Academician,"Fire services play a vital role in protecting communities by providing rapid emergency responses, preventing loss of life, and reducing property damage. Recently, the frequency and intensity of major urban incidents have increased, due to the expansion of urban areas alongside limited resource adaptability. Major incidents such as large-scale fires and technical rescues severely deplete available resources, exposing gaps in coverage and affecting response times. However, the current systems for managing these resources lack automatic adaptability, often resulting in delayed services and inefficient resource distribution within jurisdictions. Furthermore, most studies primarily focus on fire unit and station locations, with little attention paid to the strategic relocation of fire resources. Thus, to fill in this research gap, this paper aims to develop a predict-then-optimize framework that guides effective fire resource relocation strategies. Specifically, we first develop a novel predictive model to estimate fire alarm arrival rates, accounting for the spatiotemporal dependencies from historical fire incidents. Then, we formulate a fire resource relocation optimization model to prescribe the positioning of resources among fire stations, aiming to minimize response times and ensure maximum coverage across all areas. Finally, we present a case study in the city of San Francisco to demonstrate the applicability of the proposed framework. This framework offers flexible application to both Fire Department and Emergency Medical Service (EMS) organizations, enhancing the operational efficiency of emergency services, reducing response times, and increasing the overall coverage of urban areas during major incidents.",1,114 | Cobb Galleria Centre,ML for Risk Management and Cybersecurity,73
253,6395.0,Optimization Techniques in Smart Grids: A Review of Machine Learning and Artificial Intelligence Approaches.,Academician,"In today's rapidly changing world, a Smart Grid (SG) solves the power demand problems by providing two-way power and information flow between consumers and utilities to meet the extra demand for energy. In this application, optimization techniques using Artificial Intelligence (AI) including Machine Learning (ML) and Deep Learning (DL) algorithms play a crucial role in ensuring efficient and reliable energy management. This paper presents a comprehensive review of the state-of-the-art studies on AI algorithms and techniques to optimize electricity power through smart grids, focusing specifically on machine learning and artificial intelligence approaches. This review investigates the potential of these techniques in improving various aspects of smart grids, including power flow analysis, power quality, photovoltaic systems, intelligent transportation, and load forecasting. The review paper highlights the increasing interest and rapid expansion in utilizing ML techniques to address technical challenges in the smart grid domain. The review reveals that machine learning algorithms can handle large volumes of high-dimensional data and identify hidden characteristics of complex systems, leading to improved performance compared to traditional approaches. These techniques have shown promise in supporting the integrating of renewable energy resources, energy storage systems, demand response, and grid and home energy management. Additionally, the paper highlights future perspectives on utilizing advanced computing and communication technologies such as edge computing, ubiquitous Internet of things, and 5G wireless networks, in the smart grid. Overall, this review provides a comprehensive overview of the current state and future potential of machine learning and artificial intelligence techniques in optimizing smart grids.",2,114 | Cobb Galleria Centre,ML for Risk Management and Cybersecurity,73
254,5606.0,State-Space Representation of a Blur-Generated Non-Separable Space-Time Process,Academician,"In the stationary state, a blur-generated space-time process is a Gaussian process with space-time non-separable covariance function. Such a process can be obtained through a series of convolution, in both time and space, of white-in-time and colored-in-space noise. Because convolution in space and time can be computationally costly, this talk presents some early results on the state-space representation of the blur-generated space-time process. In particular, the state-space model generates a process that has the same space-time covariance structure as the targeted blur-generated process, and can be computationally faster leveraging the filtering recursions of state-space models.",1,106 | Cobb Galleria Centre,AI-Driven Innovation in Enterprise and Engineering Systems,74
255,4973.0,Leveraging Generative AI Text-to-Action in Enhancing Enterprise Productivity,Practitioner,"This presentation explores the application of generative AI models in transforming text inputs into actionable tasks to enhance enterprise productivity. By automating workflows and decision-making processes, generative AI can bridge the gap between natural language inputs and complex operational tasks. We analyze key use cases within enterprise environments, examining how text-to-action systems can improve efficiency, reduce errors, and optimize resource allocation. Our research demonstrates significant gains in productivity across various sectors through AI-driven task automation, offering insights into future developments and challenges in deploying these technologies at scale.",2,106 | Cobb Galleria Centre,AI-Driven Innovation in Enterprise and Engineering Systems,74
256,6355.0,Public Blockchains Since 2015: Analytical and Visual Approach to Display Development,Academician,"This paper discusses the factors contributing to the internet paradigm shift from Web 2.0 to Web 3.0. Blockchains are posed to replace centralized servers as the databases of the Web 3.0 environment. Public blockchains such as Bitcoin and Ethereum have had active networks for over nine years. These blockchain networks are analyzed, along with the overall public blockchain environment. Technical functioning of Bitcoin and Ethereum are explained. This project will display developments in these two public blockchains from 2015 to 2022. Data is pulled from network associated “Block Explorers” and visualized comparatively with a common time identifier in Tableau. Visualized data will be analyzed in terms of financial, blockchain, network, and mining metrics. Data will also be modeled in MATLAB to conclude significant features of each public blockchain that contribute the growth in coin price. After concluding the development of the public blockchains since 2015, the current environment of public blockchains will be discussed with a focus on future development.",3,106 | Cobb Galleria Centre,AI-Driven Innovation in Enterprise and Engineering Systems,74
257,8610.0,Electric Vehicle Battery Manufacturing Process Management with GS1 Standards and a Digital Product Passport Framework,Academician,"With the exponential growth of the electric vehicle (EV) industry, there is a critical need for effective systems to manage and trace EV battery lifecycles. GS1 standards, globally recognized for product identification and traceability, offer a strong foundation for developing a digital product passport specifically tailored to EV batteries. In this talk, we introduce a comprehensive framework that leverages GS1 standards to support an EV battery digital passport, enabling seamless tracking in its manufacturing processes. Our approach involves collaborative integration of GS1 identifiers, allowing batteries to carry unique, standardized data points that facilitate data exchange across the supply chain in its entire manufacturing process. By centralizing battery data and promoting open access for stakeholders, this framework supports regulatory compliance, enhances supply chain transparency, and enables efficient battery reuse and recycling. Through this digital passport, stakeholders can access real-time battery data, streamlining processes and enabling better lifecycle management aligned with environmental sustainability goals. Our framework also addresses the interoperability challenges associated with battery management, proposing scalable solutions to foster industry-wide adoption of GS1 standards. Ultimately, this initiative lays the groundwork for a sustainable, circular economy within the EV industry by providing a robust, data-driven approach to lifecycle management and resource optimization.",4,106 | Cobb Galleria Centre,AI-Driven Innovation in Enterprise and Engineering Systems,74
258,6123.0,Quantum Convolutional Neural Networks for Cardiac Arrhythmia Detection from ECG Signals,Academician,"Electrocardiography (ECG) is a primary diagnostic tool for identifying cardiac arrhythmias, a critical indicator of heart disease. While ECG signals contain vital information about the patient’s health conditions, their high-resolution waveform nature poses significant challenges in effective feature extraction. Recent advancements in quantum computing provide a transformative opportunity to accelerate research in data-driven predictive modeling for healthcare. This paper investigates a variety of Quantum Convolutional Neural Network (QCNN) architectures for ECG signal classification. By leveraging optimized convolutional structures, adaptable pooling layers, and advanced encoding techniques, our QCNN model is designed to preserve essential ECG characteristics while addressing challenges in dimensionality reduction. Experimental results demonstrate that our approach holds significant potential for enhanced arrhythmia classification. This innovative QCNN model has the potential to advance ECG-based diagnostics for heart disease, offering valuable support for clinical decision-making in cardiac care.",1,106 | Cobb Galleria Centre,DAIS Best Track Paper Competition,75
259,5553.0,Machine Learning-AI Approach for Early Detection of Latent Autoimmune Diabetes in Adults (LADA) in a Primary Care Setting,Practitioner,"Latent Autoimmune Diabetes in Adults (LADA) is a subtype of Type 1 diabetes often mistaken for Type 2 due to its gradual onset and overlapping features, leading to misdiagnoses and adverse patient outcomes. Recognizing the critical need for precise and early identification, this study employs a machine learning-AI approach to develop a model aimed at detecting LADA in primary care settings. Utilizing data from 144 patients at FLCH, our model seeks to differentiate LADA patients more effectively. Key biomarkers such as C-peptide, Glutamic Decarboxylase, IA2 Antibody, BMI, Glucose, and A1C levels were incorporated as predictive features. Implementing Stochastic Gradient Descent (SGD) and Balanced Random Forest (B-RF) algorithms, the model achieved detection accuracies of 93% and 91%, respectively, surpassing other tested methodologies. The most influential features contributing to this accuracy were identified as C-Peptide and the latest A1C levels. Validation on hypothetical patient data demonstrated the model's robustness, with both SGD and B-RF algorithms successfully classifying all samples. These results highlight the significant potential of machine learning-AI in supporting early LADA diagnosis and facilitating personalized treatment, thereby mitigating the risks associated with misdiagnosis. Nevertheless, challenges such as ensuring data privacy, integrating into clinical workflows, and addressing algorithmic bias persist. Future research and collaborative efforts will focus on refining the model, enhancing interpretability, and promoting its adoption in clinical practice, ultimately improving care quality for individuals with or at risk of LADA.",2,106 | Cobb Galleria Centre,DAIS Best Track Paper Competition,75
260,6880.0,Multi-Scale Transformer for Long-Term Time Series Forecasting,Academician,"In order to make well-informed decisions, long-term time series forecasting is crucial for a number of applications in finance, energy, and environmental science. While traditional transformer models have demonstrated a strong ability to capture temporal dependencies, they frequently struggle to handle the lengthy sequences and multi-scale temporal patterns present in time series data. In this paper, we propose a novel architecture to improve forecasting performance over long horizons: the Multi-Scale Transformer. Our model effectively captures both short-term fluctuations and long-term trends by extending the transformer by integrating multi-scale processing. In addition, we suggest a feature interaction module that allows the model to learn intricate interdependencies in the data by explicitly modeling the relationships between individual features. We compare our models’ performance to the most advanced techniques using a variety of benchmark datasets. According to experimental results, our method continuously outperforms current models and achieves notable gains in forecasting accuracy. A promising path forward for long-term time series forecasting is the transformer framework's incorporation of multi-scale analysis.",3,106 | Cobb Galleria Centre,DAIS Best Track Paper Competition,75
261,6669.0,Hyperdimensional Regression: Potentials  and insights into Hadamard Matrix - Based Encoding For Predictive Analysis,,"The use of Hadamard matrices for hypervector generation within hyperdimensional computing (HDC) offers a transformative and efficient approach to data encoding, significantly reducing the dimensionality of hypervectors. Unlike traditional encoding methods that often require hypervectors with dimensions exceeding 10,000, Hadamard-based encoding creates a more compact representation while maintaining key properties for robust data processing. The inherent binary orthogonality of Hadamard matrices ensures effective and distinct data representation, supporting high-performance classification and regression tasks within HDC frameworks. Our analysis demonstrates that hypervectors generated through Hadamard matrices achieve competitive performance, particularly on datasets with normal or near-normal distributions, when compared to advanced machine learning models. The compact nature of these hypervectors enhances computational efficiency without sacrificing accuracy. However, the method’s performance diminishes on datasets that deviate substantially from normality. This work underscores the value of Hadamard-based encoding in HDC, combining dimensionality reduction with effective data processing capabilities.",4,106 | Cobb Galleria Centre,DAIS Best Track Paper Competition,75
262,6076.0,Knowledge-Informed Learning for Extracapsular Extension Prediction in Head and Neck Cancer,Academician,"Effective diagnosis and treatment planning for head and neck squamous cell carcinoma (HNSCC) heavily depend on identifying extracapsular extension (ECE) within lymph nodes, a critical factor in predicting patient outcomes. Current ECE detection methods often depend on visual identification and pathology confirmation, which are time-consuming, error-prone, and lack explainability. Building on the Gradient Mapping Guided Explainable Network (GMGENet), this study introduces a knowledge-informed machine learning model that optimizes ECE identification by selectively enforcing domain-specific knowledge constraints in the model learning process. The GMGENet framework uses a two-stage extractor-classifier model based on 3D DenseNet, guided by gradient-weighted class activation mapping (Grad-CAM) to highlight volumes of interest (VOIs) critical for ECE detection in computed tomography (CT) images. A knowledge-regularized loss function is introduced and enhances the learning by constraining mask contributions, guiding the model to assign higher relevance to regions associated with ECE while minimizing the influence of non-relevance areas. The knowledge-informed constraints also reduced false positives, particularly in regions unrelated to lymph node involvement. Model performance was validated through five-fold cross-validation. By introducing a knowledge-regularized objective, the model achieves a more clinically relevant focus, paving the way for more accurate and interpretable ECE detection in cancer staging and treatment planning.",1,106 | Cobb Galleria Centre,Advanced AI Solutions in Health and Diagnostics,76
263,6078.0,Sleep Stage Biomarkers in Polysomnography Data for ADHD Diagnosis Using Machine Learning,Academician,"Attention-Deficit/Hyperactivity Disorder (ADHD) significantly impacts children and adults, presenting challenges in neurodevelopmental diagnostics. Polysomnography (PSG) data, encompassing EEG, EOG, EMG, and ECG signals, offers potential for identifying biomarkers that improve diagnostic accuracy. This study explores the use of graph metrics, specifically the average shortest path length, to analyze information transfer within brain networks across different sleep stages in ADHD and non-ADHD individuals. We analyzed PSG recordings from 48 participants, including 25 ADHD and 23 non-ADHD patients, spanning five sleep stages: Wake, Stage 1, Stage 2, Stage 3-4, and REM. Graph theory was employed to model physiological signal interactions, with features extracted and classified using a Random Forest model. Nested cross-validation ensured robust model evaluation, while feature importance analysis identified the most critical sleep stages. The Random Forest model achieved an average accuracy of 72%, with a precision of 71%, recall of 85%, and F1-score of 76%. Sleep Stage 1 and Sleep Stage 3-4 were identified as the most influential for distinguishing ADHD from non-ADHD individuals, validated through feature importance and permutation analysis. This study demonstrates the potential of using sleep stage-based graph metrics as biomarkers for ADHD diagnosis. The findings underscore the disrupted transitions between light and deep sleep in ADHD patients and highlight the role of explainable machine learning models in advancing neurodevelopmental disorder diagnostics.",2,106 | Cobb Galleria Centre,Advanced AI Solutions in Health and Diagnostics,76
264,7051.0,Decoding Significant Genomic Loci for Traits from Image Encoded DNA,Academician,"Understanding the complex relationship between genotype and phenotype is a critical goal in biological research, with significant implications for fields such as medicine, agriculture, and biosciences. The ability to predict phenotypes from genetic information can advance crop improvement, precision medicine, and disease prevention. However, genotype-to-phenotype prediction presents substantial challenges due to the high dimensionality of genomic data, complex gene-environment interactions, and the difficulty of pinpointing key genomic loci that drive phenotypic variation. Traditional approaches often struggle to identify the precise genomic regions that are crucial for specific traits. To address these challenges, this study explores the use of image-based encoding of genomic data, which transforms linear DNA sequences into spatial representations amenable to deep learning models. This approach facilitates pattern recognition and spatial feature extraction, capabilities that are unparalleled by traditional methods. Additionally, we employ Gradient-weighted Class Activation Mapping (Grad-CAM) to interpret model predictions, providing a mechanism to trace significant features back to specific DNA sequences. By leveraging Grad-CAM, we aim to not only improve predictive accuracy but also enhance biological interpretability by mapping key predictive signals to their genomic origins. This study demonstrates an innovative pathway toward more accurate and interpretable genotype-to-phenotype prediction models, advancing our understanding of the genetic basis for complex traits.",3,106 | Cobb Galleria Centre,Advanced AI Solutions in Health and Diagnostics,76
265,6180.0,A Decision Support System for Conference Session Selection using Natural Language Processing and Machine Learning,Academician,"Conference attendees are often faced with selecting from hundreds of presentations and sessions in pursuit of new findings and methods relevant to their area of interest, an overwhelming amount of information from which to clearly make a decision. To address this, we aim to develop a tool leveraging machine learning (ML) and natural language processing (NLP) techniques such as topic modeling (BERTopic) and semantic matching. By creating and matching embeddings of conference presentation abstracts and titles, the tool will provide improved query matching compared to keyword searching. Attendees can select from a variety of input types, including keyword selection and natural language query, and receive a collection of presentations with abstracts semantically matching the input. Notably, users can input an abstract of a paper most relevant to their area of interest and receive a collection of relevant conference presentations. We hope that this tool will enable academics and practitioners to rapidly identify relevant presentations, fostering knowledge exchange and professional network exploration.",1,113 | Cobb Galleria Centre,Generative Model and Its Application,77
266,8589.0,Generative Models and Applications,Academician,"Generative AI platforms are revolutionizing the way we do many things. At its core, a generative model learns from existing examples and generates new, similar things. A generative AI platform is a computer program or algorithm that can create content, whether the content is images, text, or other data types. This is an introductory talk about some widely used generative models, including generative adversarial networks (GANs), variational autoencoders (VAEs), large language models (LLM), recurrent neural networks (RNNs), Bayesian networks, and flow-based generative models. Applications in various fields will be discussed.",2,113 | Cobb Galleria Centre,Generative Model and Its Application,77
267,9071.0,Surface Image Generation Using Conditional GAN for Vision-Based Quality Modeling in Turning Process,Academician,"The surface roughness of machined parts is an important quality indicator because it affects surface friction, lubrication, wear, coating, and appearance. Accurate surface roughness characterization is critical for achieving the desired surface finish, and vision-based models have shown promise by surpassing conventional methods in speed and cost. However, insufficient high-quality training data limits their performance in accuracy and the ability to generalize. To address this limitation, we propose a conditional generative adversarial network (GAN) designed to generate synthetic surface images for the turning process. The conditional GAN takes feed rate and depth of cut as input process parameters to produce turned surface images. Training data were collected from 1018 steel parts machined with a HAAS ST-15 Lathe. Images were captured under varying camera angles and lighting conditions to enhance model robustness. Preliminary results show that the conditional GAN can generate detailed surface patterns given process parameters as input. These synthetic images provide a scalable and cost-effective solution to produce high-quality datasets for vision-based surface quality model training.",3,113 | Cobb Galleria Centre,Generative Model and Its Application,77
268,6570.0,Identifying Trends In How Readers Evaluate Generative AI Writing: An Engineering Case Study,Academician,"Generative Artificial Intelligence has seen widespread growth in its use over the last few years, particularly in writing. This can be a very helpful tool when used judiciously, but can undermine the human author if not used carefully. In particular, errors and other “hallucinations” are a risk, as these models are generating text based on input, not necessarily thoroughly researching the topic at hand. This highlights the need for readers to be the fact checkers. While this is generally the case with any informative writing, the issue is that readers may not know that Generative AI has been used, meaning that readers may not know they need to evaluate the article or writing differently. Identifying trends in how readers process these writings could help close that gap, giving readers tools to see through any errors, “hallucinations”, or other incorrect information that may slip into writings that employ GenAI. For this experiment, 17 participants were recruited, all of whom were engineering students. Of these 17, the first two participants had incomplete data sets due to technical difficulties, meaning 15 participants had usable data. These participants were tasked with identifying errors in four papers (two of which were written by Generative AI, two by a human author). Eye tracking was also employed to supplement this data with information such as fixations on errors.",4,113 | Cobb Galleria Centre,Generative Model and Its Application,77
269,6047.0,A Holistic Weakly Supervised Approach for Liver Tumor Segmentation with Clinical Knowledge-Informed Label Smoothing,Academician,"Liver cancer is a leading cause of mortality worldwide, and accurate CT-based tumor segmentation is essential for diagnosis and treatment. Manual delineation is time-intensive, prone to variability, and highlights the need for reliable automation. While deep learning has shown promise for automated liver segmentation, precise liver tumor segmentation remains challenging due to the heterogeneous nature of tumors, imprecise tumor margins, and limited labeled data. We present a novel holistic weakly supervised framework that integrates clinical knowledge to address these challenges with (1) A knowledge-informed label smoothing technique that leverages clinical data to generate smooth labels, which regularizes model training reducing the risk of overfitting and enhancing model performance; (2) A global and local-view segmentation framework, breaking down the task into two simpler sub-tasks, allowing optimized preprocessing and training for each; and (3) Pre- and post-processing pipelines customized to the challenges of each subtask, which enhances tumor visibility and refines tumor boundaries. We evaluated the proposed method on the HCC-TACE-Seg dataset and showed that these three key components complementarily contribute to the improved performance. Lastly, we prototyped a tool for automated liver tumor segmentation and diagnosis summary generation called MedAssistLiver . The app and code are published at https://github.com/lingchm/medassist-liver-cancer.",1,113 | Cobb Galleria Centre,AI Methods for Healthcare I,78
270,6245.0,Assessment of Cognitive Impairment among patients diagnosed with Parkinson’s Disease Using Machine Learning,Academician,"With nowadays increased life expectancy and therefore the prevalence of neurodegenerative diseases among the elderly population, it has become prevalent to look for prevention and early detection of such diseases as Parkinson Disease (PD). Not only affecting the well-known motor aspects but also cognitive functions in late stages of the disease´s development called Parkinson Disease Dementia (PDD). PDD significantly affects the quality of life and healthcare costs of a great part of the United States population, worsening the burden as the disease advances. This study aims to identify patterns and predictors of cognitive decline in PD patients, applying classification algorithms such as Random Forest or Cat Boost to demographic data, family medical history, and cognitive assessments. Data from 966 evaluations of 314 patients were analyzed and results indicate that used models achieved the highest accuracy, facilitating early detection of cognitive decline in PD patients with specific variables.",2,113 | Cobb Galleria Centre,AI Methods for Healthcare I,78
271,6101.0,Segmentation and Classification of Pap Smear Images for Cervical Cancer Detection Using Deep Learning,Academician,"Cervical cancer is a leading cause of cancer-related deaths among women globally. Early detection through pap smear tests is crucial in the reduction of mortality rates. However, manual examination of these images is time-consuming, subjective, and prone to error. Recent advancements in deep learning have introduced automated methods to enhance diagnostic accuracy. Nonetheless, challenges remain, particularly in effective cell segmentation, which is important for accurate classification. While many studies focus on classification using Convolutional Neural Networks (CNNs), there is a notable gap in robust segmentation techniques for cervical cells. This research addresses this gap by developing a deep learning framework that integrates segmentation and classification of pap smear images to improve detection accuracy. In the first stage, various CNN models are employed for segmentation, with automatic hyperparameter tuning to optimize performance. The segmented cells are then analyzed using clustering techniques, which provide insights into their distribution and characteristics. In the second stage, both segmented and non-segmented images are classified to compare the impact of segmentation on accuracy. Explainable AI techniques, such as Class Activation Maps (CAM), are applied to provide transparency in the model’s decision-making process. This approach aims to enhance the accuracy, efficiency, and interpretability of automated cervical cancer detection, which offers a valuable tool for clinical applications.",3,113 | Cobb Galleria Centre,AI Methods for Healthcare I,78
272,8508.0,Enhancing Early Dementia Detection with AI and Character-Level Speech Biomarkers,Academician,"The rising prevalence of neurocognitive disorders, such as Alzheimer’s disease (AD), poses a significant global health challenge. Traditional diagnostic methods, including clinical interviews and paper-based tests like the Mini-Mental State Examination (MMSE), Mini-Cog Test, and Montreal Cognitive Assessment (MoCA), are limited by subjectivity, memory biases, and interviewer variability. To address these limitations, there is an increasing need for noninvasive, real-time, and data-driven diagnostic tools. This study introduces an interpretable artificial intelligence (AI) approach to analyze speech transcripts for the early detection of dementia signs. Initially, each unique character in the speech transcripts is encoded as a distinct number. Recurrence Quantification Analysis (RQA) is then applied to generate recurrence plots, capturing dynamic linguistic patterns associated with cognitive decline. From these plots, we extract salient features, termed linguistic biomarkers , using deep metric learning with Siamese networks, which effectively represent essential linguistic characteristics. These biomarkers are transformed into numerical embeddings and processed using an XGBoost classifier, recognized for its robustness, to differentiate between dementia and healthy subjects. Utilizing stratified K-Fold cross-validation, our model achieves a mean ROC AUC of 88.5% ± 2.9% , demonstrating strong and consistent performance across diverse data subsets. Additionally, SHAP (SHapley Additive exPlanations) analysis identifies the most important linguistic features influencing the model's predictions, providing clear insights into the language patterns linked to cognitive impairment. This research not only enhances the accuracy of dementia diagnosis but also offers valuable understanding of the linguistic indicators of cognitive disorders, enabling the development of more effective and interpretable diagnostic tools in clinical settings.",4,113 | Cobb Galleria Centre,AI Methods for Healthcare I,78
273,6478.0,Predicting Cognitive Decline in Healthy Adults using Structural MRI,Academician,"Alzheimer’s disease (AD) is a progressive neurological disorder and the leading cause of dementia, affecting millions globally. As populations continue to age, AD prevalence is expected to rise significantly, placing substantial burdens on healthcare systems. Early detection of cognitive decline is essential, as interventions during preclinical stages may slow disease progression and improve quality of life. In this study, we analyzed structural MRI (sMRI) data from 194 participants aged 70 and above from UK Biobank, focusing on specific brain features to identify predictors of cognitive decline. Using machine learning techniques, we assessed a diverse set of sMRI features, including regional grey matter and subcortical volumes, gray-white matter intensity ratios, multiple parcellations of the white surface, and a parcellation of the pial surface, capturing distinct aspects of brain structure. Each feature group included multiple measurements representing different dimensions of brain morphology. We evaluated the predictive performance of each feature group to determine which provided the most accurate prediction of cognitive decline. Cognitive performance was assessed through Fluid Intelligence (FI) and Reaction Time (RT) tests, allowing classification of participants as “Cognitive Decliners” vs. “Non-Decliners”. Our findings underscore the predictive value of these sMRI features in identifying early indicators of cognitive decline, advancing the development of MRI-based biomarkers to support timely interventions for Alzheimer’s and related dementias.",1,113 | Cobb Galleria Centre,AI Methods for Healthcare II,79
274,5795.0,A novel self-attention prediction model for brain tumor recurrence prediction,Academician,"Traditional methods for predicting postoperative brain tumor recurrence typically involve a combination of manual feature extraction and deep learning techniques. However, conventional deep learning approaches require manual screening and cropping of specific regions, making them unsuitable for handling dynamic-sized magnetic resonance imaging (MRI) data. To overcome this limitation, we introduce a convolutional neural network model incorporating a self-attention mechanism. This model eliminates the need for traditional preprocessing steps like manual cropping and lesion-specific training, instead integrating a self-attention mechanism that utilizes both channel and spatial attention. The model begins with a channel attention module that redistributes weights across image layers, creating a streamlined Multilayer channel configuration. Subsequently, an intricate spatial attention structure analyzes these layers in detail to identify crucial spatial features associated with tumor recurrence. This holistic image analysis approach retains essential diagnostic information from the entire image. Furthermore, the model's effectiveness was demonstrated on a meticulously annotated dataset for brain tumor recurrence, consistently outperforming traditional methods in predictive accuracy. This innovation enhances the predictive process in clinical oncology imaging, providing a robust tool that improves the accuracy of postoperative care.",2,113 | Cobb Galleria Centre,AI Methods for Healthcare II,79
275,8552.0,Multi-layer Privacy Protection for Federated Learning and Encrypted Healthcare Analytics,Academician,"The Internet of Medical Things (IoMT) enables continuous collection and transmission of healthcare data through interconnected networks of patient wearables and other devices. This capability transforms traditional healthcare systems into data-rich environments. However, this data-rich environment also brings privacy concerns because of the widespread distribution of health data across multiple healthcare systems. Such concerns, including data breaches and privacy violations, become paramount when aggregating data into a centralized location for analytical purposes. To tackle these challenges, this paper presents a new privacy-preserving framework with three-layer protection for distributed analytics on encrypted data from different units across multiple independent healthcare systems. First, we propose encrypting the data for analytical computation to mitigate the likelihood of data breaches. Second, a distributed framework with federated learning is designed to circumvent the need for centralized data storage and enable iterative learning and model updates when new data become available. Finally, the proposed framework leverages an ensemble learning approach to enhance both computational efficiency and model performance. Experimental results from real-world intensive care unit (ICU) case studies show that the proposed framework effectively protects data privacy while maintaining the performance of analytical models. These findings highlight the potential of a federated privacy-preserving framework to avoid centralized data storage and support collaborative analytics in data-rich healthcare environments.",3,113 | Cobb Galleria Centre,AI Methods for Healthcare II,79
276,8940.0,Developing a Predictive Model for Medical School Applicant Selection,Practitioner,"A college of osteopathic medicine receives approximately 10,000 applications each year for only 300 spots. With an aging population and a growing physician shortage, selecting applicants who will matriculate and succeed as medical school graduates has become an increasingly important focus for universities offering these programs. A predictive model that the admissions team can easily use to identify top applicants was developed using academic data from current students. A dataset containing 30,000 applicant records and current student performance from two campuses was analyzed. The data included various academic, extracurricular, and demographic variables. Many advanced models were tested against the dataset provided, including gradient boosting, random forest, and multiclass regression with varying levels of success. The final method chosen utilized linear optimization to calculate feature importance. Linear optimization was selected considering model explainability and the incorporation of domain knowledge. The model’s objective function sought to maximize the difference between classes while minimizing variance (at half importance), incorporating both L1 and L2 regularization. The model was integrated with a user-friendly dashboard that allows the admissions team to easily upload new applicant data and see a ranked list of candidates to invite for an interview. This research reviews the process used to build a model with complex applicant data and the results of testing the model for the applicant pool in the current academic year.",1,113 | Cobb Galleria Centre,AI Methods for Healthcare III,80
277,5135.0,Embracing Tradition and Technology in Shaping Modern Professional Interviews,Practitioner,"Since 2020, many institutions have been forced to change their interviewing process. This transition includes shifting from conducting in-person interviews to virtual, asynchronous, or a hybrid approach combining different interview modalities. The shift from in-person interviews presents numerous benefits such as efficiency, cost savings, inclusivity, and convenience for both the candidates and recruiters. On the other hand, in-person interviews allow for personal connection, face-to-face interaction, and for the candidate to experience the work environment. Although both strategies have their respective advantages, the implications of candidate selection and post-interview success from the type of interview are not widely known. This study explored the impact of interview modality. This research introduces a case study analyzing a medical school’s historical interview data to understand recent matriculation concerns. A comprehensive literature review and statistical analysis were performed to identify the relationship between interview modality and student success. The results provide data-driven insights on the medical school’s current interviewing methods, recommended interviewing methods, and the success of students from different interview modalities. Success was determined by variables such as interview-to-offer yield rates, offer-to-accept yield rates, offer-to-deposit yield rates, matriculation rates, cancellation rates, dropout rates, duration to complete degree, and GPA. By focusing on the medical school’s doctoral interview process, this case study provides in-depth research of different interview procedures and the resulting outcome. The findings will outline the most effective interview modalities to foster success for both recruiters and candidates in industry",2,113 | Cobb Galleria Centre,AI Methods for Healthcare III,80
278,5888.0,Anger and Emotional States Detection from PPG Signals Using Machine Learning Approaches,Academician,"Smartwatches, which capture photoplethysmographic signals (PPGs), are widely used to monitor human health. PPGs have been employed in a variety of human health monitoring applications, which include heart rate, noninvasive blood glucose measurement, and emotion detection. In contrast to existing literature that typically employs heart rate-based features, this study utlizes time domain characteristics derived from PPG pulse patterns to passively detect anger and more general emotional states, which include valence and arousal. To ensure reliable and independent testing across participants, leave-one-out cross-validation is used to assess model performance. In discrete label classification, support vector machine (SVM) achieved an accuracy of 0.75 ± 0.26 in detecting anger states. Furthermore, SVM and Naïve Bayes achieved a detection accuracy of 0.67 ± 0.14 and 0.56 ± 0.14 for arousal and valence states, respectively. These findings suggest that anger and other emotional states can be detected from PPGs using simple classification models. Given the accessibility of PPGs from smartwatches, this research offers a cost-effective and unobtrusive method for emotion monitoring, especially in situations where facial recognition is unsuitable or unavailable.",3,113 | Cobb Galleria Centre,AI Methods for Healthcare III,80
279,7054.0,The Impact of COVID-19 on Cyber Attack Trends,Practitioner,"This study investigates the influence of the COVID-19 pandemic on cyberattack trends between 2018 and 2023, with a particular emphasis on the healthcare, government, and education sectors. These businesses experienced increased cyber threats during the pandemic as a result of fast digital transformation, a shift to remote employment, and reliance on personal devices without corporate-grade protection. The study uses a curated dataset from sources such as Hackmageddon and Kaggle to evaluate notable cyber occurrences, assessing shifts in both attack frequency and types, with a focus on ransomware, phishing, and DDoS attacks. The study aims to answer two main questions: did the frequency of cyberattacks rise during the pandemic, and what kinds of attacks become more common? The results show that phishing and ransomware attacks, which took advantage of pandemic-related vulnerabilities and the rise in digital communication, were very successful during this time. The study's distinctive comparison methodology covers pre-, during-, and post-pandemic periods and provides data-supported insights that highlight cybersecurity weaknesses caused by crises in important industries. Aware of the persistent cybersecurity issues brought up by the move to remote work, this study highlights the necessity of scalable, flexible security frameworks that can endure comparable future changes. It not only identifies vulnerabilities unique to a given industry, but it also offers businesses practical suggestions for enhancing their resistance to cyberthreats associated to crises. By advancing our knowledge of the dynamics of cyberattacks in the pandemic era, this research helps organizations plan for and predict dangers during international emergencies.",1,114 | Cobb Galleria Centre,Trustworthy and Adaptive AI for Critical Systems I,81
280,8820.0,Sustainable Load Allocation Using Differentiable Optimization Layers in Neural Networks,Academician,"The increasing urge to reduce fossil fuel dependency and increase renewable energy integration in power grids has triggered the need for efficient load allocation strategies. Looking forward to prioritizing the usage of renewable energy, this research proposes a novel integrated predictive optimization framework that integrates optimal energy allocation with demand forecasting. While numerous machine learning models have been developed to forecast energy demand and optimization models to allocate resources at minimal cost or loss, these approaches operated independently. In contrast, this research leverages a historical dataset containing energy generation data from renewable and fossil fuel sources, along with actual load demand, to develop an integrated neural network model embedded with an optimization layer within its loss function. The neural network predicts future energy demand based on various input features. A differentiable optimization layer implemented using CVXPY, assigns energy sources to satisfy this predicted demand to minimize fossil fuel usage, constrained to real-world load and power generation limitations. The framework dynamically modifies energy allocations during training by integrating the optimization model into the prediction process, promoting sustainable energy-use decisions. The model significantly reduces reliance on fossil fuels and, when practicable, shifts the bulk of demand to renewable sources, demonstrating the feasibility of this method. This integrated model enhances current methods by integrating forecasting and allocation into a single procedure, making it a useful instrument for the sustainable energy management of power grids. The suggested approach promises to facilitate data-driven, real-time decision-making that supports ecologically friendly energy choices.",2,114 | Cobb Galleria Centre,Trustworthy and Adaptive AI for Critical Systems I,81
281,5963.0,Detecting Concept Drift in Object Detection Models:  A Collaborative AI-Human Approach for Defense Applications,Practitioner,"Concept drift, a persistent challenge in machine learning, can lead to the deterioration of model performance over time. This is a pressing issue for the U.S. armed forces as they strive to integrate machine learning and other AI-based systems into their defense operations. In response, the U.S. Army Research Laboratory is keen to identify when concept drift affects the object detection models they employ. This study introduces a novel drift detection method to establish a metric that alerts users to concept drift and supports retraining as needed. The method leverages AI-human interaction to bolster the model’s trustworthiness and facilitate retraining when new data becomes available. Results demonstrate the method’s capacity to detect drift through calculated Drift Avoidance Values for target objects, such as tanks and humans. The system gathers diverse data—images, AI-generated confidence values, and human corrections—to inform these metrics. A “You Only Look Once” (YOLO) object detection model, initially trained on thousands of images, was tested on a set of 200 images, with retraining based on user corrections. This process produced a 3% accuracy improvement with only 200 additional images, a significant gain relative to typical retraining requirements. The study highlights the value of human input in reinforcing model reliability and demonstrates the method’s effectiveness in adapting to performance degradation. This approach supports the sustainability of object detection systems and decision-making in defense applications.",1,114 | Cobb Galleria Centre,Trustworthy and Adaptive AI for Critical Systems II,82
282,5486.0,Defenses for Visual Recognition in Connected Autonomous Vehicles: A Review,Academician,"Connected Autonomous Vehicles (CAVs) promise enhanced safety, efficiency, and accessibility in transportation by operating autonomously while communicating with road infrastructure and other vehicles. The development of CAV technology involves advanced vision-based classification systems, utilizing deep neural networks (DNNs) to perceive and interpret sensor data for navigation. However, the increased reliance on automation technology makes CAVs susceptible to adversarial attacks—malicious inputs designed to disrupt model decision-making, potentially leading to safety and reliability hazards. Traditional and modern classification algorithms have shown susceptibility to various forms of adversarial perturbations, which can lead to potential erroneous decisions in real-world scenarios. This review explores the transformative potential and challenges posed by CAVs and associated visual recognition technologies, particularly DNNs and compressive sensing (CS) techniques that can be used to defend against adversarial attacks on CAVs. This review examines various types of adversarial attacks, including the Fast Gradient Sign Method (FGSM), Carlini & Wagner (CW), and sticker attacks, which exploit CAV visual recognition system vulnerabilities. These attacks pose significant risks to CAV decision-making, emphasizing the critical need for robust defense mechanisms. Several defensive approaches are analyzed, including compressive sensing, incremental training, and classifier-aware training, which each offering unique advantages in strengthening CAV systems against adversarial threats. This review highlights the need for adaptive defense strategies specifically designed for CAV visual recognition systems, proposing the integration of these techniques as a critical step toward securing the future of autonomous transportation.",2,114 | Cobb Galleria Centre,Trustworthy and Adaptive AI for Critical Systems II,82
283,6273.0,Leveraging Inequality Measures for Equitable Food Bank Operations: A Data – Driven Approach,Academician,"Addressing food insecurity remains a significant global challenge, further intensified by natural phenomena such as hurricanes, which disrupt food supply chains, damage infrastructure, and limit access to essential resources. Recent hurricanes, including Helene and Milton, have exemplified how these events exacerbate vulnerabilities, placing immense pressure on existing food distribution systems. In the United States, food banks play an essential role in mitigating hunger and ensuring equitable access to nutritious food. This research investigates the complex dynamics of equity within food bank operations by employing established inequality measures—specifically, the Gini Index, and Hoover Index—to assess and quantify disparities in food distribution across the Food Bank of Central and Eastern North Carolina (FBCENC). The Gini Index provides a numerical depiction of distribution inequality, while the Hoover Index quantifies the proportion of resources that would need redistribution to achieve equity. Utilizing data from the FBCENC, the study calculates these indices at both network and branch levels utilizing Python algorithms and visualizes the results using Tableau. This dual approach facilitates comprehensive mapping and trend analyses, offering interactive insights into distribution patterns. The findings highlight areas with significant disparities, revealing branches where food distribution exhibits marked inequity. These insights guide targeted strategies for more balanced resource allocation. Furthermore, this research sets the stage for future work to compare these traditional inequality measures with the Fairshare analysis—a standard currently used by food banks—to determine alignment and potential discrepancies.",3,114 | Cobb Galleria Centre,Trustworthy and Adaptive AI for Critical Systems II,82
284,5838.0,"Exploring autistic Community Tweets Themes through Embeddings, Matrix Factorization, and Generative AI",,"The increasing presence of the autistic community on social media platforms has created a valuable repository of discourse that reflects their unique perspectives, challenges, and experiences. This study aims to explore and cluster themes within tweets from the autistic community using a novel approach that combines matrix factorization techniques with tweet embeddings and generative AI. Traditional text analysis methods, such as TF-IDF, often fail to capture the semantic richness inherent in social media, also thematic extraction for autistic people communication in social media posts was usually done by a manual process. To address these limitations, we utilize pre-trained language models to generate tweet embeddings, that undergo a dimensionality reduction using autoencoder neural network whose output is then subjected to Matrix Factorization to uncover latent themes. The further reduced-dimensional representations are clustered to identify coherent thematic groups. For this purpose, generative AI is employed to analyze and extract these clusters themes, offering deeper insights and enhancing the interpretability of the findings. The integration of tweet embeddings into matrix factorization introduces a novel methodology for thematic analysis in social media research.",4,114 | Cobb Galleria Centre,Trustworthy and Adaptive AI for Critical Systems II,82
285,5699.0,A Graph Neural Network Approach for Aquatic Biodiversity Prediction Leveraging Water System Interconnections,Academician,"Rapid climate change and human activities have led to dramatic declines in aquatic biodiversity, destabilizing ecosystems and accelerating ecological imbalances. As a result, analyzing and predicting these shifts in aquatic biodiversity accurately are crucial for effective conservation and sustainable resource management. Previous studies have focused on statistical analysis and basic machine learning methods based on water quality data from aquatic zones as independent, disregarding their connectivity. However, the interconnected nature of rivers, estuaries, and oceans means that aquatic biodiversity is influenced across these linked water systems. Therefore, in this study, we propose a new data structure, an aquatic biodiversity graph (ABG), to take advantage of these interconnected relationships. Then, we develop a novel deep learning model named as an ABG-based graph neural network (ABGNN), to analyze the ABG for aquatic biodiversity prediction. Firstly, ABGNN learns water quality information of observation-point nodes using a graph attention network by considering both spatial and temporal characteristics of the ABG and extracts node embeddings at the observation-point level. Then, our proposed method aggregated such embeddings into the sub-basin node embeddings and predicted aquatic biodiversity of each sub-basin in an end-to-end manner. For empirical analysis and model validation, we used river and marine water quality data and biodiversity data from the Republic of Korea, and the experimental prediction results of ABGNN showed superior performances to various baselines for aquatic biodiversity prediction. Our study provides a valuable tool for ecosystem management, enabling more informed decision-making for conservation efforts by accurately predicting biodiversity dynamics.",1,114 | Cobb Galleria Centre,Complex Systems and Scientific Discovery I,83
286,5819.0,An Investigation of the Effects of Clearness Index on Solar Energy Production,Academician,"Clearness Index (CI) plays a pivotal role in determining the efficiency of solar energy systems. In particular, the relationship between CI and solar production is non-linear and may significantly vary among sites. Based on a real dataset, this paper investigates the relationship between CI and solar energy generation, as well as the site-to-site variation of this critical relationship. At a site, the relationship between CI and solar generation is described by a non linear parametric model, while the random-effects model is employed to capture the site-to-site variation. We evaluate the model’s performance based on the Root Mean Square Error (RMSE) using Leave-One-Out Cross-Validation (LOOCV). The results provide insights on long-term solar energy planning by considering CI.",2,114 | Cobb Galleria Centre,Complex Systems and Scientific Discovery I,83
287,5962.0,Enhancing Microgrid Solar Output: Correlating Weather Data with Energy Generation in SEAR Lab,Academician,"In an effort to improve the analysis of solar energy generation within the SEAR Lab's microgrid, this study additionally incorporated a weather API to collect real-time meteorological data, such as temperature, cloud cover, UV index, and solar irradiance. We sought to gain a better understanding of the environmental elements impacting peak power production by establishing a correlation between weather and solar power output. As a direct indicator of sunlight reaching the Earth's surface, solar irradiance was especially important for determining times of high power output, while the UV index offered more information about variations in intensity throughout the day. Data on cloud cover made it possible to analyze how different sunshine conditions and shading impacts affected energy generation and, consequently, grid power dependency. Regression analysis was used to find a linear correlation between the microgrid's solar power statistics and weather variables. This enabled us to identify the ideal circumstances for the highest solar output, which showed that times with high irradiance and little cloud cover produced the most energy. The investigation also revealed consistent power outages during cloudy or humid weather, which can be utilized to predict energy consumption and dynamically modify grid dependency. These results are crucial for evaluating the reliability of solar energy as a renewable energy source and help optimize microgrid performance in response to meteorological conditions. This method offers a thorough framework for improving solar power reliability and making wise judgments in sustainable energy management by tying weather patterns to energy production.",3,114 | Cobb Galleria Centre,Complex Systems and Scientific Discovery I,83
288,8636.0,Cardiovascular Risk Prediction Utilizing Decision-Tree and Information Theory Methods,,"Cardiovascular diseases have conventionally remained at the top of the list with regard to mortalities worldwide, creating an extraordinary health challenge to both developing and industrialized nations. These diseases require effective early detection and monitoring to ensure that the rates of mortality are reduced significantly. It is in identifying this key need that our project focuses on sophisticated prediction models using machine learning techniques in analyzing a number of patient heart attributes. Specifically, the decision trees in our models make use of Entropy and Gini impurity analyses. Our methods also draw from information theory, especially mutual information. We applied these techniques to find a publicly available dataset on Kaggle. Thorough testing of our predictive models was done by confusion matrices and cross-validation techniques to ensure the results were sound and reliable. A key conclusion of the present work is that early identification of heart diseases with our models will help high-risk patients make more informed choices about their lifestyle modification. Such preventive measures are critical to prevent further disease cascades and can prove to be a milestone in medical science. This study has pointed out predictive modeling, which is most important for changing the face of preventive care and improving patient management in cardiology. These can allow for early detection and, thus, early intervention, therefore enormously improving health outcomes and efficiency. This is how advanced analytics can substantially impact public health and medical practice.",1,113 | Cobb Galleria Centre,AI Methods for Healthcare IV,84
289,6357.0,Challenges in Predicting Breast Cancer Recurrence Time: Literature Review,Academician,"Breast cancer relapse poses a significant risk, affecting patient survival rates. However, the early prediction of recurrence can encourage adherence to post-treatment surveillance guidelines and enable timely interventions and appropriate measures to mitigate disease progression and mortality effectively. Researchers have leveraged machine-learning techniques for risk group identification, screening methods improvement for early detection of recurrence, biomarkers identification, and personalized treatment strategies.However, limited focus has been directed toward forecasting the recurrence of breast cancer timing. Hence, this study aims to investigate the machine learning and deep learning models used to predict the timing of recurrence for breast cancer survivors. A non-biased search of literature was conducted using the PRISMA methodology, across different databases Web of Science, PubMed, and Science Direct, up to 2024, to identify machine learning and deep learning models used to predict the time-to-recurrence. Studies were included if they focused on predicting the risk of recurrence or recurrence time using machine learning, deep learning, or statistical methods, with a population of breast cancer patients who completed curative treatment. Screening involved title and abstract screening, followed by full-text screening to ensure the inclusion of relevant studies that met the predefined criteria. Data extraction focused on model types, predictive accuracies, data sets used. Initial analysis indicates a gap in the literature with only eight articles specifically addressing the prediction of time-to-recurrence using machine learning techniques. This limited focus underscores the need for further development in this area, as most existing models prioritize risk group identification without targeting precise recurrence timing.",2,113 | Cobb Galleria Centre,AI Methods for Healthcare IV,84
290,7069.0,Enhancing Hospital Mortality Prediction: A Comprehensive Approach Through Machine Learning and Model Interpretability,Practitioner,"Hospital mortality prediction remains a significant challenge in healthcare, primarily due to the complexity and diversity of patient data, which spans physiological measurements and demographic information. Accurate predictions have the potential to enhance clinical decision-making, optimize resource allocation, and improve patient outcomes. This study addresses critical gaps in predictive modeling for hospital mortality, with an emphasis on comprehensive data preprocessing, class balancing, and model interpretability. Robust preprocessing techniques, including Principal Component Analysis (PCA) for dimensionality reduction, were applied to optimize model performance by minimizing multicollinearity and computational demands. Among the models tested, Logistic Regression emerged as the most balanced and reliable, achieving an accuracy of 84.06%, a precision of 84.82%, and a recall of 83.23%, making it the preferred choice for practical application due to its balance of high performance and reduced risk of overfitting. Additionally, interpretability frameworks, SHAP and LIME, were incorporated to increase transparency regarding feature influence, which is crucial for clinical applicability. An interactive dashboard was developed to support healthcare professionals in interpreting and utilizing model results in real-time. This research present a transparent and user-centered framework for hospital mortality prediction, with implications for improving clinical decision support systems in healthcare.",3,113 | Cobb Galleria Centre,AI Methods for Healthcare IV,84
291,6720.0,Early prediction of patient admission likelihood in emergency departments,,"Efficiently transitioning patients from the emergency department (ED) to inpatient units is a critical challenge in the US, often contributing to ED overcrowding. Early prediction of patients' admission likelihood and the demand for inpatient units enables hospital managers to apply proper capacity management strategies before real changes in demand happen. Our developed prediction model incorporates clinical, demographic, and visit-related information, as well as unstructured triage notes. This model serves as a decision support tool to provide the inpatient unit managers with near real-time predictions of their unit's demand when the patients are still in the early stages of their caregiving process in the ED. The study proves that predicting the likelihood of admitting patients to inpatient units will help identify the system's changing status and make proactive decisions before any surge in demand happens. Such a responsive decision-making approach can improve healthcare systems' operational efficiency and capacities.",1,113 | Cobb Galleria Centre,AI Methods for Healthcare V,85
292,6500.0,A Comparative Analysis of Machine Learning Models for Heart Disease Prediction Using Clinical Parameters,Academician,"Cardiovascular disease remains one of the leading causes of mortality worldwide, with early detection being crucial for improving patient outcomes. This study aims to develop and validate a machine learning-based prediction model for heart disease using electronic health records from a major hospital system in New Jersey. We will analyze retrospective data from adult patients who received care between 2021 and 2024, incorporating demographic information, vital signs, laboratory values, medication history, and imaging results. The study will utilize supervised machine learning algorithms, including random forest, gradient boosting, and deep learning approaches, to predict the presence of significant coronary artery disease. The dataset will be split into training (70%) and validation (30%) sets, with feature selection performed to identify the most significant predictors of cardiovascular disease. Performance metrics including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve will be calculated to evaluate model effectiveness. The findings from this study will contribute to the development of more accurate risk prediction tools for cardiovascular disease in clinical settings and potentially aid in early intervention strategies. Furthermore, this research will provide insights into the most influential predictors of heart disease specific to our patient population in New Jersey.",2,113 | Cobb Galleria Centre,AI Methods for Healthcare V,85
293,6380.0,A Machine Learning Approach to Predicting Nonalcoholic Fatty Liver Disease Using Non-Invasive Health Indicators,,"Nonalcoholic fatty liver disease (NAFLD) is a widespread liver condition associated with obesity, type 2 diabetes, and metabolic syndrome, impacting over a quarter of the global population. This study aims to develop a machine learning model for predicting NAFLD, using only demographic, vital sign, and laboratory data from the National Health and Nutrition Examination Survey (NHANES) 2017–2020 cycle to enable accessible, non-invasive screening options. Adults aged 18 years and older with relevant health data were included, excluding individuals with high alcohol consumption or certain liver-related conditions. NAFLD was defined using a Controlled Attenuation Parameter (CAP) threshold of ≥302 dB/m based on non-invasive measurements. An extensive data-driven approach was applied to enhance model reliability and address data challenges, focusing on understanding the significance of various health indicators while ensuring model interpretability and accuracy. The model effectively identified NAFLD prevalence within a diverse U.S. cohort, highlighting key health indicators associated with increased disease risk. The results demonstrate strong potential for accurately identifying NAFLD using commonly available health data, supporting the model’s applicability in large-scale, real-world settings. This study emphasizes the value of a non-image-based, data-driven approach to developing scalable screening tools for NAFLD. By utilizing routinely collected clinical and demographic information, the model aids in early detection and risk stratification for NAFLD, addressing diagnostic gaps and advancing targeted public health interventions, particularly in underserved areas.",1,113 | Cobb Galleria Centre,AI Methods for Healthcare VI,86
294,6465.0,Modeling the menstrual cycle stages in the identification of endometriosis biomarkers,,"Endometriosis is a gynecological disorder where the endometrial tissue that lines the inside of the uterus also grows outside it, often causing severe pain and infertility. Approximately 10% of women worldwide have endometriosis, although prevalence statistics are hindered by the lack of satisfactory non-invasive diagnostic methods. Researchers have long sought biomarkers for early, non-surgical diagnosis, yet no potential biomarkers have been approved. One obstacle is the lack of studies accounting for the fluctuation of gene expression throughout the menstrual cycle stages. This study's objective is to make a comparative analysis of gene expression in endometriosis lesions and in endometrium samples and to evaluate the transcriptomic impact of the menstrual cycle stages. The 150 expression samples were obtained from the Gene Expression Omnibus (GSE141549). Samples were separated by the menstrual stage in which they were recollected (proliferative and secretory). To determine the most relevant genes, four feature selection methods were implemented – recursive feature elimination (RFE), gain ratio, one R, and symmetry uncertainty – and their performance was evaluated using accuracy and area under the curve using Random Forest classifier. Preliminary results showed nine genes (MEOX1, MEOX2, C7, LCN6, LOC375295, SCN4B, C18orf34, LTC4S, and EBF3) to be differentially expressed across all groups. Stage-specific genes were also identified, revealing more differentially expressed genes in the proliferative stage than in the secretory. This work is currently confirming these putative markers across other datasets using a consensus machine learning approach which will provide insight on possible biomarkers and their potential in diagnosis and treatment.",2,113 | Cobb Galleria Centre,AI Methods for Healthcare VI,86
295,5189.0,Breast Cancer Prediction: A Comparative Study with Radial Basis Function Networks and Traditional Machine Learning Models,,"Breast cancer remains a major global health challenge, representing the second leading cause of cancer-related mortality among women. Early detection and accurate prognosis are critical to improving treatment outcomes and increasing patient survival rates. Recent advancements in machine learning (ML) have demonstrated significant potential in enhancing breast cancer prediction and diagnosis. In this study, we propose a comprehensive ML-based framework for breast cancer classification using features derived from fine needle aspiration (FNA) cytology, aiming to classify cases as benign or malignant. To enhance predictive performance, we employed Principal Component Analysis (PCA) for dimensionality reduction in conjunction with logistic regression and support vector machine (SVM) models, while the original feature set was used for XGBoost, random forest, and radial basis function network (RBFN) models. All models were trained on a balanced dataset, with class imbalance addressed through appropriate resampling techniques. The Wisconsin Breast Cancer Dataset (WBCD), comprising 569 instances, was subjected to data cleaning, normalization, and resampling to correct class imbalance. To enhance feature selection, Recursive Feature Elimination (RFE) was employed. The predictive performance of logistic regression, SVM, random forest, XGBoost, and RBFN models was systematically evaluated and compared to determine the most effective model for breast cancer classification.",3,113 | Cobb Galleria Centre,AI Methods for Healthcare VI,86
296,9010.0,Should We Stop Doing Geriatric Assessments in Patients Under 75?,Practitioner,"The Older Adults with Cancer Clinic (OACC) performs geriatric assessments (GAs) for cancer patients aged 65+ and has both primary and secondary outcomes for these patients. With limited resources and increasing wait times, targeting age 75+ vs 65+ is being considered. To compare whether the GA is associated with similar impact on the treatment plan (primary) as well as addressing abnormal GA domains and enhancements to care (secondary outcomes) for patients under 75 compared to those 75+. Data was obtained from a customized clinical database. Descriptive analytics were used to identify differences in the primary and secondary outcomes of GA for patients under 75 versus those 75+ using chi-square testing. For the primary outcome, the results reveal a significant association between age group and treatment change ( 2 = 8.25, = 0.004). Specifically, 52.6% of patients aged 75+ had a treatment change versus 41.8% under 75. Aside from the 'mood' domain ( 2 = 6.43, = 0.01), no notable difference was observed in the percentage of patients with abnormalities across the GA domains. Similarly, no significant differences in enhancements to care were found between patients under 75 and those 75+. While patients aged 75+ are more likely to undergo treatment changes after GA, the 41.8% change rate among those under 75 suggests GA remains valuable for them as well. Furthermore, the absence of a difference between age groups in secondary outcomes confirms the value of GA in improving the care of patients under 75, as well as those aged 75+.",1,113 | Cobb Galleria Centre,AI Methods for Healthcare VII,87
297,5689.0,Knowledge-Informed Sex-Specific Predictive Modeling for Advancing Precision Medicine,Academician,"Precision medicine in oncology faces significant challenges due to complex intratumoral heterogeneity, particularly in aggressive cancers like glioblastoma (GBM). Recent studies have revealed sex-specific cellular compositions in GBM, emphasizing the critical need for sex-specific approaches in tumor characterization and treatment planning. While Spatial Transcriptomics (ST) technology enables detailed mapping of cellular distributions within tumor samples, its invasive nature limits frequent assessments. To address this limitation, we propose a Machine Learning (ML) model to predict cell type proportions based on non-invasive Magnetic Resonance Imaging (MRI), integrating domain knowledge and accounting for sex-specific patterns. Our dataset comprises MRI scans and corresponding ST-derived cell type proportions from 191 image-localized biopsies (76 female, 115 male) across 56 GBM patients. We employ advanced ML algorithms to predict proportions of multiple cell types based on MRI. Our approach integrates prior knowledge from existing datasets, considers tumor region specificity, and accounts for sex-dependent characteristics in GBM. This study aims to develop a non-invasive method for predicting GBM cellular composition, potentially reducing the need for frequent biopsies and enabling more personalized treatment strategies. By integrating sex-specific patterns and domain knowledge, we expect to enhance prediction accuracy and provide insights into sex-dependent tumor characteristics. Successful implementation could significantly impact personalized GBM management and advance our understanding of tumor biology through the lens of imaging-based cellular composition prediction.",2,113 | Cobb Galleria Centre,AI Methods for Healthcare VII,87
298,5219.0,How to predict stock prices of trillion dollar club companies using deep learning analysis?,Academician,"Global interest in forecasting stock prices has encouraged using neural networks to improve model accuracy. This study explores the relationship between technical analysis indicators and Recurrent Neural Networks (RNN) to predict stock prices for NVIDIA, a key player during the artificial intelligence revolution. NVIDIA became the world's most valuable company in 2024, surpassing Microsoft and Apple, driven by demand for graphic processing units in the Artificial Intelligence (AI) space. NVIDIA first joined the trillion club in May 2023 and has added more than two trillion in additional market value since then. This research focuses on the use of deep learning (DL) algorithms to predict the daily closing stock price, employing financial indicators, which include simple moving average (SMA), convergence divergence moving average (MACD), and relative strength index (RSI) as predictive variables. This approach contains data preprocessing and feature selection through Principal Component Analysis (PCA). The effectiveness of three different DL algorithms is compared: Long Short Term Memory (LSTM) networks, Gated Recurrent Units (GRU), and Convolutional Neural Networks (CNN), as well as combined architectures between them. Optuna facilitates hyperparameter tuning, enhancing model performance. The BILSTM-GRU model yields the best performance, improving up to 19.05% on the mean average error and up to 15.52% on the R2 test. The proposed approach demonstrates combining technical indicators with recurrent neural networks increases model effectiveness in stock market forecasting practices.",1,114 | Cobb Galleria Centre,Machine Learning for Time Series I,88
299,9076.0,"A Hybrid Approach to Asset Pricing: Integrating Markov Chains, Brownian Motion, and Machine Learning",Academician,"Accurate asset pricing is critical for informed decision-making in financial markets, yet existing models often struggle to capture the complex, stochastic nature of price movements. This study proposes a novel hybrid framework that integrates Markov Chains, Brownian Motion, and Machine Learning to address these challenges. The Markov Chain component models discrete state transitions in asset price regimes, capturing temporal dependencies, while Brownian Motion is employed to reflect the continuous and random fluctuations inherent in financial time series. Machine Learning algorithms, including Random Forest and Gradient Boosting, are leveraged to enhance predictive accuracy by learning non-linear relationships in the data. This framework is further refined using advanced parameter estimation techniques such as Markov Chain Monte Carlo (MCMC), ensuring robust calibration and seamless integration of stochastic and predictive components. By combining these methodologies, the model effectively bridges the gap between theory and practical application, offering improved performance in both short-term and long-term asset pricing scenarios. The innovative design enables the framework to not only predict asset prices with greater precision but also to provide a clearer quantification of associated uncertainties, making it a robust tool for risk assessment and portfolio management. The study’s findings highlight the transformative potential of blending discrete and continuous stochastic modeling with modern machine learning techniques. Applications include scenarios such as high-frequency trading and credit risk evaluation, showcasing the adaptability and practical significance of this hybrid approach in addressing complex financial challenges.",2,114 | Cobb Galleria Centre,Machine Learning for Time Series I,88
300,6858.0,Neural Stochastic Delay Differential Equations for Astronomical Time Series Classification and Novelty Detection,Academician,"Fundamental difficulties in astronomical research arise from irregular sampling and incomplete time series data, especially when working with datasets from large-scale surveys such as the Large Synoptic Survey Telescope (LSST). In order to tackle the crucial tasks of classification and novelty detection in astrophysical data analysis, we suggest a novel mathematical framework based on Neural Stochastic Delay Differential Equations (Neural SDDEs). Our method combines stochastic delay differential equations with neural networks, where the neural component explicitly accounts for time delays in the underlying dynamics while learning the drift coefficients. The framework includes mechanisms to deal with irregular temporal sampling, a numerical solver for the resulting stochastic system, and a neural network architecture for learning SDDE parameters. Using astronomical time series data that is irregularly sampled, we assess the model's performance on classification tasks. The outcomes show that even when there are temporal irregularities in the observations, the model can still maintain classification accuracy. Furthermore, we use our framework to detect anomalous astrophysical events for novelty detection when the labels are not complete. By providing improvements in classification and anomaly detection capabilities over current techniques, this work establishes Neural SDDEs as a mathematically principled method for analyzing astronomical time series impacted by observational constraints.",1,114 | Cobb Galleria Centre,Machine Learning for Time Series II,89
301,5995.0,Transformer Conformal Prediction for Time Series,Academician,"We present a conformal prediction method for time series using the Transformer architecture to capture long-memory and long-range dependencies. Specifically, we use the Transformer decoder as a conditional quantile estimator to predict the quantiles of prediction residuals, which are used to estimate the prediction interval. We hypothesize that the Transformer decoder benefits the estimation of the prediction interval by learning temporal dependencies across past prediction residuals. Our comprehensive experiments using simulated and real data empirically demonstrate the superiority of the proposed method compared to the existing state-of-the-art conformal prediction methods.",2,114 | Cobb Galleria Centre,Machine Learning for Time Series II,89
302,5274.0,Enhancing Time Series Forecasting with an Ensemble Model of Neural Networks and HGARCH,Academician,"In this paper, we propose a novel ensemble model that integrates a Neural Network (NN) with a GARCH-incorporated loss function and an HGARCH (Hierarchical Generalized Autoregressive Conditional Heteroskedasticity) model to improve forecasting accuracy in time series data with volatility clustering. This approach is particularly useful for financial data, healthcare demand prediction, or call center workload forecasting, where volatility is a key feature of the data. The neural network serves as the first ensemble member and is known for its strength in capturing complex, non-linear relationships. By embedding a GARCH penalty in its loss function, the NN is better equipped to manage volatility patterns that are typically modeled by GARCH. This allows the network to account for dynamic fluctuations in the variance of residuals, enhancing its ability to forecast in volatile environments. The second ensemble member is an HGARCH model, which captures long-term volatility persistence in the data. Combining these models in an ensemble framework allows for improved accuracy, as the NN addresses non-linearities, while the HGARCH model robustly handles volatility. This hybrid approach offers a powerful tool for time series forecasting where volatility and persistence are crucial factors.",3,114 | Cobb Galleria Centre,Machine Learning for Time Series II,89
303,6511.0,Machine Learning Classification of Parent-Child Stem Design Conversations,Academician,"There is growing interest in text conversational tools that aid learning. Family engagement in STEM design activities play a huge role in shaping children's STEM interests and skills. Understanding these interactions is crucial for improving learning process and outcomes. Comprehensive analysis of large data from STEM design activities is often done manually, which is time-consuming. The application of machine learning supports automated, rapid, reproducible analyses of large datasets, identifying patterns that would be infeasible through manual coding. This study applied machine learning techniques to classify conversations between parents and children during STEM design activities. The dataset was generated by transcribing video recordings from STEM design activities involving different families and annotating them to label the conversations by design stages. These stages included problem identification, understanding, ideating, designing, building, testing, and communicating. Standard preprocessing and feature extraction techniques were applied to the data, split into 80% for training and 20% for testing the developed model. Accuracy, precision, recall, confusion matrix and F1-score were used to assess the performance of the different models. Results showed that feature overlap and imbalanced datasets across different design stages impact the accuracy of predictions. The results show the potential of machine learning techniques to classify design stages in conversations from parent-child STEM design activities. The findings indicate machine learning potential to effectively distinguish various stages of conversations, which is a strong starting point for developing tools to help educators create more engaging and effective learning environments. Recommendations for model improvement are also suggested.",1,114 | Cobb Galleria Centre,Machine Learning Applications,90
304,6853.0,Association between Longitudinal Cognitive Measures and Brain Atrophy in Healthy Adults,Academician,"This study investigates the association between longitudinal cognitive measures and brain atrophy in healthy adults using data from the UK Biobank. Our primary aim is to explore whether changes in cognitive performance over time correlate with volumetric alterations in specific brain regions, as measured by structural magnetic resonance imaging (sMRI). We analyzed longitudinal cognitive assessments, including measures of Fluid Intelligence Score (FI) and Reaction Time (RT), and compared these data with repeated sMRI scans to assess changes in brain volumes. This study underscores the value of combining longitudinal cognitive and neuroimaging data to improve our understanding of brain aging, potentially informing early detection of cognitive decline in the general population.",2,114 | Cobb Galleria Centre,Machine Learning Applications,90
305,5178.0,"Comparative Analysis of LDA, BERTopic, NMF for Topic Modeling",,"This study investigates the integration of topic modeling and emotion detection within healthcare-related tweets to gain deeper insights into public sentiment and thematic trends. To identify common themes, multiple topic modeling techniques, including Latent Dirichlet Allocation (LDA), BERTopic, and Non-negative Matrix Factorization (NMF), are applied and compared. This comparative analysis helps in uncovering consistent topics across models, ensuring robust and comprehensive theme identification.",3,114 | Cobb Galleria Centre,Machine Learning Applications,90
306,6762.0,Professional Use of Large Language Models - What works and what doesn't?,Practitioner,"AI large language models (LLMs) like ChatGPT can improve the efficiency of common language generation tasks performed by engineering professionals. In this session, we draw upon emerging academic research to suggest applications of LLMs in the field of industrial and systems engineering. Our demonstrations will showcase both the immense capabilities as well as the significant risks involved in using these tools. Our session includes six specific, practical principles to effectively and safely leverage LLMs. Learning Objectives Identify ChatGPT and other large language models. Recognize ChatGPT’s abilities in financial accounting. Analyze strengths and weaknesses of large language models. Apply six principles for the effective application of ChatGPT and other large language models.",1,106 | Cobb Galleria Centre,LLM and Its Application,91
307,6621.0,Leveraging LLM for Sentiment Detection in News Headlines,Academician,"In this study, a longitudinal dataset of more than 23 million news headlines from 47 U.S.-based media outlets is used to investigate the use of large language models (LLMs) for sentiment detection. Recent developments in LLMs offer potential gains in contextual understanding, adaptability, and generalization, even though traditional models have been used extensively for emotion classification. Using zero-shot and few-shot learning paradigms, which enable the model to generalize to new contexts with little task-specific data, we apply LLM-based emotion detection and assess its performance. The usefulness of sophisticated NLP models for media sentiment research is demonstrated by our analysis, which focuses on how well LLMs capture subtle emotional cues across time periods and news sources. The findings establish a new benchmark for emotion analysis in extensive, real-world text datasets by showing that LLMs can perform competitively and frequently better even with little supervision.",2,106 | Cobb Galleria Centre,LLM and Its Application,91
308,6964.0,"The Role of Large Language Models in Transforming Radiology and Oncology: Applications, Benefits, and Challenges",Academician,"This paper explores the potential applications of large language models (LLMs) in the fields of radiology and oncology, assessing their effectiveness in diagnostic support, clinical decision-making, and patient management. By analyzing recent advancements in LLM technology, we evaluate its effectiveness in interpreting complex medical imaging data, enhancing diagnostic accuracy, and contributing to developing personalized treatment plans for oncology patients. Furthermore, the study investigates how LLMs can facilitate knowledge extraction from medical datasets, providing clinicians faster access to relevant documents and literature to integrate the latest clinical guidelines into patient care practices. In addition to these benefits, the study addresses critical challenges associated with LLM implementation, including data privacy, model interpretability, and integration into clinical infrastructures. These challenges and ethical considerations offer insights into the benefits and limitations of LLMs in enhancing radiology and oncology practices. Our findings highlight the transformative potential of LLMs, with implications for improving patient outcomes and streamlining clinical workflows in cancer care.",3,106 | Cobb Galleria Centre,LLM and Its Application,91
309,6270.0,Exploring Large Language Models as Decision Support Tools: A Proof of Concept for Procedure-Related Information Retrieval,Academician,"Policy and procedural documentation are essential for the effective operation of government agencies. However, the vast volume of these documents often obscures critical information within hundreds of pages of unrelated content. Recent advancements in large language models (LLMs) enhance text search and response generation capabilities, enabling efficient extraction and summarization of key content from extensive documents. This study evaluates whether open-source LLMs can generate accurate policy briefs to support decision-making. The research has two primary objectives: (1) to assess multiple LLMs available through Ollama, including LLAMA 3.1-8B, Mistral 7B, and Gemma 2, to determine their efficacy with policy and procedural texts, and (2) to assess whether these generated responses can serve as a practical tool for government employees. Evaluation metrics include computational analyses and feedback from professionals in the public policy sector. Preliminary results indicate that open-source LLMs effectively analyze extensive policy documentation and generate concise, relevant summaries in response to user queries. We conclude that LLMs present a valuable resource for policymakers and government employees, enabling efficient access to accurate summaries that support policy development and implementation. Future work is still needed to improve response structure and explore the capabilities of larger models.",4,106 | Cobb Galleria Centre,LLM and Its Application,91
310,6139.0,Automatic Real-time Facial Expression Recognition using a Novel AI Enabled Extended Reality System,,"Facial Expression Recognition (FER) holds a pivotal role in deciphering human emotions, yet its progression is hindered by complexities stemming from varying facial structures, distinct poses, fluctuating illumination, and unconventional angles. The urgency to address the surging demand for instantaneous FER solutions has never been more critical. Extended Reality (XR) emerges as a transformative force, unlocking new possibilities in immersive training, innovative education, precision healthcare, enhanced user interfaces, and the seamless acquisition of pertinent data. This study seeks to pioneer an AI-powered XR system tailored for FER by fusing the ingenuity of a novel Depthwise Separable Convolutional Neural Network (DS-CNN) with the immersive capabilities of XR technology. Employing the FER2013 image dataset as the foundational training framework, the envisioned FER model was meticulously crafted and rigorously optimized. To substantiate the robustness of the design, the model underwent validation against two distinct image datasets, consistently surpassing the performance metrics of pre-existing models across both. Subsequently, the optimized CNN model was seamlessly amalgamated with Microsoft HoloLens 2 XR technology, culminating in the realization of a real-time, autonomous FER system. The efficacy of this integrated system was meticulously evaluated through the System Usability Scale (SUS) and NASA-TLX metrics. The findings revealed exceptional system usability and a markedly reduced cognitive load, especially when juxtaposed with traditional FER approaches reliant on ocular analysis. This AI-driven XR paradigm unveils profound applicability across a multitude of domains, delivering actionable managerial insights and operational efficiencies.",1,106 | Cobb Galleria Centre,Extended Reality and ML Applications,92
311,8495.0,Large Language Models for Condition Monitoring: A Feature Encoder-Decoder Framework,Academician,"The development of generative artificial intelligence, particularly large language models (LLMs), offers new opportunities for condition monitoring (CM) systems. Unlike conventional machine learning techniques, LLMs can identify patterns, reason about real-world events, and analyze time series data while generating human-readable explanations for changes in industrial asset conditions. However, LLMs’ linguistic data process nature makes them struggle with handling the noisy and high-dimensional sensory data from industrial assets. The cost of training and running LLMs also impeded their application in CM systems. We explore a CM framework that combines customized feature encoders with LLMs as decoders, to achieve good condition prediction, possibly performing on-par or better than the performance of the existing approaches. The encoder in the framework converts sensory data into three canonical time series features that can capture both abrupt and gradual changes in the condition of industrial assets. These features are then transformed into open-ended question-answer style prompts. In the decoder phase, LLMs are prompted to learn the patterns generated by the encoder through fine-tuning, using a low-rank adaptation technique, which reduces the number of trainable parameters to between 0.1% and 1% of those required for full tuning. Using our experiments on Backblaze hard disk drive data, we demonstrate the feasibility and practicality of the LLM-based condition monitoring approach with GPT-2, Gemma, and Llama LLMs.",2,106 | Cobb Galleria Centre,Extended Reality and ML Applications,92
312,5636.0,AI-enabled Extended Reality System for Real-Time Driver Behavior Monitoring,,"The World Health Organization (WHO) reports that approximately 1.19 million people die annually due to road traffic accidents, while 20 to 50 million others suffer non-fatal injuries, often leading to disabilities. One of the primary causes is driver distraction, with mobile phone use being an increasing concern for road safety. This study utilized the image-based distracted driver dataset V2, featuring data from 44 drivers across 10 different classes. The dataset, in RGB format, includes images captured from a side-view camera focusing on the driver’s body. To enhance the dataset, a new class was added for drowsy drivers, as statistics indicate that drowsy driving contributed to an estimated 17.6% of fatal car crashes between 2017 and 2021. Transfer learning techniques were applied for classification purposes, and the proposed Deep Convolutional Neural Network (DCNN) model, based on MobileNet, achieved 93.60% accuracy and an F1-score of 0.94. The trained AI model was subsequently deployed on an Extended Reality (XR) device to facilitate real-time monitoring of driver behavior.",3,106 | Cobb Galleria Centre,Extended Reality and ML Applications,92
313,6484.0,Optimize ProCCM Sweeps With Simulation Optimization for Malaria Control,Academician,"Malaria remains a deadly disease, killing thousands each year, especially children under five. Timely treatment is crucial not only to save lives but also to reduce transmission and protect susceptible populations. However, in many regions, poor access to healthcare due to physical barriers, cultural factors, and mistrust prevents timely treatment. Proactive community case management (ProCCM), where healthcare workers visit households regularly to find and treat sick individuals, is one approach to reducing the impacts of these burdens. The effectiveness of ProCCM depends on factors such as transmission patterns, intensity, and treatment-seeking behavior. To optimize ProCCM implementation, we propose a simulation optimization approach. We first calibrate the agent-based malaria simulation with field data and use it to evaluate the impact of different implementation strategies. At the beginning of each stage, the decision maker chooses the frequency of sweeps for the next stage under limited healthcare resources. Our goal is to find the best decision that minimizes the total symptomatic cases over the entire transmission season. We formulate this problem as a large scale ranking and selection problem and apply the parallel algorithm to select the best sweep strategies.",1,Fulton | Renaissance Waverly Hotel,Simulation for Planning and Intervention in Healthcare,93
314,5951.0,Hospital Resource Capacity Planning in Labor and Delivery Department: A Simulation Approach,Practitioner,"Efficient resource allocation and capacity planning are critical challenges in hospital labor and delivery departments, where patient variability and unpredictable demands can strain existing resources, which lead to prolonged wait times and can negatively impact patient care quality. Previous research highlights the growing importance of optimizing hospital operations to maximize resource utilization while maintaining care standards. This study develops a discrete-event simulation model that is tailored to a hospital's labor and delivery department. The primary objective is to identify strategies for reducing bottlenecks and crowding, minimizing patient wait times, and at the same time optimizing resource utilization within the department. Through multiple ""what-if"" scenarios and experimentations, the simulation model assesses different resource allocation strategies, which offer insights into areas where changes can yield operational improvements. After modeling various intricate patient flows, preliminary results indicate that targeted adjustments to resource distribution can mitigate bottlenecks and enhance overall efficiency of the labor and delivery department. Serving as a decision-support tool, the research outcomes provide hospital administrators with strategies to better allocate resources and capacity planning in short and long term planning horizons.",2,Fulton | Renaissance Waverly Hotel,Simulation for Planning and Intervention in Healthcare,93
315,9337.0,Simulation and Optimization Approaches for Pandemic Response: EpiMORPH,Practitioner,"Epidemiological modeling is a critical tool for understanding the dynamics of pathogen transmission and predicting future outcomes, supporting public health responses. While illustrating the power of modeling in epidemic management, SARS-CoV-2 also highlighted inefficiencies in current modeling practices, with myriad models of varying assumptions, approaches, and quality flooding public forums. One of the main issues is that modeling studies and software are so customized that scientists must often start from scratch to create model-based technologies that address new pathogens or that seek to understand transmission dynamics in new locations. Moving epidemiological modeling from a collection of independent studies towards a more efficient, consensus-building endeavor will require the innovation of tools that accelerate the construction and testing of models, and that facilitates rigorous model comparison. This talk introduces EpiMoRPH ( E pidemiological M odeling R esources for P ublic H ealth), a groundbreaking tool for advancing epidemiological modeling. It provides a robust framework for rapid model development and evaluation, promoting automation, collaboration, and deeper understanding of epidemic dynamics. EpiMoRPH features an advanced optimization toolkit, empowering public health practitioners to make informed decisions on intervention placements and strategies. We will focus on the latest components of the tool, including its system dynamics modeling and projection capabilities and advanced optimization models for resource and vaccine allocation during a pandemic.",3,Fulton | Renaissance Waverly Hotel,Simulation for Planning and Intervention in Healthcare,93
316,6862.0,Redesign of Hospital Admission Workflows: A Value Stream Mapping Approach,,"Hospital admissions are a critical component of patient care, influencing efficiency, quality, safety, and satisfaction for both patients and staff. Enhancing these workflows is essential for delivering high-quality care in dynamic healthcare environments. This study examines admission processes across three distinct care units in a midsize regional hospital. Using a combination of process observation, time studies, and interviews with care unit staff, we developed current-state process maps and value stream maps to analyze the workflows and identify key problem areas. Through root cause analysis using the ‘5 Whys’ technique, we derived future-state value stream maps with targeted improvements for each unit's admission process. Key recommendations include establishing a transfer team, implementing written communication protocols, developing checklists to ensure information accuracy, and uploading discharge assessments directly to patient charts for timely review by hospitalists. Our findings highlight supporting activities within admission workflows that significantly impact care efficiency, facilitating value-driven process redesigns to enhance patient flow and satisfaction. This research provides a framework for healthcare process improvement by integrating Value Stream Mapping and root cause analysis. While our study is limited to specific units, it encourages further exploration of admission workflow variations across diverse healthcare settings to inform broader applications in strategic healthcare improvements.",1,Fulton | Renaissance Waverly Hotel,Lean Approaches for Healthcare Transformation,94
317,6576.0,Complex Organizational Change: Understanding Clinician Readiness for Intervention Change in Cardiovascular Critical Care,Practitioner,"Early mobility and assistance with activities of daily living can improve outcomes and a patient’s sense of independence. Yet our current restrictive sternal precautions (no lifting more then 5-10 lbs for 8 weeks, may increase by 5 lbs. each week; no pushing or pulling with both arms; no raising both arms above shoulder level at the same time, single arm movements are encouraged; activities with vibration or aggressive arm motion or chest presser should be avoided) limit patient’s participation in routine activities. These precautions can decrease their feeling of independence and increase their reliance on others. The feeling of dependance can have an impact on emotional wellbeing. Current restrictive standards for sternal precautions are often based on past practice, provider preference, and they vary from facility to facility. There has been an increase in research assessing alternatives to our standard sternal precautions. Studies have shown increased discharge to home with the less restrictive activities. The purpose of this study was to assess if changing from current restrictive post sternotomy sternal precautions to a less restrictive “Keep Your Move In the Tube” (KYMITT TM ) activity will have an impact on post-surgical Length of Stay (LOS), patient discharge disposition, and readmissions for sternal complications. The team also assessed the staff members that work in our Intensive Care Unit, Cardiovascular Unit, Cardiovascular Surgical specialists, and inpatient physical and occupational therapists readiness for change. Staff’s knowledge and attitudes can have an impact on adaptability to the new program and their ability to provide patient education.",2,Fulton | Renaissance Waverly Hotel,Lean Approaches for Healthcare Transformation,94
318,8802.0,Human-in-the-Loop and Systems Thinking: Transforming Sepsis Management in Complex Comorbid Conditions,Academician,"Integrating human-in-the-loop (HITL) methodologies and systems thinking offers transformative potential in managing sepsis among patients with comorbidities. This study examines the intersection of these approaches, emphasizing their role in enhancing decision-making, diagnosis, and treatment. Sepsis, a complex condition further complicated by comorbidities such as diabetes, chronic kidney disease, and cardiovascular disorders, requires adaptive and multidisciplinary strategies for effective management. HITL systems, which combine human expertise with advanced computational tools, support clinicians by refining diagnostic accuracy, prioritizing interventions, and mitigating the risks of overreliance on automated systems. Meanwhile, systems thinking provides a framework to understand the interdependencies between sepsis pathophysiology and comorbidities, highlighting how these interactions influence clinical trajectories. These approaches boost the dynamic, real-time collaboration between clinicians and technology, enabling tailored care for complex patient populations. Case studies show improved outcomes through HITL-supported early warning systems and predictive models integrated with comorbidity-aware protocols. However, challenges such as cognitive overload, interoperability issues, and the ethical implications of decision support systems remain. Future research must optimize these frameworks, address biases in data-driven algorithms, and enhance clinician training to synergize human judgment with machine intelligence. By leveraging HITL methodologies within a systems thinking approach, healthcare providers can better navigate the complexities of sepsis management in patients with comorbid conditions, improving precision, efficiency, and patient outcomes. This approach underscores the need for collaborative, technology-enabled care models to advance sepsis treatment in diverse, high-risk populations.",3,Fulton | Renaissance Waverly Hotel,Lean Approaches for Healthcare Transformation,94
319,5714.0,Patient Experiences between Urban and Rural Emergency Departments Visits,Practitioner,"Patient experiences in urban and rural emergency departments (EDs) often differ significantly due to variations in resources, wait times, and access to specialized care. Urban and rural hospitals fulfill distinct roles within the healthcare system, influencing patient expectations and experiences in each setting. This study explores patient online feedback from ED visits across urban and rural areas, focusing on how expectations, experiences, and resource needs vary between these environments. Urban patients frequently face challenges such as overcrowding and extended wait times, while rural patients may benefit from more personalized care despite limited services. By understanding these contrasts, healthcare providers can tailor improvements in both settings, addressing specific challenges and enhancing patient outcomes across urban and rural communities. Hospitals may leverage these findings to develop targeted healthcare features and service plans that meet the unique needs of their patient populations.",4,Fulton | Renaissance Waverly Hotel,Lean Approaches for Healthcare Transformation,94
320,6112.0,Transforming Healthcare Experience: Utilizing Digital Door Signs to increase efficiency and satisfaction,Practitioner,"Since 2023, the 24-bed Innovation unit at Mayo Clinic admitted 1584 patients. Patient care signage placed outside clinical rooms plays a critical role in patient and staff safety by displaying crucial information regarding isolations, precautions and care considerations. Currently, clinical staff manually review and update these signs, spending up to 65 minutes reviewing and walking over 400 feet each time. This process is required to be repeated at least every 4 hours due to changes in care plans, patient turnover and staffing, adding significantly to their workload. To address this issue, a decision was made to implement digital door signs. A project team was formed and used the DMAIC framework to integrate this technology with the EHR and in unit workflows with a strategy to operationalize. This involved a current state assessment to capture workflows and analyze tasks, followed by a gap analysis. Post-implementation analysis showed the digital door sign saves up to 60% of care team staff time per shift and reduce instances of missed signage placements by 40%. Staff surveys indicated that over 80% of members utilized the digital door sign, with more than 60% expressing satisfaction with the features and 80% approving of their future use. This successful implementation demonstrated the potential to optimize operations by saving time, reducing errors, and increasing staff satisfaction. This initiative also paves the way for utilizing this technology across Mayo Clinic enterprise locations and within the broader healthcare industry, showcasing a significant step towards modernizing hospital operations and improving overall efficiency.",1,Fulton | Renaissance Waverly Hotel,Lean Principles and Process Improvement in Healthcare Delivery,95
321,6761.0,Improving Patient Access to a Rural Autism and Developmental Medical Center,Practitioner,"The Geisinger Autism & Developmental Medicine Institute (ADMI) serves approximately 1000 new patients per year who are presenting for initial diagnostic assessment for a wide range of neurodevelopmental disabilities (NDDs) including global developmental delay, intellectual disability, and autism spectrum disorder. As with other medical centers evaluating for NDDs, ADMI has experienced excessive wait times; however, it has implemented several approaches to reduce its wait times to approximately 6-9 months. Since ADMI is in a rural area it serves a large catchment region, and many patient visits need to include both a diagnostic evaluation and a medical evaluation. Thus, all parts of new patient evaluations need to occur in one visit rather than multiple visits, thereby increasing the complexity of the schedule and expanding wait time. The typical approach to reduce wait times in any process is to obtain more bottleneck resources. However, due to the shortage of neurodevelopmental pediatricians and developmental-behavioral pediatricians, this is not possible for ADMI. Thus, ADMI modified its evaluation process for new patients to allow for clinical psychologists, speech pathologists, and advanced practice providers (physician assistants and nurse practitioners) to conduct new patient evaluations. In ADMI’s unique multidisciplinary approach, new visits are paired between clinical psychologists who provide diagnostic evaluations and advanced practice providers who provide medical evaluations, resulting in medically and behaviorally comprehensive assessments in a single visit. This multidisciplinary approach has resulted in reduced wait times for new evaluations, ADMI has a need to understand the process from a systematic viewpoint.",1,Ascot | Renaissance Waverly Hotel,Improving Healthcare Delivery through Adaptive Strategies,96
322,6562.0,Integration of a new Electronic Health Record Application to Manage the Complexities of Clinical Trials: An Industry and Academic Partnership with Lessons Learned,Practitioner,"Human-computer interaction (HCI) is essential to enhancing clinical trial management processes and ensuring patient safety by optimizing how healthcare providers engage with critical information systems. However, technology transitions present challenges that require effective training and evaluation to ensure smooth adoption and usability. To strengthen clinical research capabilities, our team implemented comprehensive workflow and process mapping strategies to develop a novel custom-built electronic medical record application, PowerTrials. Understanding how healthcare team members interact with the PowerTrials interface during training is crucial for identifying potential shortcomings and optimizing future training designs. This study aimed to improve PowerTrials training by analyzing staff perceptions, performance, and user experience through biometric data. Two pre-recorded videos on clinical trial information and PowerTrials interface navigation were presented to 17 and 27 clinical participants, respectively, from a rural hospital in the western United States. Biometric data, including eye-tracking and facial expressions, along with pop quiz scores, were collected to evaluate the PowerTrials training sessions. On average, participants directed more visual attention to key text (2,707 fixations) compared to random images (390 fixations) and focused more on central screen content (1,995 fixations) than on side content (512 fixations). Negative emotions were noted more frequently (38.6%) than positive ones (0.4%) based on emotional intensity. Pop-up windows and audible explanations effectively captured participant attention. Quiz results showed higher scores for the PowerTrials interface training (91.3/100) than for the clinical trial information training (85/100). Future research will evaluate user interaction with PowerTrials following training to further refine the interface.",2,Ascot | Renaissance Waverly Hotel,Improving Healthcare Delivery through Adaptive Strategies,96
323,5943.0,Examining Perinatal Regionalization in Practice: A Network Analysis of Maternal Transport in Georgia,Practitioner,"Perinatal regionalization is a systems-level strategy for coordinating care to ensure pregnant people receive timely care in facilities with risk-appropriate personnel and services. Inter-facility transport of patients to risk-appropriate facilities is a critical component of regionalized systems. As regionalized systems of maternal care are recently emerging, it remains unclear how maternal transport is being operationalized in practice. Using network analysis, we constructed maternal transport network to represent maternal transport routes (network edges) among obstetric facilities (network nodes) as directional network. We characterized patients and regions that were more likely to be involved in transport, and those more likely to be transported across the state’s designated perinatal regions. Network metrics were used to discern dynamics and structure of maternal transport in each perinatal region. Finally, we applied Louvain community detection algorithm to cluster obstetric facilities into communities that are observed to transport among each other most frequently. We compared these algorithm-detected communities with state’s formal designated perinatal regions. We demonstrated this data-driven approach using maternal transports as indicated on 2017-2022 birth records from the state of Georgia. We identified 8 algorithm-detected communities within each of which transports were coordinated frequently. While these detected communities were largely aligned with the state’s designated perinatal regions, geographical proximity between facilities and transporting within health systems tend to trump formal perinatal region designations. Redesigning the state’s perinatal region to align with the observed maternal transport networks and to formalize relationships that are naturally occurring in practice could potentially improve access to risk-appropriate maternal care.",3,Ascot | Renaissance Waverly Hotel,Improving Healthcare Delivery through Adaptive Strategies,96
324,9061.0,Forecasting Energy Consumption in U.S. Inpatient Healthcare Facilities,Academician,"According to the Energy Information Administration (EIA), healthcare buildings comprise 4.2% of the total commercial floorspace in the United States and are responsible for 8.6% of total energy consumption. The literature studies on energy consumption in healthcare are categorized into two main areas: one focuses on energy characterization, assessment, and energy-saving approaches, and the other area studies energy consumption prediction and analysis. Forecasting energy consumption of inpatient facilities, particularly hospitals, can support more effective budget planning and provide deeper insights into the factors influencing energy usage. This study focuses on energy consumption in inpatient facilities in the United States, which represent 56.2% of the total healthcare floor area. The first phase involves a thorough analysis of energy data in US healthcare systems, and the second phase focuses on forecasting energy consumption in inpatient facilities. This is the first study of its kind in the United States, aiming to identify the key predictors that influence energy consumption in inpatient facilities and to apply various forecasting techniques for predicting energy usage.",4,Ascot | Renaissance Waverly Hotel,Improving Healthcare Delivery through Adaptive Strategies,96
325,6013.0,Preventing rural hospital closures to ensure accessible risk-appropriate maternal care: A multi-level optimization approach incorporating patient choice behavior,Academician,"A lack of geographic access to a facility with an obstetric unit prevents some pregnant people from getting timely and risk-appropriate care. In particular, rural residents face barriers in accessing care including far travel distances for obstetric care, which is a worsening trend as many rural hospitals are closing their obstetric units. Rural hospital administrators often point to low delivery volume as a reason for closing their obstetric units, due to associated concerns about patient safety and financial viability. The government has initiated programs to prevent rural hospital closures and maintain access to high-quality obstetric care, but there are limited tools to identify where best to allocate these resources. In this work, we propose a multi-level, multi-criteria optimization model to identify which hospitals should receive subsidies to support their obstetric unit. In our multi-level framework, the government allocates subsidies to hospitals which then decide whether to close their respective obstetric units. Then, patients choose where to deliver among open obstetric facilities. The government considers multiple criteria including travel distance and the rates of deliveries in low-volume facilities when making their allocation decisions. Our model predicts delivery volume by incorporating a discrete choice model to represent patients’ hospital choice as probabilistic behavior influenced by individual and hospital factors. This multi-level model yields a mixed-integer non-linear optimization problem, which we then solve. We present a case study of the state of Georgia to demonstrate this approach to allocating subsidies to rural hospitals and the implications for the state’s maternal healthcare system.",1,Ascot | Renaissance Waverly Hotel,Optimization Strategies for Sustainable Healthcare Systems,97
326,6026.0,Designing Tertiary Cancer Services Under Burden Uncertainty,Academician,"Cancers are a growing cause of morbidity and mortality in low- and middle-income countries (LMICs). Geographic access plays a key role in both treatment and diagnosis - however, a dearth of existing facilities often means that the demand for cancer services is not well understood. Further, resources such as funding, pharmaceuticals, and skilled medical professionals are often scarce. Using Rwanda as a case study, we present a novel-path-based formulation of two stage decision problem modeling the expansion of cancer services. In this problem's first stage a decision maker must simultaneously choose a set of facilities at which to add a set of tertiary cancer services and design a transportation network to improve patient access. In the second stage, patients use the transportation network to seek care at the place facilities. We then extend this formulation to instances in which demand is unknown and present preliminary work on solving large-scale instances of this problem using Branch, Price and Cut.",2,Ascot | Renaissance Waverly Hotel,Optimization Strategies for Sustainable Healthcare Systems,97
327,6563.0,Enhancing Hospital Planning and Efficiency through Demographic Analysis and Optimization Techniques,,"This study investigates how to improve hospital design and operational efficiency by integrating demographic data with optimization techniques. Utilizing demographic attributes such as age, color, ethnicity, income, and medical requirements, the study seeks to develop a data-driven model for service design, resource allocation, and hospital site selection. The study uses Mixed Integer Programming (MIP) to find the best site placement, capacity planning, and personnel levels to meet the community's particular health needs. As an illustration, regions with high birth rates could need more maternity and pediatric facilities, whereas areas with a large senior population might prioritize departments for managing chronic diseases. Racial and ethnic demographics also influence culturally competent hiring practices and service provision, which can lessen healthcare inequities and enhance patient outcomes. Socioeconomic variables, such Medicaid dependence and income distribution, helps in allocating funds to regions with high-need or underserved populations. Through resource alignment with actual demand, this method optimizes operating costs while simultaneously improving healthcare accessibility and service quality. This approach provides a repeatable framework for building long-lasting, community-focused hospitals by integrating industrial engineering methods into healthcare planning. This study has applications for healthcare professionals and legislators who want to enhance patient satisfaction, healthcare access, and resource use in a variety of population contexts.",3,Ascot | Renaissance Waverly Hotel,Optimization Strategies for Sustainable Healthcare Systems,97
328,6619.0,Enhanced Risk Assessment for Autism With Machine Learning and Supersparse Linear Integer Modeling,Academician,"Early diagnosis is crucial for improving treatment outcomes in Autism Spectrum Disorder (ASD), a developmental disorder affecting 1 in 36 children in the US. The Modified Checklist for Autism in Toddlers (M-CHAT) is the most commonly used tool for ASD-specific screening during pediatric well-child visits. However, its effectiveness is limited due to concerns about accuracy. Although previous research has shown that machine learning models using electronic health record (EHR) data can improve ASD risk prediction, these models are challenging to integrate into primary care settings to assist decision-making. This is due to their complex computations, which make them challenging to interpret and implement, and their lack of direct incorporation of M-CHAT results. To address these limitations, we first applied machine learning models to EHR data to assess the added predictive value of various variables alongside M-CHAT. We then utilized a Supersparse Linear Integer Model (SLIM) to develop a score-based risk assessment system for straightforward computation and easy implementation. The enhanced risk assessment model demonstrated significant improvements, increasing the area under the receiver operating characteristic (ROC) curve by over 10% with the machine learning model and by 7% with the score-based model compared to M-CHAT alone. This new model complements the existing M-CHAT by simplifying the calculation process and enhancing the overall accuracy of ASD screening.",1,Ascot | Renaissance Waverly Hotel,Analytics and Modeling to Inform Health Decision-Making,98
329,6715.0,Strategic Influence of Household Attitudes on HPV Vaccination in Kenya: A Network-Based Optimization Approach,,"Human Papillomavirus (HPV) vaccination can prevent cervical cancer and is most effective when administered to adolescents before they are exposed to the virus. However, this life-saving vaccination can be poorly understood and therefore not accepted by parents and households. An effective strategy to enhance parental knowledge and willingness to vaccinate may leverage social networks to use word-of-mouth information spread to change vaccination attitudes. Such interventions may be particularly useful in resource-constrained settings where limited funds for vaccination education may be available. Using social network models to design interventions therefore is a critical step in improving health outcomes. However, existing network models often neglect the possibility of heterogeneous influence and receptiveness of individuals/households, which can be crucial to accurately model the information spread. We therefore develop a novel network-based optimization model to address this influence maximization problem. We use a directed network with a linear threshold model to capture the dynamics of attitude spread in a setting characterized by heterogeneous influence and receptiveness of individuals/households. To optimize influence, we formulate a MILP that selects a subset of households for vaccine education to maximize the number of total positive household attitudes. We demonstrate our outcomes on a network model parameterized using a unique 2022 dataset from Kenya. Our findings indicate that single-criteria selection policies often used in practice are likely insufficient to achieve efficient resource use. We also vary network parameters to explore their impact on network dynamics and optimization results.",2,Ascot | Renaissance Waverly Hotel,Analytics and Modeling to Inform Health Decision-Making,98
330,5800.0,Quantifying the Impact of Targeted Vaccine Distribution on Polio Outbreak Response Outcomes,Practitioner,"Polio, a paralytic infectious disease, remains a pressing global health issue, particularly with the rise of circulating vaccine-derived poliovirus (cVDPV) and recurrent outbreaks in regions with under-immunized children. Prior research has indicated that prioritizing vaccination for under-immunized children during outbreak responses can significantly reduce the incidence of paralytic cases and curtail outbreaks with fewer vaccination rounds compared to a generalized approach targeting children regardless of immunity levels. However, individuals’ immunity levels are challenging to observe in practice, which presents a challenge for prioritizing vaccine allocation. Instead, prioritizing vaccines based on vaccination history, while an imperfect measure of immunity, may be a practical and promising strategy. In this study, we systematically assess the effects of vaccine distribution strategies based on children’s vaccination histories on polio outbreak outcomes. Using a compartmental simulation model, we project poliovirus transmission dynamics across varying scenarios, including distinct vaccine allocation schemes by vaccination history, vaccination campaign coverage (i.e., proportion of the target population planned for vaccination), and vaccination campaign timeliness (i.e., time elapsed between outbreak detection and vaccination initiation). We will compare outcomes in terms of (i) outbreak size (i.e., cumulative paralytic cases within the prediction period), (ii) time to outbreak resolution, and (iii) total vaccine doses required to achieve containment. Additionally, we evaluate the potential added value of using vaccination history as a guiding metric for vaccine allocation, relative to an ideal scenario where immunity levels are assumed to be known.",3,Ascot | Renaissance Waverly Hotel,Analytics and Modeling to Inform Health Decision-Making,98
331,6390.0,Public Health Surveillance and Resource Allocation via Adaptive Sampling,,"Effective public health surveillance is essential for evidence-based decision-making, yet identifying optimal data collection sites remains challenging due to the broad scope of human activities and limited resources. Inefficient surveillance can lead to inaccurate insights on disease progression, potentially impacting critical public health policies, such as vaccine distribution and testing kit allocation. To address these challenges, we developed a sequential resource allocation framework that leverages limited samples to learn health event trends, detect potential outbreaks, and guide subsequent sampling to areas of greatest predictive uncertainty. Our results demonstrate that this framework significantly outperforms established benchmarks, offering a robust approach to enhancing disease monitoring and response.",4,Ascot | Renaissance Waverly Hotel,Analytics and Modeling to Inform Health Decision-Making,98
332,5768.0,Real-time control of proton therapy delivery using two-stage stochastic programming,Academician,"As an advanced radiation treatment, proton therapy is a highly targeted treatment for tumors and can minimize damage to surrounding normal tissues. Its dose-depth curve is helpful for improving patient treatment outcomes, making proton therapy a preferred choice for patients. A typical proton therapy facility consists of one proton accelerator and multiple gantries, and at any time the proton accelerator can only serve one gantry. Such a resource sharing feature of the proton therapy delivery makes patients wait on treatment couches of gantries. Long waiting time compromises treatment precision, causes patient muscle injuries, and reduces the delivery efficiency. To address this challenge, we propose a two-stage stochastic programming model for real-time control of proton therapy delivery. The model considers uncertainties of durations in each step of the proton therapy delivery and optimizes patient waiting times and resource utilization. The two-stage stochastic programming model is capable of managing uncertainties, providing more flexible and effective control solutions than the traditional first-come-first-served policy. Numerical experiments show improved performance of the proposed method. Integrating two-stage stochastic programming into the proton therapy delivery control provides substantial enhancements for current proton therapy systems in terms of both care quality and system efficiency.",1,Ascot | Renaissance Waverly Hotel,Modeling and Simulation in Critical Care Delivery,99
333,6910.0,Informing ICU Digital Twins: Dynamic Assessment of Cardiorespiratory Failure Trajectories in Patients with Sepsis,Practitioner,"Understanding clinical trajectories of sepsis patients is crucial for prognostication, resource planning, and to inform digital twin models of critical illness. This study aims to identify common clinical trajectories based on dynamic assessment of cardiorespiratory support using a validated electronic health record data that covers retrospective cohort of 19,177 patients with sepsis admitted to ICUs of Mayo Clinic Hospitals over eight-year period. Patient trajectories were modeled from ICU admission up to 14 days using an unsupervised machine learning two-stage clustering method based on cardiorespiratory support in ICU and hospital discharge status. Of 19,177 patients, 42% were female with a median age of 65 (IQR, 55-76) years, APACHE III score of 70 (IQR, 56-87), hospital length of stay (LOS) of 7 (IQR, 4-12) days, and ICU LOS of 2 (IQR, 1-4) days. Four distinct trajectories were identified: fast recovery (27% with a mortality rate of 3.5% and median hospital LOS of 3 (IQR, 2-15) days), slow recovery (62% with a mortality rate of 3.6% and hospital LOS of 8 (IQR, 6-13) days), fast decline (4% with a mortality rate of 99.7% and hospital LOS of 1 (IQR, 0-1) day), and delayed decline (7% with a mortality rate of 97.9% and hospital LOS of 5 (IQR, 3-8) days). Distinct trajectories remained robust and were distinguished by Charlston comorbidity index, Apache III scores, day 1 and day 3 SOFA (p<0.001 ANOVA). These findings provide a foundation for developing prediction models and digital twin decision support tools, improving both shared decision-making and resource planning.",2,Ascot | Renaissance Waverly Hotel,Modeling and Simulation in Critical Care Delivery,99
334,8729.0,A Hybrid Simulation Approach For Modeling Critical Care Delivery in ICU,Academician,"Critical care delivery entails a complex human-centric system. Patients and a multidisciplinary care team are the major autonomous agents in the system. Their actions and interactions with each other and the environment drive the dynamic evolution of the system and determine the system outcomes (e.g., patient outcome, provider burnout, care quality, system efficiency). The objective of this study was to model critical care delivery in an ICU to provide decision support to ICU resource management. A hybrid simulation approach combining agent-based simulation for modeling patients and care providers, and discrete event simulation for modeling care pathways was developed. This approach leverages clinical knowledge for modeling individual patient trajectories and care services endogenized from patient needs. It allows us to understand how the arrival flow of patients, the patient disease condition, and the care protocols jointly affect ICU census and provider workload, and build a pathway towards digital twinning of ICUs.",3,Ascot | Renaissance Waverly Hotel,Modeling and Simulation in Critical Care Delivery,99
335,5719.0,A Data-Driven Graph Model Approach to ICU Patient Trajectory Prediction,,"Digital medicine, driven by the increasing adoption of Electronic Health Records (EHRs), enables comprehensive longitudinal tracking of patient health indicators. These indicators are routinely measured and collected by various devices to monitor patient health status, offering valuable insights for clinical decision-making. This project focuses on estimating the correlations and causal relationships among key health indicators in order to enhance patient trajectory prediction, ultimately improving patient care outcomes. We developed a comprehensive analysis pipeline that leverages EHR data to investigate trends and causal relationships among vital health indicators across patient populations. By analyzing the temporal patterns in vital health indicators, we identified distinct patient clusters with similar health trajectories. For each cluster, we implemented a structural equation model (SEM) to capture both intra-time and inter-time dependencies among variables, providing a deeper understanding of the dynamic relationships within each group. To estimate the parameters within these models, we applied an optimization-based method to compute the autoregressive matrices, encapsulating the intra-time and inter-time dependencies. Additionally, we utilized Directed Acyclic Graphs (DAGs) to visualize the causal and correlative relationships among the indicators, offering an intuitive representation of the interactions between vital health metrics. This integrated approach of clustering, SEM, matrix estimation, and DAG visualization enables more accurate modeling of patient health trajectories, yielding predictive insights that are tailored to specific patient cohorts. This framework has the potential to guide more personalized and effective interventions, ultimately improving patient care outcomes.",4,Ascot | Renaissance Waverly Hotel,Modeling and Simulation in Critical Care Delivery,99
336,5922.0,The Development of a Shortages Prediction Model for Drug Supply Chains,,"This research develops an initial data-driven framework for prediction of shortages in drug supply chains. It starts with acquiring publicly available drug shortages data. Pre-processing steps include dimensionality reduction, label encoding and text pre-processing. Additionally, feature engineering is applied to the temporal features to capture seasonal trends. The dataset is expanded by bootstrapping. The predictive model is refined through hyperparameter tuning, feature selection and time-series cross-validation. The performance is evaluated using several accuracy metrics such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), r-squared (r 2 ), and explained variance. A user interface is developed to allow medical practitioners to quickly search the availability of a particular drug. The results from this initial model underscore the importance of the availability of more extensive historical shortage data through reporting, larger sample sizes and diverse features for better prediction of drug shortages.",1,Ascot | Renaissance Waverly Hotel,AI and Data Analytics for Healthcare,100
337,6282.0,Evaluating the Feasibility and Accuracy of Large Language Models for Medical History-Taking in Obstetrics and Gynecology,Practitioner,"In pre-diagnostic settings, effective doctor-patient communication is crucial for gathering essential information, while time-consuming and privacy-sensitive for some health issues. Large Language Models (LLMs) have shown potential in supporting these conversations by streamlining repetitive tasks and enhancing patient comfort. By utilizing ChatGPT-4o's advanced audio conversational capabilities, this study aims to evaluate its feasibility, effectiveness and accuracy in medical history-taking interactions in obstetrics and gynecology. ChatGPT-4o were set to play both the patients and doctors using 100 cases, simulating the daily doctor-patient communication and generating medical records. The feasibility of using AI-agent for pre-diagnostic medical history-taking was assessed. We expect the AI-driven conversation to yield a complete and coherent medical record, capturing essential information with minimal human intervention. Full experiments are under progress. However, preliminary results are promising based on the available data collected. The doctor AI agent performed nearly the same as a realistic physician, asking in a professional and logical sequence and engaging patients with a comfortable and respectful tone. The final assessment will include an evaluation of logic, readability, and empathy in the LLMs-generated records and practitioners’ evaluation of accuracy. Preliminary tests indicate that ChatGPT-4o can produce coherent and structured medical records, though adjustments may be needed to improve accuracy in capturing nuanced medical details. Implementing LLM for history-taking may reduce physicians' workload by automating routine tasks, while enhancing patient comfort in sharing private information. ChatGPT-4o 's effectiveness in automating medical history-taking is evaluated, with potential for improving efficiency and data accuracy in clinical settings.",2,Ascot | Renaissance Waverly Hotel,AI and Data Analytics for Healthcare,100
338,6828.0,Advancing Surgical Care: Unsupervised Neural Network Classification of Surgery Urgency with Transfer Learning,Academician,"Timely and precise classification of surgical urgency is crucial for optimal resource allocation in healthcare; this study leverages unsupervised neural networks and transfer learning techniques to innovate urgency classification, ensuring efficient patient management without the need for predefined data labels. This research aims to develop an unsupervised neural network model for classifying surgical procedures based on urgency levels (Immediate, Urgent, or Elective) through sentiment analysis of clinical notes. Initially, the dataset undergoes advanced preprocessing, including text normalization, tokenization, lemmatization, and n-gram extraction to enhance feature quality. Using the BioBERT model, surgery-related transcriptions are then embedded to capture their semantic nuances. To address dataset imbalance, techniques like SMOTE are employed before applying dimensionality reduction using UMAP. The study explores various clustering algorithms (K-Means, GMM, DBSCAN, Agglomerative, and Ensemble) and employs Deep Embedded Clustering (DEC) which integrates an autoencoder with a clustering layer for grouping surgeries by their contextual similarity. Domain Expert Validation ensures the reliability of the labels generated through unsupervised learning. Then the labeled data will use in neural network model to classify the urgency level of the unseen data. Performance of the clustering is measured using the Silhouette Score, Davies-Bouldin Index, and Calinski-Harabasz Index, ensuring robust validation of the clustering quality and model effectiveness.",3,Ascot | Renaissance Waverly Hotel,AI and Data Analytics for Healthcare,100
339,7017.0,Navigating Uncertainties in Predicting Catastrophic Health Expenditure:  A Conformal Prediction Approach,Academician,"Catastrophic health expenditure (CHE) refers to situations where a family's medical costs are so high that they cause financial hardship or require significant sacrifices in essential areas such as food and housing. Health economics offers two main methods for estimating CHE: budget share (BS) and capacity-to-pay (CTP). This research employs the budget share approach using data from the Medical Expenditure Panel Survey (MEPS) to investigate CHE among US households, focusing on thresholds of 10%, 15%, and 20%. Various tools like the concentration index (CI), Wagstaff index (WI), and Erreygers Index (EI) are utilized to assess socioeconomic disparities in CHE. Additionally, five machine learning techniques, including random forest (RF), support vector machine (SVM), Artificial neural network (ANN), Deep learning (DL), and XGboost (XG) are applied to predict the risk of CHE in US households and identify key factors affecting CHE. Conformal prediction methods are then used to evaluate the uncertainties in the model's predictions and determine the best prediction intervals and alpha (α) values. The purpose of this work is to develop robust prediction models for identifying households at high risk of CHE and explore the uncertainties surrounding these predictions. This approach aims to provide more reliable insights for policymakers to implement targeted interventions that reduce the financial burden on vulnerable populations. Results indicate that household CHE rates are increasing with decreasing thresholds. Moreover, CHE is found to be more prevalent among households with low socioeconomic status (SES).",1,Fulton | Renaissance Waverly Hotel,Advanced Analytical Approaches in Health Risk and Diagnostic Modeling,101
340,6774.0,Enhancing Patient Safety through Predictive Analytics: A Framework for Risk Stratification and Management in Healthcare Systems,Academician,"Ensuring patient safety is fundamental to delivering high-quality healthcare, as adverse events and clinical complications impose significant operational and economic burdens. This study proposes a predictive analytics framework integrating machine learning and systems engineering principles to enable the early identification and proactive management of patient safety risks. Leveraging machine learning algorithms and systems approaches, the framework enhances detection accuracy and risk stratification, facilitating targeted interventions aimed at both prevention and individualized care. The results highlight the efficacy of predictive analytics in reducing the incidence of complications, optimizing resource utilization, and ultimately improving patient outcomes, establishing it as a critical tool for enhancing resilience and quality in healthcare delivery. Future research will focus on increasing model interpretability and conducting validation studies across varied clinical environments to support implementation.",2,Fulton | Renaissance Waverly Hotel,Advanced Analytical Approaches in Health Risk and Diagnostic Modeling,101
341,8998.0,Real Time Control of Blood Glucose Level For Type 1 Diabetes Patients By Predicting Insulin Infusion Amount Using Machine Learning Algorithms,Academician,"Diabetes Mellitus (DM) arises from irregular insulin production on the endocrine pancreas, with Blood Glucose (BG) levels controlled by insulin, and glucagon, which increases it. In clinical practice, an artificial pancreas system helps manage BG by administering these hormones as required. An implementation of adaptive Machine Learning (ML) algorithm can automatically and continuously adjust insulin delivery based on feedback from sensor-detected glucose concentration. The insulin infusion is directed by the Artificial Intelligence (AI) algorithm hosted on an infusion pump, which receives and processes continuous glucose monitoring data. This paper presents a comparative analysis of ML algorithms for adjusting the blood glucose level in Type 1 diabetes (T1D) patients’ post-meal by regulating insulin infusion. Several regression-based ML algorithms have been applied to a dataset enriched with continuous glucose monitoring data for successfully predicting the insulin infusion amount for T1D patients. This paper focuses on developing adaptive and data-driven solutions to refine insulin delivery and improve diabetes management. The proposed predictive analysis can help to reduce chronic conditions such as hypoglycemia or hyperglycemia in T1D patients.",1,Fulton | Renaissance Waverly Hotel,Machine Learning Applications for Intelligent Healthcare,102
342,8882.0,Machine Learning Approaches for Predicting Dental Caries in Permanent Molars of Children and Adolescents Using NHANES 2011-16 Data,Academician,"Dental caries is a widespread chronic disease impacting children's quality of life, education, and school attendance. Between 2011 and 2016, caries affected 17.4% of children aged 6–11 and 56.8% of adolescents aged 12–19, with disproportionate prevalence among non-Hispanic Black, Mexican American, and lower-income youth. This study aims to develop machine learning models to predict the presence and severity of decayed, missing, or filled permanent molars (DMFT) using data from the National Health and Nutrition Examination Survey (NHANES) from 2011 to 2016. In this research, data from three NHANES cycles (2011–12, 2013–14, and 2015–16) for individuals aged 6–19 were merged, incorporating demographic, dietary, oral health, and insurance data. Two models were developed: (1) a classification model for binary DMFT outcomes (DMFT = 0 or DMFT > 0) and (2) a regression model predicting the number of affected teeth. Algorithms including XGBoost, random forest, LightGBM, and ensemble techniques were applied, with predictors such as income-to-poverty ratio, age, dietary intake, parental education, and race/ethnicity. Preliminary analysis identified significant predictors of dental caries, including income-to-poverty ratio, age, screen time, insurance type, dietary sugar and carbohydrate intake, parental education levels, and race/ethnicity. The models' efficacy was assessed using metrics such as accuracy and area under the ROC curve. Detailed comparisons of model performances will be presented to identify the most effective methods for DMFT prediction. For future work, we aim to implement a hybrid modeling approach where clustering and classification will be applied sequentially to improve the model's predictive performance.",2,Fulton | Renaissance Waverly Hotel,Machine Learning Applications for Intelligent Healthcare,102
343,8898.0,Deep Learning Approaches for Monitoring Fetal Acidosis,Academician,"Lack of oxygen being shared with a fetus (Intrapartum asphyxiation) can lead to fetal acidosis, a life-threating condition that can cause organ complications and brain injury in neonates. Detection of acidosis early is done using cardiotocography (CTG); medical practitioners assess key features in CTG signals to evaluate fetal health. Despite standardized CTG interpretation guidelines, high false-positive and false-negative rates have prompted efforts to make CTG interpretation more objective. Deep learning can offer a promising solution by automating the process of feature extraction and acidosis classification. This study investigates using convolutional neural networks and attention mechanisms to enhance acidosis prediction by identifying relevant features. Traditional models based on raw Fetal Heart Rate and Uterine Contraction signals were less effective, while models incorporating waveform mixing, CNNs, and attention mechanisms significantly improved precision, recall, and F1-scores. Spectral mixture models trained with Gaussian noise showed exceptional classification metrics, capturing complex patterns associated with acidosis.",3,Fulton | Renaissance Waverly Hotel,Machine Learning Applications for Intelligent Healthcare,102
344,8546.0,Augmenting Individualized Treatment Planning via Data-Driven Clinical Role Model Selection,Academician,"Type 2 Diabetes (T2D) and Atherosclerotic Cardiovascular Disease (ASCVD) comprise a significant portion of all chronic disease in the United States. Existing clinical guidelines provide recommendations for managing T2D and ASCVD. However, most guidelines focus on each condition separately and there is little guidance for managing the two chronic diseases jointly. Further, many population-based guidelines are one-size-fits-most; consequently, individual differences in risk and treatment response are unaccounted for in these guidelines which may inadvertently widen existing health disparities. This talk will present the use of operations research methodologies to design responsible Artificial Intelligence (AI) technologies for the joint management of T2D and ASCVD. In particular, we study the problem of generating clinical role models whose physiological measurements can serve as personalized treatment targets for patients with T2D who are at high risk of ASCVD. Our goal is to design a framework that is interpretable for clinicians and robust to data shifts. We formulate the problem as a data-driven robust recourse optimization problem and derive a tractable reformulation of the problem by exploiting its structural properties. We also design and analyze an active learning algorithm to solve the problem efficiently. Our computational analyses using these models demonstrate the potential impact in patients’ health outcomes by adopting these technologies, as well as their health equity implications for diabetes care.",1,Fulton | Renaissance Waverly Hotel,Data-Driven Decision Support for Healthcare,103
345,6058.0,A Data Driven Optimization Approach for Personalized Baseline Testing in Concussion,Academician,"Concussions, the most prevalent form of traumatic brain injuries, pose a significant public health concern, contributing to over 4 million sports and recreation-related incidents annually. Baseline testing has emerged as a promising tool for diagnosing concussions. However, in low-resource settings, baseline tests are often allocated to high-impact sports like football, disregarding other demographics. Recent research has shown that the diagnostic benefits of baseline testing are heterogeneous across athletes, with benefits varying by individual and by type of assessment. This underscores the need for personalized design of baseline testing, especially in low-resource settings. This work proposes a data-driven approach to optimally personalize baseline tests based on athlete demographics and medical history. Our model generates an ordered list of baseline tests tailored to these sensitive features. We formulate the problem as a sequential knapsack problem, producing a prioritized list of baseline tests in decreasing order of added value. Additionally, our model incorporates sensitive features using a decision tree, where splits at each node are based on the output from the sequential knapsack. Our results demonstrate that using this ordered list instead of the full battery maintains diagnostic accuracy while significantly reducing the resources required for baseline testing. This approach is applicable not only to athletes who may not benefit from standard baseline testing but also in resource-constrained settings where the cost of comprehensive baseline testing is prohibitive.",2,Fulton | Renaissance Waverly Hotel,Data-Driven Decision Support for Healthcare,103
346,6141.0,Social Determinants of Health & Congestive Heart Failure Using Electronic Health Records Data.,Academician,"The progression and management of congestive heart failure (CHF), like other chronic diseases, are influenced by lifestyle choices shaped by Social Determinants of Health (SDOH). Given the identified impact of SDOH on health outcomes, health institutions now incorporate SDOH data into electronic health records (EHR) through evidence-based questionnaires targeting social domains linked to diverse social needs. This study aims to outline the method used to investigate the relationship between various SDOH and CHF using inpatient data from the Epic EHR system at Our Lady of the Lake Regional Medical Center (OLOL RMC). Patients will be categorized into CHF and non-CHF groups, with CHF patients identified using the Medicare Severity Diagnosis Related Groups (MS-DRGs) codes 291, 292, and 293. The methodology will also outline the steps taken to overcome the challenge of choosing the best set of control group, considering geographic location of care and presence of comorbidities. This is because some conditions might be overrepresented due to OLOL specializations and the nature of chronic diseases. After de-identifying patient data (removing identifiers to patients' identity), the relevant information such as demographics, patient encounters, and identified SDOH factors will be exported to a spreadsheet for analysis using Chi-square and Random Forest algorithm. A heatmap with dendrograms will also be used to visualize the relationship between CHF patients and SDOH factors. The use of diverse analytical tools in the study will help to reduce potential errors, and enhance the generalizability of findings. The study will expose SDOH connections to CHF at OLOL.",3,Fulton | Renaissance Waverly Hotel,Data-Driven Decision Support for Healthcare,103
347,4935.0,Optimizing Campus Health and Wellbeing Services to Meet Rising Demand Within Fixed Cost Constraints,Practitioner,"Campus health and wellbeing centers at growing universities face the common challenge of meeting the increasing healthcare needs of students with limited resources. Expanding building space is a difficult and long-term solution due to high costs and administrative constraints, while hiring more healthcare providers is an expensive and unsustainable option. This research developed an optimization model that assesses the feasibility and effectiveness of various strategies to expand a health center's capacity to provide care without increasing fixed costs. The capacity-expanding strategies considered include shifting eligible work from providers to other care providers, such as registered nurses or nurse practitioners, implementing telehealth services, utilizing self-testing options, establishing nurse clinics near dormitories, and deploying vending machines for over-the-counter medications. Each strategy was assessed based on multiple criteria, including estimated implementation and maintenance costs, forecasted utilization of services, patient satisfaction, and staff satisfaction. The optimization model developed in this research provides a flexible framework that any campus health and wellbeing center can use to make data-driven decisions tailored to its unique needs. The findings aim to offer actionable insights for expanding capacity while maintaining high standards of care, ensuring that limited resources are utilized effectively without sacrificing access to services. By addressing the growing demands of an expanding student population, this research lays the groundwork for informed decision-making in the future of campus healthcare systems.",1,Fulton | Renaissance Waverly Hotel,Innovative Approaches to Healthcare Personnel Scheduling,104
348,6474.0,Mitigating Staffing Strain in Emergency Departments through Clinician-Engaged Modeling,,"Although emergency departments (EDs) serve as our country’s healthcare safety net, ED staffing is chronically strained. Understaffing delays patient care and negatively impacts patient experiences and outcomes. To evaluate how strains may propagate through a network of emergency departments and affect patients, we develop a discrete-event simulation model of the Prisma Health-Upstate ED network. Model development occurred via a participatory research process between the clinical and engineering team. The model is calibrated to real-world patient flow data (EPIC) and clinician staffing levels (ShiftAdmin). We use this model to quantify the relationship between clinician absenteeism on patient Length of Stay (LOS), a surrogate measure for quality of care. We discuss the key results as well as the model development process and the potential impact of clinical-industrial engineering collaborations.",2,Fulton | Renaissance Waverly Hotel,Innovative Approaches to Healthcare Personnel Scheduling,104
349,6558.0,Rural Critical Access Hospital: Scheduling Prediction Challenges in Isolated Frontier Areas with Large Seasonality Factors,Practitioner,"Rural communities are served by critical access hospitals (CAHs) across the United States. However, many struggle with staffing, schedule prediction and large seasonality influences due to the service and agricultural industries. Moreover, these smaller communities struggle recruiting and retaining staff from nursing assistants to surgeons. Yet these facilities serve as the backbone of the US healthcare system by providing critical emergency, inpatient, outpatient and transport services to areas with as few persons as 6 persons per square mile. In Montana, there are ~50 CAHs that serve 147,040 square miles which is the 3 rd least population dense state of about 1.1 million people. Akin to Alaska, Montana faces significant weather, terrain, communication and many other barriers that limit healthcare access especially for surgical and specialty services. It is common for communities to experience significant changes in population density (tourism) and shortages in all types of healthcare personnel due to the cost of living. A 2-week Kaizen rapid improvement project was conducted with a single Montana CAH to understand root cause issues regarding staff scheduling, recruitment and retention, financial viability of additional permanent staff, and the impacts of a resort community with extreme seasonal shifts in peak summer and winter months. This study evaluated the current staff scheduling practices, reviewed past annual scheduling, financial analyses of the service area, and a staff input survey to understand challenges and barriers. This research demonstrates how CAHs can integrate this vast information into tangible and generalization methods for all CAHs to better predict scheduling.",3,Fulton | Renaissance Waverly Hotel,Innovative Approaches to Healthcare Personnel Scheduling,104
350,8624.0,Optimizing CT Modality Scheduling: An Approach to Improve Resource Utilization and Staff Satisfaction,,"Effective staffing in healthcare is critical, where demand fluctuations challenge resource optimization. This project focuses on developing an optimal staffing schedule for the Computed Tomography (CT) modality in the Medical Imaging Department at a healthcare institution in Maryland. It also demonstrates the value of industrial engineering in healthcare by highlighting a data-driven approach to improve staffing efficiency and enhance patient care in Medical Imaging. Initially, scheduling and staffing decisions were made in real-time without a pre-defined schedule, which led to increased reliance on costly temporary staff (travelers). This study uses the Plan-Do-Check-Act (PDCA) framework to determine the Full-Time Equivalent (FTE) requirements to meet demand variations by hour and day. In the ""Plan"" phase, initial staffing levels, working shift preferences per FTE, and demand constraints were analyzed to identify optimal staffing needs. In the ""Do"" phase a structured Linear Programming (LP) model was designed to meet these constraints. The percentage of total hours of staff shortage was evaluated in the ""Check"" phase, and compared with the initial percentage - before the interventions were done. And finally, the ""Act"" phase focused on refining and standardizing the scheduling process, ensuring sustainability and adaptability to changing demand.",4,Fulton | Renaissance Waverly Hotel,Innovative Approaches to Healthcare Personnel Scheduling,104
351,4901.0,The Impact of Fatigue on Clinical Decision-Making Accuracy in Healthcare Professionals,Practitioner,"Fatigue among health professionals dramatically affects decision-making accuracy—a critical threat to patient safety. This literature review will discuss how fatigue influences clinical decision-making for safety-critical roles, structured based on three objectives: understanding the relationship between fatigue levels and accuracy in decisions, identifying the key factors contributing to healthcare professional fatigue, and assessing the effectiveness of the currently applied fatigue mitigation strategies in error reduction. It does a systematic review of various studies showing that cognitive impairment in the form of reduced vigilance and slower response times can adversely affect clinical judgment owing to fatigue. Long shifts, heavy workloads, and stressful work environments are all facilitative factors employed in discussing how they help enhance fatigue among healthcare workers. It concludes by reviewing interventions, such as improved shift scheduling and specific programs focused on fatigue management, to outline how these might help reduce fatigue's negative impact on decision-making. This review provides evidence-based insights to guide policies and practices toward bettering decision-making accuracy and improving patient safety in healthcare by tackling the prevalent problem of fatigue.",1,Fulton | Renaissance Waverly Hotel,Advancing Public Health Systems through Engineering Innovation,105
352,5612.0,Leveraging Lean Six Sigma for Equitable Vaccine Distribution: A Literature Review and Future Directions,Academician,"The equitable distribution of vaccines in under-resourced and at-risk communities remains a critical challenge, especially during public health crises and outbreaks. In such situations, the demand for timely and efficient vaccine deployment increases and this, in turn, escalates existing disparities in healthcare access. Lean Six Sigma (LSS) methodologies offer a promising framework to optimize vaccine distribution networks, which ensure timely and effective delivery while minimizing waste and inefficiencies. This paper presents a preliminary literature review of current LSS applications in healthcare, with a specific focus on vaccine distribution. The review identifies key challenges in ensuring equity in healthcare access, including but not limited to logistical bottlenecks, resource allocation, and community engagement. Furthermore, it explores successful case studies where LSS tools, such as DMAIC (Define, Measure, Analyze, Improve, Control) and process mapping, have been utilized to enhance distribution networks. Through synthesizing these findings, the review identifies significant gaps in the current literature, particularly in addressing marginalized populations and incorporating social determinants of health within Lean Six Sigma (LSS) frameworks. Additionally, it proposes potential research directions to overcome these limitations and improve healthcare delivery for underserved communities. The insights gained from this review will inform ongoing research with UHS hospitals, which aims to develop data-driven, equitable vaccine distribution models tailored to vulnerable populations.",2,Fulton | Renaissance Waverly Hotel,Advancing Public Health Systems through Engineering Innovation,105
353,6667.0,The Effect of Scale of Technology Updates on Workflow Disruptions: A Scoping Review,Practitioner,"In healthcare, technology upgrades can lead to varying degrees of workflow disruptions that require workers to adapt their practices. These upgrades range from minor updates such as software patches to major transitions like the adoption of electronic health records. While substantial changes have been the primary focus of prior research, minor yet more frequent changes are often overlooked despite their potential to disrupt workflows and reduce efficiency. This scoping review synthesizes existing literature to examine the impact of technological change across different scales, including major, moderate, and minor, on clinical efficiency, workflow disruptions, and the resulting adaptation challenges. We introduce a categorization framework to systematically differentiate these levels of change and their respective impacts. The analysis integrates quantitative metrics such as time delays, error rates, and productivity shifts along with qualitative accounts of staff adaptation strategies. The review aims to identify patterns in workflow disruption and adaptive responses while highlighting gaps in existing research. Our findings will inform systems-based strategies to design and implement technology updates with minimal disruption to clinical workflows and improved adaptability in healthcare settings.",3,Fulton | Renaissance Waverly Hotel,Advancing Public Health Systems through Engineering Innovation,105
354,5273.0,Evaluating CYBHI Fee Schedule Program Implementation as a Solution for Fee-Funded Health Center Costs in the CSU System,Academician,"Healthcare costs continue to rise annually, leaving health centers underfunded and unable to keep up with growing service demands. Particularly, fee-funded health centers, such as those servicing higher-education, face challenges balancing high service demands with increasing operational costs. In these cases, students often pay monthly premiums for their own health insurance while also paying their campus’ health services fee. However, when students use their campus health center, these centers provide the services, allowing insurance providers to retain the premiums without contributing to the actual care. In 2022, Governor Newsom announced the Master Plan for Kids’ Mental Health, a commitment aiming to expand mental health support for Californians under the age of 25. The core of this plan is the Children and Youth Behavioral Health Initiative (CYBHI), which established the statewide multi-payer fee schedule. This program allows health centers to be reimbursed by Medicaid Managed Care Plans for the mental health services they provide. Since health centers often have to self-advocate for funding, a system to hold insurance companies accountable for owed funds is crucial. Campus health centers are then able to create a stable revenue stream to offset the increasing costs and reduce their reliance on student health services fees. The aim of this project was to evaluate the feasibility of implementing the CYBHI Fee Schedule program within the CSU system, focusing on cost reduction, program effectiveness, and infrastructure capabilities. A financial analysis was conducted comparing current costs and revenue structures with projected financial outcomes after implementation.",4,Fulton | Renaissance Waverly Hotel,Advancing Public Health Systems through Engineering Innovation,105
355,6988.0,Bridging Healthcare Access Gaps: Assessing Telehealth Readiness and Barriers,,"The expansion of telehealth services presents a transformative shift in healthcare accessibility, especially in regions facing socio-economic challenges like Puerto Rico. This study aims to assess the readiness of Puerto Ricans to adopt telehealth, focusing on understanding the population’s attitudes, preparedness, and the barriers impacting technology acceptance in healthcare. Employing the Technology Readiness Index (TRI) as a framework, this research evaluates the preparedness of Puerto Ricans for telehealth adoption by exploring the perspectives of various demographic, cultural, and regional segments. The survey captures key factors influencing acceptance, including perceived ease of use, anticipated benefits, and household access to digital tools. Beyond the TRI’s core dimensions—optimism, innovativeness, discomfort, and insecurity—this study also investigates familiarity with common telehealth practices to build a comprehensive profile of community readiness. By analyzing these factors, this research seeks to illuminate both the opportunities and obstacles for telehealth implementation in Puerto Rico, offering critical insights for healthcare providers and policymakers. The findings will support the design of strategies aimed at enhancing telehealth accessibility, bridging healthcare gaps, and promoting equitable healthcare delivery. This study aspires to contribute to the sustainable integration of telehealth, establishing a digital healthcare landscape that is accessible, effective, and aligned with the needs of Puerto Rican communities.",1,Fulton | Renaissance Waverly Hotel,Approaches to Public Health Access and Risk Management,106
356,7073.0,Segmentation and Classification of Traumatic Brain Injury amongst Industrial Workers,Practitioner,"Traumatic Brain Injury (TBI) is one of the major occupational health concerns among industrial workers since they work in high-risk environments. The impact of TBI extends beyond individual health; it also affects workplace productivity and overall safety, but, with timely interventions, the survivability of traumatic brain injuries (TBI) and the quality of life after treatment can significantly be improved. With the advancements in imaging technology today, there is still the issue of significant hurdles associated with accurate segmentation with its array of symptoms, varying levels of severity, and recovery paths. Challenges also arise from the lack of access to datasets for workers, the presence of imaging artifacts, and the difficulty in applying segmentation and classification techniques across different settings, with subtle injury patterns posing classification challenges, while the limited integration of machine learning tools in clinical practices for early TBI detection remains an obstacle. To address such issues, a machine learning-based framework was developed for segmenting and classifying lesions in TBI imaging using a modified U-Net framework. This method uses magnetic resonance imaging (MRI) data from a cohort diagnosed with TBIs to enhance the accuracy of diagnosis and subsequently enable early intervention. The effectiveness of the models was evaluated using standard metrics: accuracy, sensitivity, and specificity; also, the Dice score was applied to assess the accuracy of segmentation. This research shows the promise of imaging and methods in the classification and diagnosis of TBI cases among industrial workers to effectively enhance health outcomes and workplace safety measures.",2,Fulton | Renaissance Waverly Hotel,Approaches to Public Health Access and Risk Management,106
357,8599.0,Forecasting Seasonal Influenza Patterns and Trends Using Historical Data and Time Series Analysis,Academician,"Abstract Effective forecasting of seasonal influenza patterns is crucial for public health preparedness, as influenza remains a recurrent threat with considerable morbidity and mortality, especially among vulnerable populations. The virus's high mutation rate and the emergence of new strains complicate accurate prediction of outbreak timing and severity, despite ongoing surveillance efforts. Early detection and robust influenza surveillance are essential for equipping healthcare systems to respond and for alleviating the seasonal strain on resources. This study leverages historical data and time series analysis to identify patterns in influenza spread and predict outbreak peaks. Key findings reveal that incorporating multivariate and ensemble forecasting models—such as autoregressive integrated moving average (ARIMA), Bayesian inference, and machine learning approaches—substantially improves prediction accuracy. Notably, combining epidemiological data with search engine trends and environmental variables yields a more comprehensive and real-time forecast model, enabling more accurate and timely predictions. These methods also capture regional variations and can adapt to emerging trends, increasing the robustness of outbreak preparedness. The significance of this research lies in its potential to enhance public health response by enabling early detection and resource allocation, improving vaccination timing, and ultimately reducing influenza-related morbidity and mortality. With influenza remaining a persistent global threat, this study’s approach underscores the importance of integrating advanced predictive tools and real-time data in epidemiological forecasting, providing a foundation for more effective health intervention strategies.",3,Fulton | Renaissance Waverly Hotel,Approaches to Public Health Access and Risk Management,106
359,8487.0,A Novel AI-Driven Markov Chain Approach to Dementia Detection,Practitioner,"Dementia, one of the most prevalent and devastating neurodegenerative diseases, affects millions worldwide. Identifying early markers of dementia is crucial for timely intervention and management. Current non-invasive diagnostic methods, including the Montreal Cognitive Assessment (MoCA) and the Saint Louis University Mental Status (SLUMS), are paper-and-pencil-based, manually interpreted, and time-consuming. This paper introduces a first-of-its-kind, interpretable artificial intelligence (IAI) approach that utilizes first-order Markov Chain models to analyze linguistic patterns for detecting early-stage dementia. By computing steady-state probabilities for each character in speech transcripts from both dementia subjects and healthy controls, our analysis revealed that the space character “ ”, representing pauses and voice breaks, and specific letters such as “n” and “i”, exhibited statistically significant differences between the groups. Principal Component Analysis (PCA) visualization demonstrated clear clustering of the two groups based on these features, indicating effective differentiation by our method. These linguistic markers were validated through Kolmogorov-Smirnov (KS) testing, confirming distinct probability distributions between dementia subjects and healthy controls. Furthermore, employing Lasso Logistic Regression on all available features, our best model achieved a state-of-the-art (SOTA) cross-validated AUC of 80.6% , emphasizing the robustness and effectiveness of the selected linguistic features in early dementia detection. Our findings not only reinforce the potential of automated speech analysis as a non-invasive, cost-effective tool for cognitive health assessments but also demonstrate the utility of character-level Markov Chain analysis in enhancing the sensitivity and timeliness of early dementia detection.",2,Fulton | Renaissance Waverly Hotel,Innovations in Healthcare Delivery and Clinical Operations,107
361,9008.0,Optimizing Referral Strategies for Whole Genome Sequencing in Lung Cancer Diagnosis,,"The implementation of Whole Genome Sequencing (WGS) in lung cancer diagnosis offers a comprehensive approach to identifying genetic mutations, enabling more targeted treatments. However, the high costs and limited capacity for WGS present significant challenges, particularly within referral systems among hospitals with varying testing capabilities. This study develops a simulation framework to optimize referral strategies across general, teaching, and academic hospitals, as well as WGS facilities. In addition to modeling patient flow, the framework incorporates patient mortality rates, associated costs, and the cost of waiting time for diagnosis. The study compares different referral scenarios: the standard of care, which follows a sequential referral pathway through multiple hospital types, and a proposed scenario where patients bypass teaching hospitals and are referred directly from general to academic hospitals.",4,Fulton | Renaissance Waverly Hotel,Innovations in Healthcare Delivery and Clinical Operations,107
362,8975.0,Using AI and health information exchange to summarize community-based care information in the hospital: a position paper,Academician,"Using AI and health information exchange to summarize community-based care information in the hospital: a position paper Despite advancements in Health information exchange (HIE), effectively integrating community-based care information into hospital settings remains a significant hurdle, hindering optimal care coordination and care outcomes for all patients, particularly complex older adults with cross-sector care needs. However, current data sharing methods often provides clinicians with volumes of semi-processed data that requires significant time and effort to validate and interpret. By integrating artificial intelligence tools into regional health information organizations (RHIOs) we can create a Hospital Admission Summary (HAS) that leverages trusted data sharing networks and analytic capabilities to provide personalized summaries of individuals’ health status and care needs, and significantly improve hospital treatment and discharge planning decision. To achieve this, we focus on three key elements: meaningfulness, or summaries that accurately reflect an individual’s most important health status and care needs; relevance, or summaries that are directly applicable to hospital treatment and discharge decisions by providing a unified and actionable understanding of care received from multiple community-based clinicians; and trust, summaries that are accurate, interpretable, and appropriately address missing or conflicting data drawn from multiple sources. We will discuss our research vision, exploratory analysis, prototype development efforts, algorithmic challenges, and workflow considerations for implementing AI-enhanced admission summaries. We will also discuss critical research needs in system design, information flow protocols, and data-sharing parameters, while highlighting how AI and HIE can connect community and hospital-based care to improve treatment decision-making.",1,Fulton | Renaissance Waverly Hotel,Artificial Intelligence for Healthcare Solutions,108
363,8795.0,Advancing Explainable Machine Learning for Healthcare: Pathways to Clinical Integration and Future Innovations,Academician,"This study presents a comprehensive review of predictive models for patient outcomes in critical care, with a specific focus on acute kidney injury (AKI) prediction. A selection of peer-reviewed articles from 2010 to 2024 was analyzed to assess the state of predictive modeling in this domain. The review explores a range of machine learning algorithms, including Logistic Regression, Support Vector Machines, and Random Forests, evaluating their performance through metrics such as AUROC, accuracy, and recall. Model evaluation methods, including calibration techniques and internal validation strategies, were also examined to understand their strengths and limitations in clinical applications. Key aspects of the analysis include feature selection approaches, such as SHAP-based ranking and ensemble feature ranking, to identify significant predictors across studies. Common predictors included demographic characteristics, clinical biomarkers, and comorbidities, which demonstrated strong relevance in predicting outcomes. Studies varied in terms of data sources, sample sizes, and demographic characteristics, influencing the generalizability of the models. Despite significant progress in the development of predictive algorithms, challenges such as limited external validation and integration into clinical workflows remain. This review highlights the importance of standardized evaluation frameworks and the need for greater emphasis on model interpretability and validation. These findings provide valuable insights for researchers and clinicians aiming to develop and implement robust predictive models, ultimately enhancing patient care in critical settings.",2,Fulton | Renaissance Waverly Hotel,Artificial Intelligence for Healthcare Solutions,108
364,9222.0,Reinforcement Learning for Optimizing Laboratory Test Utilization in Intensive Care Units: A Framework for Reducing Unnecessary Testing,Academician,"Unnecessary laboratory testing in intensive care units (ICUs) has long been identified as a major contributor to low efficiency, increased costs, and heightened medical risks. Laboratory tests are very critical in monitoring and managing patient health; however, a high percentage of these tests have very limited clinical value. This underscores the urgent need for optimization strategies for lab test utilization without compromising clinical outcomes. Most current decision-making processes regarding laboratory test orders still require dynamic adaptation to patient conditions changes. This study proposes a novel framework leveraging reinforcement learning (RL) that makes high-quality decisions about ordering a laboratory test. By learning from historical patient data, our RL model aims to recommend whether a lab test is necessary at any given time, balancing patient needs with resource efficiency. Using the MIMIC-IV dataset, the model captures temporal patterns in patient health trajectories, considering test timing and clinical event irregularities. Herein, our approach tackles two significant challenges in irregularities in the ICU data and dynamic decision-making by incorporating reward systems prioritizing safety and cost reduction. By implementing our RL framework, we could reduce the number of unnecessary tests without affecting clinical outcomes.",3,Fulton | Renaissance Waverly Hotel,Artificial Intelligence for Healthcare Solutions,108
365,6800.0,Stakeholder Perspectives on Digital Twin Implementation Challenges in Healthcare: Insights from a Provider Digital Twin Case Study,Practitioner,"Digital twin (DT) technology holds immense potential for transforming healthcare systems through real-time monitoring, predictive analysis, and agile interventions to support various decision-making needs. However, its successful implementation is contingent upon addressing an array of complex socio-technical challenges. Using a case study of provider workload DT, this research investigates DT implementation challenges in healthcare based on the perspectives of four stakeholder groups: family medicine specialists (FMSs), organizational psychologists (OPs), engineers (EEs), and implementation scientists (ISs). We conducted semi-structured interviews with these diverse stakeholder groups guided by the Consolidated Framework for Implementation Research (CFIR) 2.0, an implementation science framework widely used for understanding factors influencing implementation outcomes. We then mapped each stakeholder groups’ preferences and concerns across five CFIR 2.0 domains, revealing a nuanced landscape of converging and diverging perspectives that highlight both shared and group-specific implementation barriers. Our findings reveal convergence on core concerns such as data privacy & security, interoperability, and regulatory compliance, highlighting shared priorities. However, divergences also emerged, reflecting each group’s functional focus. For instance, while EEs emphasize technical issues like usability and scalability, FMSs prioritize practical challenges, including workload impact, staffing shortages, and resistance to change. OPs highlight collaboration, communication, and organizational alignment; while ISs concentrate on ethical issues and technology readiness. These findings emphasize the need for a multidisciplinary, stakeholder-sensitive approach that addresses both functional and practical concerns; highlighting the importance of tailored implementation strategies to facilitate successful DT adoption in healthcare.",4,Fulton | Renaissance Waverly Hotel,Artificial Intelligence for Healthcare Solutions,108
366,6613.0,"Location Planning, Resource Reallocation and Patient Assignment During a Pandemic Considering the Needs of Ordinary Patients",Academician,"During the initial phase of a pandemic outbreak, the rapid increase in the number of infected patients leads to shortages of medical resources for both pandemicrelated and ordinary patients. It is crucial to efficiently utilize limited existing resources and strike a balance between controlling the pandemic and sustaining regular healthcare system operations. To tackle this challenge, we introduce and investigate the problem of optimizing the location of designated hospitals, reallocating beds within these hospitals, and assigning different types of patients to these hospitals. Designated hospitals isolate pandemic-related patients from ordinary patients to prevent cross-infection. Moreover, isolation beds can be converted into ordinary beds and vice versa. We model this problem as a mixed-integer nonlinear programming model and incorporate a compartmental model to facilitate the dynamic adjustment of available resources as the pandemic progresses. We illustrate the effectiveness of our model using real data from the COVID- 19 pandemic. In comparison to two benchmark policies, our model demonstrates superior performance in controlling the spread of the pandemic while addressing the needs of both pandemic-related patients and ordinary patients. We also conduct a series of experiments to uncover managerial insights for policymakers to better utilize existing resources in response to pandemic outbreaks. Results indicate that the optimal strategy is to admit as many pandemic-related patients as possible to flatten the pandemic peaks during its initial phases, especially under high transmission rates. We also demonstrate the flexibility of the proposed model by integrating interventions such as mask wearing and lockdown.",1,Ascot | Renaissance Waverly Hotel,Analytics for Pandemic Response and Recovery,109
367,5597.0,Small Area Estimation of Case Growths for Timely COVID-19 Outbreak Detection,Academician,"The COVID-19 pandemic has exerted a profound impact on the global economy and continues to exact a significant toll on human lives. The COVID-19 case growth rate stands as a key epidemiological parameter to estimate and monitor for effective detection and containment of the resurgence of outbreaks. A fundamental challenge in growth rate estimation and hence outbreak detection is balancing the accuracy-speed tradeoff, where accuracy typically degrades with shorter fitting windows. In this paper, we develop a machine learning (ML) algorithm, which we call Transfer Learning Generalized Random Forest (TLGRF), that balances this accuracy-speed tradeoff. Specifically, we estimate the instantaneous COVID-19 exponential growth rate for each U.S. county by using TLGRF that chooses an adaptive fitting window size based on relevant day-level and county-level features affecting the disease spread. Through transfer learning, TLGRF can accurately estimate case growth rates for counties with small sample sizes. Out-of-sample prediction analysis shows that TLGRF outperforms established growth rate estimation methods. Furthermore, we conducted a case study based on outbreak case data from the state of Colorado and showed that the timely detection of outbreaks could have been improved by up to 224% using TLGRF when compared to the decisions made by Colorado's Department of Health and Environment (CDPHE). To facilitate implementation, we have developed a publicly available outbreak detection tool for timely detection of COVID-19 outbreaks in each U.S. county, which received substantial attention from policymakers.",2,Ascot | Renaissance Waverly Hotel,Analytics for Pandemic Response and Recovery,109
368,6214.0,Evaluating Post-Pandemic Efficiency in Mexico’s Public Healthcare System: A Two-Stage DEA Analysis,Practitioner,"The COVID-19 pandemic brought significant changes to global health care systems, including Mexico’s, where quality of services suffered, and hospitals faced capacity shortages. The Mexican public health system, however, remains under-researched, and its post-pandemic efficiency largely unexplored. This study aims to evaluate the efficiency of the Mexican Public Healthcare System from 2021 to 2022 by analyzing thirty-two states and six Nielsen zones using Data Envelopment Analysis (DEA). A two-stage BCC input-oriented efficiency model was developed. In the first stage, inputs like budget allocations, the growth rate of public healthcare spending, and current spending prices were analyzed. The second stage incorporated operational inputs such as the number of doctors, incubators, and pharmacies. Outputs included services coverage, service quality, and total consultations provided. The main findings reveal that budget size does not guarantee high quality, though quality correlates with the number of doctors and public spending prices. Mexico City, Jalisco, and Michoacán showed high efficiency levels across both stages, whereas Sinaloa, Nuevo León, and Baja California ranked as the least efficient states. Among Nielsen zones, the Pacific zone had the lowest efficiency. These findings suggest ways to improve resource allocation, prioritizing state needs to enhance healthcare coverage and quality. Additionally, this methodology can assess efficiency in services like mental healthcare, obesity, and cancer programs, providing a valuable approach to optimize healthcare resources in Mexico.",3,Ascot | Renaissance Waverly Hotel,Analytics for Pandemic Response and Recovery,109
369,6444.0,The Influence of COVID-19 on Future Tuberculosis Reporting Rates: Regional Forecasts Using Time Series Model,Practitioner,"The COVID-19 pandemic has significantly weakened healthcare systems around the world, by delaying routine care and affecting disease discovery and treatment. One of the important diseases that has been impacted is Tuberculosis (TB). This paper will analyze the implications of the COVID-19 pandemic on TB notification rates as well as create a predictive model to anticipate future cases of TB patterns within defined WHO regions. By using extracted WHO TB notification data, which shows the TB rates starting from 2020 to 2024, and employing time series with the LSTM model using Python programming. The findings indicate that TB notification rates could decline in the current year and the upcoming two years as healthcare systems stabilize after the pandemic. The findings are intended to inform health policy development and resource allocation in TB prevention and control.",4,Ascot | Renaissance Waverly Hotel,Analytics for Pandemic Response and Recovery,109
370,8841.0,A Time-to-event Analysis of Demographic and Healthcare Access Factors of Diagnostic Delay in Type 2 Diabetes,Academician,"Type 2 Diabetes (T2D) is a high prevalence chronic disease impacting 34 million Americans. Timely diagnosis of T2D in primary care is challenging due to the heterogeneity of patients and follow-up challenges. Reducing diagnostic delay relies on understanding individual- and system-level risk factors. This is critical for identifying barriers to the timely T2D diagnosis experienced by certain subpopulations. The objectives of this study are to investigate the determinants of time to T2D diagnosis in primary care and identify subpopulations most vulnerable to diagnostic delay after onset of symptoms. Utilizing time-based delay metrics, e.g., duration from initial abnormal Hemoglobin A1c (HgA1c) test result to diagnosis, we compare subpopulations – defined by demographics and health care access variables – using a Time-to-event model. Retrospective data includes health records of 2,540 patients from two primary care facilities within a healthcare system in the Midwest U.S., collected between 2017 and 2023. Methods include parametric statistical tests to compare subpopulations, survival analysis and Cox proportional hazards model to assess individual risk factors. Key results show disparities among subpopulations regarding time to T2D diagnosis. Within six months after initial abnormal HgA1c, 51% to 58% of patients remained undiagnosed, decreasing to 37% to 46% by the end of one year. Patients at the second primary care location had a 31% higher hazard than baseline location. Patients with Medicaid (and private) insurance plans experienced 68% (and 30%) higher hazard rate compared to those with Medicare. Results highlight the need for personalized interventions for groups at higher risk of diagnostic delay.",1,Ascot | Renaissance Waverly Hotel,Predictive Analytics for Improving Health Outcomes,110
371,6287.0,Revolutionizing Food Banks: The Role of AI in Operations and Allergy Management,Practitioner,"Food insecurity remains a global challenge, impacting the lives of millions of individuals. Food banks play a crucial role in addressing this problem by offering essential resources and emergency support to the food insecure populations. As the need for their services grows, the challenges they face become increasingly complex, particularly in understanding and accommodating the diverse dietary needs of the communities they serve. Beyond simply distributing food, food banks must navigate challenges such as cultural preferences, dietary restrictions, and food allergies, requiring innovative approaches to ensure adequate support. Artificial Intelligence (AI) is revolutionizing food bank operations by enhancing efficiency and improving the management of food allergies among recipients. This study examines the role of AI in optimizing inventory management and ensuring the safe allocation of allergen-free products to individuals with specific dietary needs. Using machine learning models, food banks can categorize inventory more effectively, balancing supply with community food preferences while reducing waste. AI-powered tools enhance the tracking and labeling of allergen-free and special dietary items, ensuring safer distributions for food-allergic recipients. The implementation of these AI-driven solutions has led to noticeable improvements in operational effectiveness, inclusivity, and the ability to provide tailored support to food-insecure populations, ultimately creating a more responsive and equitable system.",2,Ascot | Renaissance Waverly Hotel,Predictive Analytics for Improving Health Outcomes,110
372,6450.0,Predicting post-stroke activities of daily living: Enhancing Machine Learning with Feature,Academician,"The best care for stroke patients requires predicting their daily activities. The post-stroke rehabilitation procedure significantly impacts patients' overall health and progress. Planning for daily tasks with machine learning can substantially improve care quality. This study assessed how well machine learning techniques such as K Nearest Neighbors (KNN), Gaussian Naive Bayes, Support Vector Machine (SVM), Random Forest (RF), Decision Trees (DT), and Logistic Regression (LR) predicted the Barthel Index (BI) for participants in a Post-Care Cerebrovascular Diseases program. Data from 313 patients (208 men and 105 women) were analyzed, and 15 rehabilitation assessment criteria were considered. A comprehensive review of the data was conducted to identify features of the dataset, including its distribution and basic properties. Furthermore, Principal Component analysis (PCA) was used to determine the variables that had an impact, streamline the dataset, and enhance model performance. The data show that DT and RF perform well in predicting stroke recovery outcomes, with feature selection strategies helping them reach an accuracy rate of 99.36%. This model demonstrated 99.36% recall and 99.37% precision, and high accuracy (99.36%). The analysis confirmed the model's predictive power and dependability by analyzing a confusion matrix. Significant findings highlight the impact of variables including gender, BI at discharge, and BI at admission on predictions, highlighting the potential benefits of machine learning for improving stroke rehabilitation outcomes. This could aid medical practitioners in efficiently allocating resources and arranging patient care.",3,Ascot | Renaissance Waverly Hotel,Predictive Analytics for Improving Health Outcomes,110
373,7091.0,Survival and Recurrence in Breast Cancer: Analyzing the Effects of Treatment Timing and Receptor Status Using METABRIC Data,Academician,"Breast cancer causes over 2 million mortality cases annually, and the case number is increasing every year, making it the leading cause of death for women. Analyzing the impact of treatment type and predicting the risk of breast cancer recurrence, especially after breast surgery, is crucial in enhancing the treatment process and survival chances. The paper will use survival analysis to investigate the impact of time to surgery (TTS) and its interaction with other treatments on cancer recurrence. Previous research explored the association between delayed surgery and decreased survival rates. This paper will analyze how TTS interacts with other treatment modalities and specific cancer subtypes and their effect on survival. The pathological METABRIC dataset consists of 2,509 patients with detailed clinical data; survival analysis models, including the Kaplan-Meier estimator and Cox proportional hazards models, will be applied to incorporate covariates such as treatment types and receptor status. The dataset includes two key events (survival and relapse) with the duration, enabling a robust analysis of survival probabilities and recurrence patterns. Integrative clusters and mutation counts will aid in examining the genomic landscape and its influence on treatment results. The expected findings suggest that optimized TTS and strategic timing may have a positive impact on survival rates and help reduce relapse risks, considering different patient subgroups based on receptor status and other clinical factors. This study will provide insights for future predictive modeling that would improve future treatment decisions and provide a guide for personalized breast cancer management to improve treatment outcomes.",4,Ascot | Renaissance Waverly Hotel,Predictive Analytics for Improving Health Outcomes,110
374,6113.0,Enhanced Prediabetes Prediction through Optimization-Based Feature Selection,,"Prediabetes is a critical precursor to type 2 diabetes, which affects over 300 million people worldwide. Early identification and personalized intervention remain a challenge. This paper explores the optimizing algorithms in enhancing feature selection for machine learning classifiers in the context of prediabetes, using a dataset from the DiabHealth Clinic in Australia. The database includes records from 850 patients. We applied two feature selection methods, Particle Swarm Optimization (PSO) and Genetic Algorithms (GA), to identify critical predictive features from this dataset. PSO and GA were evaluated using two different classifiers: Logistic Regression (LR) and Multi-Layer Perceptron (MLP). Our findings reveal that optimization-based feature selection significantly improves model performance in classifying prediabetes based on glycated hemoglobin (HbA1c). In particular, PSO achieved the highest Area Under the Curve (AUC) values, with 85% for LR and 83% for MLP, and the highest accuracy values, with 82.8% for LR and 78.1% for MLP, indicating its superior effectiveness in feature selection. Integrating these algorithms into the feature selection process has strengthened the predictive models and highlighted their potential in advancing medical diagnostics. The study advocates for adopting optimization algorithms in machine learning to refine early diagnostic tools for prediabetes and potentially transform healthcare analytics.",1,Ascot | Renaissance Waverly Hotel,Predictive Modeling and Decision Support in Disease Management,111
375,6420.0,Human-AI Decision Support for Enhanced Sepsis Prediction and Treatment System,Academician,"Sepsis is a critical condition caused by an abnormal immune response to infection, often leading to poor outcomes for hospitalized patients. Existing early warning systems aimed at identifying sepsis risk tend to produce numerous false-positive alerts, which can lead to alert fatigue and result in these warnings being overlooked by healthcare providers. Recognition delays are further worsened by heavy workloads and low staffing levels, making timely intervention challenging. To tackle this problem, this study introduced a Human-AI Decision Support Framework (HADS) that combines AI with clinical expertise to identify high-risk patients and enhance sepsis management. The framework developed a human-in-the-loop model enriched with physician expertise. Employing several machine learning algorithms achieved a high accuracy of 97% and reduced false alarms by 48% compared to traditional alert systems. Critical interventions included timely antibiotic delivery, blood culture collection, and continuous patient monitoring. This framework streamlined sepsis pathways, significantly reducing response times within the critical ""golden hour,"" vital for improving patient outcomes. The study also outlined directions for future research and practical applications to improve patient care using the HADS approach.",2,Ascot | Renaissance Waverly Hotel,Predictive Modeling and Decision Support in Disease Management,111
376,6422.0,Risk Factor Prediction and Survival Analysis for Chronic Disease Recurrence Using Machine Learning Models,Academician,"Chronic diseases are a major public health concern in KSA, reflecting global trends in prevalence and impact. Among these, ccolorectal cancer (CRC) is one that is most urgent, being the most frequently diagnosed cancer in males and the third most common in females in KSA. Our research introduces a novel approach to addressing the pressing issue of chronic disease recurrence, specifically in CRC, by applying advanced machine learning models for risk factor prediction and survival analysis. Through this innovative methodology, we anticipate significant contributions to CRC management and prevention, with the potential to transform patient care by offering more accurate and individualized predictions. Expected outcomes include the improved identification of high-risk individuals and a deeper understanding of recurrence patterns, which could guide tailored interventions and enhance overall healthcare strategies for CRC The study will use descriptive statistics to summarize participants' clinical and demographic attributes, providing context for developing predictive models for CRC recurrence risk factors and survival analysis, advanced machine learning algorithms will be utilized, including random forests, logistic regression, support vector machines, and decision trees. The effectiveness of these models in predicting CRC recurrence will be evaluated using ROC curve analysis. To mitigate overfitting and enhance the robustness of the results, cross-validation will be systematically conducted on the training dataset. Feature selection will be performed using Sequential Backward Search Selection, initially considering all features and progressively eliminating non-informative ones in each iteration to enhance model performance. The performance of the machine learning models will be assessed using various metrics.",3,Ascot | Renaissance Waverly Hotel,Predictive Modeling and Decision Support in Disease Management,111
377,9027.0,Using Machine Learning to predict the likelihood or risk of early Alzheimer’s symptoms based on lifestyle,Academician,"Alzheimer’s disease (AD), a progressive neurodegenerative disorder marked by cognitive decline, poses significant healthcare challenges worldwide. Early detection of AD is crucial for improving patient outcomes through timely interventions, yet it remains complex due to the subtlety of initial symptoms and the disorder’s variability across individuals. This study aims to enhance early-stage prediction of AD by leveraging machine learning (ML) techniques to analyze correlations between lifestyle, health, and cognitive indicators with AD onset. Using a dataset from the Centers for Disease Control and Prevention (CDC), specifically from the Behavioral Risk Factor Surveillance System (BRFSS), this research examines data from 3,376 New York-based participants across 29 health-related variables, with a focus on identifying key predictors such as cognitive decline, mental health, physical activity, and substance use. By employing the Random Forest algorithm, known for its capacity to capture complex, non-linear relationships, the study investigates which factors most strongly indicate early AD risk and explores how these insights can inform healthcare strategies, enable personalized preventive measures, and ultimately contribute to alleviating the healthcare burden associated with AD.",4,Ascot | Renaissance Waverly Hotel,Predictive Modeling and Decision Support in Disease Management,111
378,6519.0,Assessing Cognitive Load via Physiological Analysis in Nursing Training Using Extended Reality,Academician,"This study introduces an advanced training system for nursing learners within an extended reality (XR) environment, focusing on the analysis of multiple sources of physiological data to assess cognitive load and stress. The system immerses learners in complex, simulated patient care scenarios mirroring real-world clinical demands. Integrated with a conversational artificial intelligence (AI) model, digital patients can understand and respond to learner’s queries, enhancing the realism and interactivity of each scenario. To better understand learners’ responses, wearable sensors continuously monitor physiological indicators, such as heart rate, electrodermal activity, and skin conductance, capturing real-time data on stress levels as learners interact with virtual patients and make critical decisions. This is complemented by eye-tracking technology, which measures visual attention and focus, providing essential insights into learner’s cognitive load and situational awareness. By analyzing this data, we capture a comprehensive view of learner’s responses to high-stress scenarios, identifying patterns of stress and cognitive load management. This multi-layered approach provides a detailed understanding of how learners process information, respond under pressure, and maintain situational awareness, pinpointing moments where mental demand is highest. This combined focus on physiological analysis allows for a more precise understanding of learners’ cognitive load, which in turn informs the design of realistic, high-stress simulations. By capturing how learners respond to demanding scenarios, this approach ensures that training sessions target cognitive challenges nurses face in real-world patient care. Such targeted insights enhance the training experience, equipping learners with the skills, mental resilience, and situational awareness necessary for effective, real-life patient interactions.",1,Ascot | Renaissance Waverly Hotel,Human-Centered Design and Technology Integration in Healthcare,112
379,6766.0,AI powered VR system for ADHD learning intervention using EEG and eye tracking data,,"There is a notable increase in students with attention deficit hyperactivity disorder (ADHD) in the college STEM programs, students with ADHD are observed to have more educational challenges in learning manufacturing related contents. Virtual Reality (VR) offers ADHD students a dynamic and immersive learning environment, enabling them to interact with complex concepts in a more engaged and attractive manner. VR has the potential to amplify the strengths of students with ADHD while addressing their limitations, making it an ideal platform for enhancing their learning experience. Bio-information collected from wearable sensors, such as eye trackers, can offer useful insights into students' current states. However, relying on a single type of sensor data may provide a misinterpretation and lead to incorrect interventions. To gain a more accurate assessment of the student's true state, combining multiple synchronized bio-sensor data, including eye movement, brain signals, and heart signals, can help identify whether inattention, cognitive overload, or stressful is occurring, leading to more precise and effective interventions. In this research, we developed several manufacturing related VR scenarios, and incorporated multiple wearable sensors to perform holistic evaluation of ADHD students' mental status during the VR experience. The results showed that integrating multiple wearable sensors could provide more accurate estimation of learning status and mental status of students with ADHD.",2,Ascot | Renaissance Waverly Hotel,Human-Centered Design and Technology Integration in Healthcare,112
380,8983.0,Extended Technology Acceptance Modelling: AI Empowered Immersive Virtual Reality Tool in Gerontological Nursing Facility,,"This study investigates the adaptation and acceptance of an AI-empowered immersive Virtual Reality (VR) tool in gerontological nursing facilities, employing an Extended Technology Acceptance Model (ETAM) as the conceptual framework. The research focuses on understanding how factors such as perceived ease of use, perceived usefulness, and user satisfaction influence the adoption of this innovative technology among caregivers and nursing staff. The tool aims to enhance elderly care by supporting cognitive engagement, emotional well-being, and operational efficiency. Data were collected through a structured questionnaire administered to nursing staff and caregivers in gerontological facilities. Partial Least Squares Structural Equation Modeling (PLS-SEM) was used to analyze the data, providing insights into the causal relationships between acceptance factors and behavioral intention to use the system. Results indicate that perceived usefulness, ease of use, and enjoyment significantly influence user purchase and adoption intention. Moreover, the tool demonstrates potential in reducing caregiver workload and improving the quality of elderly care services. The study's findings underscore the importance of strategic design and training programs to facilitate the effective implementation of AI-VR tools in gerontological settings. This research contributes to the growing field of digital healthcare innovations, offering a robust analytical framework and actionable recommendations for advancing technology adoption in elder care.",3,Ascot | Renaissance Waverly Hotel,Human-Centered Design and Technology Integration in Healthcare,112
381,5077.0,Elevating Efficiency and Innovation: The S.A.P.T. Methodology for Digital Transformation of Analytical Roles,Practitioner,"In the current business environment, the need for digital transformation in organizations is imperative to enhance efficiency and maintain competitiveness. This paper introduces the Strategic Approach to Performance and Technology (S.A.P.T.) methodology, a framework designed to facilitate the digital transformation of complex analytical roles that utilize data and systems to complete tasks. S.A.P.T. amalgamates elements from Agile, Lean, and Six Sigma methodologies to optimize and automate these functions. The methodology comprises four sequential phases aligned with proven continuous improvement methodologies: Standardization, Analytics, Performance, and Technology. The ‘Standardization’ phase, drawing on Lean tenets, establishes standardized processes for normalizing roles. The ‘Analytics’ phase, influenced by Six Sigma, focuses on developing critical insights for data empowerment. The ‘Performance’ phase, inspired by the Balanced Scorecard and Total Quality Management concepts, introduces Key Performance Indicators and scorecards for performance measurement. Finally, the ‘Technology’ phase leverages advanced technology for automation. The implementation of S.A.P.T. across various supply chain analytical functions has shown significant efficacy in driving digital transformation for nuanced analytical roles. This article is structured into distinct sections: initial challenges in digital transformation for analytical functions, a description and details of the S.A.P.T. methodology, real-world case studies demonstrating its adaptability and success, and a conclusion summarizing key insights and future research directions. The S.A.P.T. methodology presents a comprehensive and structured approach to propel digital transformation in complex analytical roles.",1,101 | Cobb Galleria Centre,Operational Excellence Session 2,113
382,6725.0,"Rocketing Production: The Secret of the Machine Designed to Transform Production and Save $14,800 a Year",Academician,"Abstract : The packaging industry plays a critical role in the supply chain, as packages protect the goods until they are delivered to the final consumer. This is especially true for the automotive industry, where the package must avoid damage. This work was developed in the packaging company MELA, which develops packaging and packaging materials for the automotive industry in the center of Mexico. The “laminate” process in MELA presented high variability in times, numerous downtimes, high defect rates, slow production time, and ergonomics were not considered. To solve these problems, a novel laminating machine was designed and implemented. This paper presents the design process for the laminating machine, from mapping the process to identifying its areas of opportunity, benchmarking, implementation, and validation. This innovation reduced the required space from 88 m² to 33 m² and decreased 30% the production time, that is, from 73 seconds to 51 seconds automated. As a result, productivity increased, with exceptional quality, with fewer ergonomic issues, as well as the reduction of annual costs from $32,000 to $17,200, achieving savings of $14,800 per year and with a return on investment of 18 months. The implementation of this innovation in the rolling process represents a significant step towards efficiency, quality, and productivity, evidencing the tangible value of the project and its potential to generate substantial savings.",2,101 | Cobb Galleria Centre,Operational Excellence Session 2,113
383,6019.0,The ISE as Internal Consultant,Practitioner,"The Industrial & Systems Engineer (ISE) has a broad skillset that can add value in many parts of an organization. Positioning one ore more ISEs as consultants allow one ISE subject matter expert to influence a large number of projects. This presentation will offer a look at how Chick-fil-A deploys its ISEs to get the most value out of the ISE resources avaialable. The mindset, practical skills, and approach necessary to be a strategic, sought-after consultant within one's own company will be addressd. Finally, I will present a proven action plan to transition ISEs from practitioner to consultant.",3,101 | Cobb Galleria Centre,Operational Excellence Session 2,113
384,6754.0,Economically Sustainable Process Optimization for Enhanced Fulfillment Performance in Healthcare Manufacturing,Practitioner,"Within healthcare manufacturing there is increasing pressure to deliver high-quality products efficiently and reliably, without cessation, to meet the needs of healthcare providers. This is amplified by the demand of care increasing for the aging population, needing to be cost effective in a competitive landscape, and staying ahead of the rapidly changing healthcare supply chain environment that was underscored by the COVID-19 pandemic. Uninterrupted equipment operation is critical in these continuous production systems, as even minor disruptions can propagate upstream, making the system unreliable and cause significant stress on the workforce and limits overall operation effectiveness. These operational issues have adverse impacts on patients’ access to life-saving products from both a quality of life and an economic perspective. This case study examines the implementation of a people-centric operational excellence (PCOEx) framework that addresses frequent disruptions, unbalanced equipment utilization, and reduced operational efficiency in a healthcare component manufacturing facility's pack-out area. Based on the Sawhney Model, this systems-thinking approach incorporates four key modules: systematic problem identification, performance measurement hierarchies, reliability-based analysis, and sustainability risk assessment. The implementation resulted in significant improvements in equipment reliability, reduced failure rates, and enhanced resource utilization. The framework's emphasis on employee engagement led to improved working conditions while simultaneously reducing equipment downtime. These results provide a comprehensive roadmap for healthcare manufacturers seeking to achieve sustainable operational excellence, demonstrating how integrating people-centric approaches with traditional operational improvement methods help organizations maintain high-quality standards in an increasingly competitive industry.",1,101 | Cobb Galleria Centre,Operational Excellence Session 3 - Healthcare Applications,114
385,6954.0,Every place has a thing and everything has a place: Transforming spaces and productivity with an efficient layout.,Practitioner,"¿How important is the design of an industrial plant? This study addresses significant logistical challenges encountered by a young but structurally sound company with a strong vision and clear values. Although the company possesses many strengths, logistical issues impeded its operational advancement. The initial layout restricted material flow and created storage inefficiencies, limiting the company's production capacity and overall efficiency. By analyzing current processes and collaborating with the management team, a new layout was proposed to optimize space utilization; dedicating storage for each product type, five production lines, material transport corridors, and designated spaces for essential equipment. The implementation of the new layout led to a substantial improvement in operational efficiency. Material handling time was reduced by 82.8%, resulting in significant time savings and an annual cost reduction of MXN 50,212.50. Additionally, the integration of an inventory and production tracking system allowed for more precise resource management, improving both decision-making and operational control. This redesign not only reduced operational costs but also strengthened the company’s ability to meet growing market demands while maintaining its commitment to social responsibility and sustainability. By minimizing energy consumption, reducing material waste, and improving production processes, the environmental impact was lowered, aligning with its sustainability goals. These improvements enhanced the company’s competitiveness in the automotive packaging market, positioning the company for continued growth and success in an increasingly demanding industry.",2,101 | Cobb Galleria Centre,Operational Excellence Session 3 - Healthcare Applications,114
386,6317.0,Using Process Modeling to Drive Lean Transformations,Practitioner,"For decades, traditional Lean tools like line balancing, operator run charts, and Takt time analyses have been invaluable in driving operational improvements. However, the increasing pace and inherent variability in modern processes require more than tools that rely on ""average"" conditions. In real-world environments, fluctuations in demand, supply disruptions, and quality variations are constants that call for deeper insights into process dynamics. Discrete event simulation (DES) meets this need by creating a digital twin of the process that replicates the real-world variability. This presentation will introduce DES, explain benefits and key considerations of a DES model, and demonstrate how it can be used to enhance Lean initiatives. We'll cover example models across varying industries such as food and beverage, healthcare, and manufacturing. The models presented will highlight enhancements in queuing reduction, process efficiency, and resource allocation. While explaining each model, we will review key considerations such as breaking a process into discrete steps, capturing data that reflects the variability in those process steps, and ensuring appropriate model scope to avoid complexity or inadequate representation. In the conclusion, we will engage the audience in a discussion on incorporating DES into the Lean practitioner’s toolkit. We’ll explore how DES can enhance forecast accuracy for Lean improvements, reduce the costs associated with live pilot testing, and provide the flexibility to rapidly test multiple improvement ideas. With DES, organizations gain a powerful tool to manage variability, enabling them to move forward with Lean transformations confidently and achieve reliable post-implementation results.",1,101 | Cobb Galleria Centre,Operational Excellence Session 4,115
387,5045.0,Streamlining a Sandwich Assembly in a Coffee Shop,Academician,"Supremo Café is a small, family-owned business coffee shop specializing in coffee and breakfast items founded in 2021. The owner, inspired by a passion for the food industry and driven by the changes brought on by the COVID-19 pandemic, decided to start her venture to offer a unique café experience. For the past 5 months, Supremo Café has been experiencing delays in food deliveries, which has led to lost revenue and customers due to the disorganization in their workstation. Exceeding the target per day of $1,000, the owner wants to reach 100% customer satisfaction. Some authors have concluded that this disorganization leads to wasted time, which delays delivery. They concluded that the layout of employees' work patterns is essential for achieving productivity gains, suggesting that facilities managers should create a balanced environment of private and communal spaces based on these patterns. This research aims to reduce the lead time from 4 to 2 minutes in 4 months. DMAIC methodology was used for continuous process improvement and to reduce lead time. This will help in reducing variability in preparation time. Analyze the current workflow to identify bottlenecks and inefficiencies and conduct comprehensive training sessions for all staff members on the new procedures. Initial assessments expect a decrease in average preparation time from 4 minutes to around 2.5 minutes, indicating successful implementation of the standardized processes and optimized workflow. Reducing sandwich preparation time may foster continuous improvement, prompting Supremo Cafe to enhance efficiency.",2,101 | Cobb Galleria Centre,Operational Excellence Session 4,115
388,6329.0,Improving Change Over Process using Operational Excellence in the Textile Industry,Academician,"In today’s fast-paced fashion industry, staying competitive means creating quality products efficiently and agilely. Industrias Valores is a textile company that has been producing quality formal apparel for men since the 1990s. At Industrias Valore, the Polo Shirt production line faces a critical challenge: prolonged model changeover times that can reach 368 minutes, especially with less common designs. This bottleneck not only drives up costs but also limits our ability to meet dynamic market demands and customer expectations. This project aims to transform this process, slashing changeover time from 368 to 60 minutes within 18 weeks. By standardizing processes, empowering the team with rapid adaptation skills, and fostering seamless communication across departments, we are setting the foundation for a leaner and faster production line. The approach includes rigorous training, efficiency testing, and targeted analysis with Lean Manufacturing tools, such as SMED, standardized processes, and 5S's. All these tools are shown using the A3 methodology. Early results are promising, with noticeable reductions in changeover times hinting at a new era of productivity for Industrias Valore. By aligning these enhancements with customer-facing metrics, this project optimizes the back end and fortifies our reputation to be more competitive in the market. In conclusion, this initiative is more than just operational efficiency—it’s about delivering on our promise of quality, speed, and innovation to keep Industrias Valore at the forefront of the textile industry.",1,101 | Cobb Galleria Centre,Operational Excellence Session 6,116
389,6631.0,Using Generative AI as a Co-Pilot in Process Improvement,Practitioner,"As work continues to evolve, Artificial Intelligence (AI), especially generative AI and Large Language Models (LLMs), is proving essential in augmenting our workflows. This session will explore integrating AI into traditional process improvement practices, demonstrating how it can enhance effectiveness and provide participants with actionable ideas for leveraging these technologies to excel in their fields. The session begins with an overview of LLMs, outlining their capabilities and limitations. Next, we will showcase practical examples of generative AI applications in process improvement, such as refining problem statements on project charters, streamlining data pre-processing, facilitating root cause discovery, improving affinitization sessions, and strengthening change management strategies. This hands-on approach will help attendees visualize how AI can support critical tasks within process improvement. Additionally, we will address common AI concerns, including the safe integration of proprietary information, effective training on AI prompting and usage, and providing systems with context beyond general training data. This segment aims to equip participants with strategies to use AI responsibly while navigating potential challenges. By treating AI as a co-pilot that enhances traditional tools, professionals can drive innovation and efficiency. This session offers participants the knowledge and skills to harness generative AI and LLMs effectively, empowering them to stay ahead in the evolving landscape of work.",2,101 | Cobb Galleria Centre,Operational Excellence Session 6,116
390,6437.0,Evaluating the Financial Returns and Time Savings of Individual Kaizen Ideas in Manufacturing Organizations,Practitioner,"Kaizen, one of the pillars of lean offices and continuous improvement, has been critical to organizational efficiency. It is typically measured in cumulative gains and overall productivity across processes and teams. However, quantifying the specific return on investment (ROI) of individual Kaizen ideas in terms of both financial savings and time efficiency remains underexplored in both research and practice. This paper introduces an innovative approach to Kaizen by examining the financial and time-saving impacts of each improvement idea independently within an established internal tracking system. As a case study, one manufacturing organization’s data related to costs, time savings, and financial benefits associated with each Kaizen have been analyzed, allowing for precise ROI calculations for individual Kaizens. The analysis addresses the question of how small-scale, employee-driven improvements translate into measurable returns. The outcome offers insights into Kaizen's role in value and time efficiency generation at a granular level. It also highlights how even modest time investments can yield substantial value. This study provides leaders with quantitative data to encourage team participation in Kaizen submissions, thereby strengthening the continuous improvement culture within their organizations.",3,101 | Cobb Galleria Centre,Operational Excellence Session 6,116
391,5304.0,HOW TO DETERMINE THE BUSINESS OPERATIONAL IMPROVEMENT LEVERAGES AND ELIMINATE THEM,Practitioner,"The purpose of this paper is to help businesses to figure out where the business operations may have the most leverage and where they may have gaps. This requires a comprehensive assessment by using a practical and beneficial approach and technique for determining the best operational improvement leverages. By the help of this new analysis approach and technique, various dimensions of critical losses affecting improvement targets of the business can be determined clearly and in detail. Moreover, it helps identifying potential operational improvements. Afterwards, moving from real situation to envisioned (target), you should develop solution options in regard to elimination of designated leverages (for fully eliminating the causes of the problem). Accordingly, the potential aspects (with their value and volume drivers; and quantified present and desired situations) and costs of the recommended improvements for each determined problem as a whole or with its individual leverages should be delivered. The content of this paper was drawn from the conclusions obtained from different application environments of the subject issue and by the composition of the cause-and-effect relations of them.",4,101 | Cobb Galleria Centre,Operational Excellence Session 6,116
392,6657.0,Lean Manufacturing from an Environmental Engineering Perspective: A Green Lean Approach,Practitioner,"Green Lean manufacturing integrates environmentally sustainable practices into traditional manufacturing processes to optimize efficiency while minimizing environmental impact. This approach addresses a central challenge in manufacturing: balancing operational efficiency, energy savings, and environmental sustainability. Conventional manufacturing, though often optimized for productivity, can neglect the environmental consequences, including high resource consumption, pollution, and elevated carbon emissions. This project investigates the application of Green Lean principles in a metal hardware manufacturing facility, analyzing the potential for environmentally optimized practices within its main processes: die casting, punching and forming, drilling and machining, polishing, plating, assembly, and final packaging. A critical component of this study is Total Productive Maintenance (TPM), which is used as a framework to optimize equipment performance and reduce energy usage. Additionally, the project explores potential facility layout and routing modifications to improve material flow and minimize waste, enhancing both financial and environmental outcomes. The use of a microturbine for cogeneration of heat and power is also discussed as a significant method for reducing energy costs and environmental impact. The facility aims to improve productivity, boost energy efficiency, and lower its environmental footprint through these Green Lean strategies. By incorporating energy-saving practices, waste reduction techniques, and sustainable energy solutions, the facility not only maintains high manufacturing standards but also aligns with broader sustainability goals. This project illustrates the benefits and feasibility of integrating environmental engineering practices within Lean manufacturing to create a more sustainable, resource-efficient manufacturing environment.",1,101 | Cobb Galleria Centre,Operational Excellence Session 7,117
393,8861.0,"Modeling CONWIP, Kanban, and Input Limiting to Optimize Processes",Academician,"A simple Monte Carlo model of an arbitrary 5-station buffered process with variable inputs and process speeds is described. The model is exercised to demonstrate the behavior of the process with Constant Work in Progress (CONWIP), capacity-limited buffers (Kanban), and limited input (underloaded) process control strategies. The behavior of the model in each case is show to agree with existing literature. The cases are compared to each other and to approximate queue theory results. It is shown that, averaged over large times and many sample runs, the behavior of all approaches are similar and agree with queue theory predictions. The models are used to explore the trade-off between cycle time and throughput. A simple optimization strategy is proposed based on the Lean concept of value to the customer. It is found that an optimum is straightforward to calculate. Based on the agreement with queue theory, an optimum strategy for any of the three process control approaches can be calculate directly without needing to run the model again. It is also found that the optimums tend to be ""soft,"" justifying the use of heuristic or approximate methods to calculate them. On the other hand, the short term behavior (e.g on any given model run) is found to vary widely, pointing out a danger of reacting too quickly to random variations when considering active feedback mechanism. Finally, it is noted that the model is simple enough and easy enough to use that it makes a good educational tool.",2,101 | Cobb Galleria Centre,Operational Excellence Session 7,117
394,6779.0,Finding Preferred Automated Vehicle Driving Styles via Exposure to Various Types of Vehicle Behavior,Academician,"As automated vehicle (AV) technologies advance, drivers will become increasingly able to personalize their interactions with them. One aspect that drivers desire to customize is the driving style of AVs. While previous work suggests that personalization of AVs can enhance driver trust and satisfaction, little is known about how drivers perceive the behavior of AVs and the ways they wish to modify AV driving styles to match their preferences. This study defined four driving behaviors based on two factors: safety (cautious vs. risky) and efficiency (slow vs. fast) to understand how drivers find their preferred driving style. Thirty-two participants completed 10 trials and experienced one of four driving styles in a simulator: very cautious and slow, moderately cautious but fast, very fast and risky, or risky but slow. After each drive, participants provided feedback to the AV to either maintain or change its driving behavior. The success rate of executing this feedback was 80%, and participants could take over control of the vehicle at any time. Overall, drivers preferred a balanced driving style, avoiding too aggressive or too conservative behaviors. Additionally, driver takeover frequency increased when the AV adopted a more aggressive style consisting of high speed and risk. Finally, driver trust in the AV peaked at the balanced driving style, suggesting that neither overly conservative nor overly aggressive behavior maximized trust. These findings can contribute to the development of personalized AV driving style selection methods that prioritize safety and satisfaction.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Automated Vehicle and Human Interaction,118
395,6517.0,Predict and Model Worker Trust for Automated Vehicles in Manufacturing Plants,Academician,"Automated Guided Vehicles (AGVs) have been deployed in numerous industrial settings, streamlining the transportation of parts for manufacturing processes. However, despite their prevalence, these vehicles typically operate separately from human workers due to a lack of trust issue. Therefore, to reach their full potential, AGVs must be assimilated into manufacturing landscapes with humans, where they can sense and respond to workers' trust. This study examined workers’ trust and behavior when interacting with AGVs. We considered three within-subject factors: AGV deceleration rate (0.1 or 0.7 m/s²), AGV approaching direction (N, NE, E, SE, S, SW, W, and NW), and user's expected crossing path (diagonal across an intersection or straight across). Then, a human subject study was conducted to collect behavioral data and self-reported trust levels from 46 participants acting as workers in a virtual reality-based manufacturing plant. For data collection, a VR headset, omnidirectional treadmill, and hand controllers were used to track eye gaze, participant movements, and hand positions, respectively. The data was then processed and analyzed to identify key factors that affect workers' trust. The results showed that an increase in users' deviation from their normal walking speed and expected crossing path corresponds to lower trust levels. Furthermore, an Analysis of Variance indicated that AGV approaching direction and expected crossing path significantly impacted the worker's trust toward AGVs. Finally, we built and evaluated several machine learning models to predict workers' trust. As a future direction, we aim to design trust-aware AGVs.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Automated Vehicle and Human Interaction,118
396,8547.0,Data Acquisition and Processing for the Optimization of an Algorithm for Vehicle Safety Objective Rating Metrics and Injury Severity Prediction,Academician,"Objective Rating Metrics (ORMs), such as CORA (Correlation and Analysis), are commonly used to evaluate the similarity between matched time histories in crash testing. However, no standardized approach exists for using CORA leaving researchers to determine their own parameters and evaluation methods. This subjectivity highlights the need for an optimized, consistent approach to CORA analysis. The goal of this research is to establish a systematic method for processing and analyzing CORA ORM in future studies. Matched time histories from ATDs (Anthropomorphic Test Devices) and PMHS (Post-Mortem Human Surrogates) were obtained from the NHTSA Biomechanics Database. Chest deflection and acceleration were the signals of interest. Injury risk predictions were also analyzed based on these features. Data underwent extensive preprocessing to ensure suitability for further analysis, with crash test reports manually filtered for relevant conditions such as impact site and human surrogate type. Data was compiled into an Excel spreadsheet containing test conditions, occupant details, instrumentation, and other relevant parameters. About 70 matched tests were identified with four to nine datasets per body region. The matched tests were then reorganized by body region into separate Excel workbooks. To automate the analysis and CORA ORM execution, MATLAB (R2024a) was used to develop functions for data truncation, normalization, and automation. The CORAPlus manual provided the basis for the default CORA parameters, with five additional parameter sets drawn from literature to optimize the analysis. This work lays the groundwork for a consistent algorithm linking ORM similarity scores to injury severity prediction for vehicle safety applications.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Automated Vehicle and Human Interaction,118
397,8499.0,Is My Car My Teammate? A Novel Exploration into the Effects of Driver and Vehicle Availability on Teaming with Semi-Autonomous Vehicles,Academician,"Today’s advanced technologies carry more agency and autonomy in their actions, and the interactions between humans and advanced technologies are beginning to resemble more of a teaming relationship. As automated vehicles (AVs) become more intelligent, the concept of teaming with a semi-autonomous AV has gained considerable traction. However, few empirical studies exist that quantify the performance of an AV-(human) driver team. To contribute to the existing work in this area, the purpose of this study was to examine how the availability of both a vehicle and driver agent, or the unavailability of one agent, influenced teaming in driving. Fourteen participants drove a simulated semi-autonomous vehicle where the goals of the vehicle and driver varied. The vehicle’s individual goal was to manage the dynamic driving task. The driver’s individual goal was to complete a work-related document-editing task. Together, the vehicle would ask the driver about potential roadway dangers and the driver would respond to the vehicle’s queries. Their shared goals were to arrive to/from work safely and timely. Overall, introducing availability allowed for function allocations where drivers had individual goals separate from driving, leading drivers to acquire greater completion and higher accuracy on their work-related task when signaling their unavailability. When the vehicle was unavailable, 45% of the time drivers regained control of the vehicle. Drivers believed they were teaming most (79%) when the vehicle asked for their input. Study results provide initial insights into operationalizing the future vision of AVs where safety and driver productivity during commutes are maximized.",4,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Automated Vehicle and Human Interaction,118
398,5159.0,Influence of individual flexibility on the effectiveness of a lumbar-assist exoskeleton in reducing lumbar extensor muscle activity,,"Wearable passive lumbar-assist exoskeleton has emerged as an ergonomics intervention strategy to reduce the risk of low back disorders. While the exoskeleton has been extensively studied, the impact of individual flexibility on its effectiveness is unclear. This study aimed to explore the effects of individual torso flexibility on the effectiveness of a lumbar-assist exoskeleton in reducing erector spinae muscle activity. Fourteen participants, divided into two groups (high, low), were asked to maintain symmetric or asymmetric trunk flexion postures, including asymmetry (0°, 30°) and trunk flexion (20°, 40°, 60°), with and without an exoskeleton system. The erector spinae muscle activity in the contralateral (left) and ipsilateral (right) sides was captured using surface electromyography (EMG). The dependent variable was the reduction in normalized EMG of the erector spinae activity with the use of the exoskeleton. The results revealed a significant interaction between individual flexibility and trunk flexion angle on the contralateral side (p<0.001) and the ipsilateral side (p<0.05). This interaction indicates that the low-flexible group had increased effectiveness in reducing erector spinae activity as a function of trunk flexion angle, whereas the high-flexible group had less effectiveness at 60° trunk flexion than at 20°. Further analysis of the exoskeleton effect showed that the high-flexion group had a different flexion-relaxation response of erector spinae muscles in the investigated 60° trunk flexion postures with and without exoskeleton, resulting in reduced effectiveness. This reduced effectiveness may not lead to the benefit of exoskeleton use in mitigating muscle fatigue at prolonged deeper trunk flexion.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Exoskeleton,119
399,5590.0,The Quasi-Stiffness Control Parameter and Soleus Muscle Activity Relationship During Powered Ankle Exoskeleton Walking,Academician,"Active exoskeletons are driven by controllers, but the precision required to assist humans efficiently is still unclear. Exoskeleton controllers store commands to aid people during walking, and changes in the exoskeleton control parameters, such as quasi-stiffness, can influence gait characteristics and muscle activation. Exoskeleton ankle quasi-stiffness is how much the exoskeleton ankle will rotate when the user moves their ankle and can be defined as the relationship between exoskeleton ankle torque and angle. This study evaluated the relationship between changes in quasi-stiffness coefficient and muscle activation. Twelve participants (6 female, 6 male, average age 23.08 ± 3.50) walked on a treadmill at 1.25 m/s while wearing a bilateral powered exoskeleton (Dephy ExoBoot) at seven quasi-stiffness values. Soleus surface electromyography (EMG) was collected. Since the exoskeleton assists during the stance phase of the gait cycle, data was analyzed during dorsiflexion (35-60% of the gait cycle). Mean peak values were calculated for torque and EMG as the peak value during dorsiflexion for each gait cycle, averaged across each quasi-stiffness value. A linear mixed-model regression model found an effect of mean peak torque on mean peak EMG. It was found as mean peak torque increases due to increased quasi-stiffness, mean peak soleus activity decreases. This result indicates that as an exoskeleton provides more assistance, through increasing the quasi-stiffness coefficient, users benefit from the exoskeleton assistance and reduce their soleus muscle activation. This information can aid the development of new co-adaptive controllers to ensure exoskeletons can enhance or assist user’s capabilities effectively.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Exoskeleton,119
400,6887.0,EMG Analysis of Vertical Ladder Climbing for Active Knee-Based Exoskeleton Control,Academician,"Exoskeleton technology is gaining increasing attention in industrial applications, such as manufacturing facilities and construction sites. Active exoskeletons, equipped with power and motors, offer enhanced performance, providing workers with better protection from injury and improved productivity. However, most current products are primarily designed for repetitive tasks, limiting their adaptability in construction activities that require transitioning between various movement types. Vertical ladder climbing, a common task in construction, demands coordinated lower-limb muscle activation for stability and control, especially as workers shift between climbing, standing, and walking phases. This study examines muscle activation patterns using electromyography (EMG) data during standing, walking, and vertical ladder climbing to establish baseline comparisons for transition movements. EMG data were collected bilaterally from the Biceps Femoris Caput Longus, Tibialis Anterior, and Gastrocnemius Medialis muscles as participants climbed vertical ladders using three movement patterns: alternating legs, right-leg dominant, and left-leg dominant. EMG signals were processed with a 20-450 Hz band-pass filter, and RMS values, normalized by Maximum Voluntary Contraction (MVC), were used to quantify activation intensity. Initial results indicate that phase-specific muscle activation remained consistent across different movement patterns, as shown by two-way ANOVA (Phase x Movement Pattern) with no significant interaction observed. However, certain muscles, such as the left Biceps Femoris Caput Longus, exhibited significantly higher activation during walking compared to climbing and descending (p < 0.001), while the Gastrocnemius Medialis in both legs showed significant differences when standing at the top of the ladder compared to standing at ground level (mean difference = 0.05, p < 0.002).",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Exoskeleton,119
401,6011.0,Evaluating Forecasting Model Accuracy in Predicting Fatigue Progression During Exoskeleton-Supported Tasks,Academician,"Predicting physiological demand trends can play a key role in designing effective health interventions. This study applied forecasting models to estimate the progression of fatigue levels during tasks assisted by an exoskeleton (EXO). Using data from nine participants performing intermittent 45° trunk flexion tasks with and without EXO support, we analyzed perceived fatigue and low-back muscle activity until moderate-to-high fatigue was reached. Two forecasting techniques, Autoregressive Integrated Moving Average (ARIMA) and Facebook Prophet, were utilized to model perceived fatigue levels independently, as well as with the added feature of muscle activity data. Results indicated that simpler, univariate models performed best, with the Prophet model yielding the lowest mean (SD) of root mean squared error (RMSE) across participants at 0.62 (0.24) for EXO-supported and 0.67 (0.29) for unsupported tasks. Temporal fatigue progression was further analyzed up to 20 trials, revealing a ~48–52% greater fatigue slope for tasks without EXO assistance. ARIMA (with muscle activity data) and Prophet showed median fatigue delay benefits of 54% and 43%, respectively. These findings highlight the potential of forecasting models to monitor workforce health, evaluate interventions, and support injury prevention strategies.",4,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Exoskeleton,119
402,6888.0,Isolating the Impact of Masked Perceptual Attacks on Attention in Mixed Reality,Academician,"Mixed Reality has redefined human-computer interaction through its ability to seamlessly integrate both spheres of physical and virtual reality. This has increased its application potential within the context of user training in various spaces, from aviation and automobile to healthcare and rehabilitation. However, the simultaneous comprehension of both realities and applied learned patterns may increase susceptibility to perceptual manipulation, steering users into performing unintended actions thus opening attentional and task performance vulnerabilities. This research aims to explore the interplay between task performance and attention allocation in Mixed Reality environments under perceptual mismatch conditions. We have designed a Mixed Reality setting with eye-tracking assistance that directs users through a set of incentive-driven tasks to assess their degree of attentional and behavioral alignment. Introducing mismatches in the SEEV ( Salience, Effort, Expectancy, Value ) model parameters within our environment design, we extract gaze patterns on pre-defined Areas of Interest (AOIs) in the setting to study attentional variations and the degree of reliance on learned patterns. With our approach, we build a confidence model aimed at improving user resilience to masked perceptual attacks, while drawing guidelines on specific aspects of Mixed Reality world design that impact perceptual awareness and information processing. The study aims to create an ethical framework that can be applied across a spectrum of applications, making for a secure user experience and increasing accessibility to Mixed Reality as a training platform through the optimization of world design, implementation and evaluation.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Extended Reality Session 1,120
403,5071.0,Enhancing Manual Assembly: A Comparative Study of Mixed Reality and Paper-Based Instructions Assessing User Performance and Cognitive Load,Practitioner,"Objective This study evaluates mixed reality (MR) versus traditional paper-based instructions for manual assembly tasks, focusing on task performance, user experience, cognitive load, and simulation sickness. Background Assembly tasks in various industries often rely on paper-based instructions, which can be hard to interpret and prone to errors. Mixed Reality (MR) technology provides interactive 3D cues, potentially reducing cognitive overload. Previous research on MR's impact on efficiency and usability has shown mixed results, indicating a need for further exploration. Method In a between-subjects design, 30 participants will be assigned to MR-based or paper-based instruction groups (15 each) to assemble a LEGO® Monster Truck. Both groups will use instructions designed with cognitive load theory (CLT) principles. The MR group will use a Meta Quest 3 headset with 3D instructions and visual cues, while the paper group will follow printed instructions. Key measures include task performance (completion time and error rate), cognitive load (NASA-TLX), user perception, and usability ratings. ANOVA will be used to analyze and validate the results. Results We hypothesize that participants using MR-based instructions will show faster or similar task completion times, with fewer errors, experience lower cognitive load, report reduced simulation sickness, and demonstrate higher satisfaction than those using paper-based instructions. Conclusion This study investigates the potential of MR technology to enhance task accuracy and user experience in assembly tasks. Application The findings aim to guide the development of MR-based instructional systems in manufacturing and construction, supporting improved efficiency, accuracy, and user comfort in these sectors.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Extended Reality Session 1,120
404,6416.0,"Comparative Study of Text, Video, and Virtual Reality Training for Electric Vehicle Emergency Reponse",,"As the number of electric vehicles (EVs) increases, so does the frequency of related emergencies. More advanced and specialized training for first responders is essential to enhance preparedness, reduce risk, and ensure safety. Traditional text-based training, while informative, often lacks clarity. Video-based methods add visual context, but they still fail to provide an immersive, hands-on experience essential for muscle memory in emergency situations. By comparison, virtual reality (VR) offers a promising solution, providing an engaging, realistic training experience. However, it remains uncertain how different training formats impact learning effectiveness, particularly in high-stress emergency contexts. To address this, we developed a VR system for EV emergency response training and conducted a between-subject experiment (N=36), comparing the effectiveness of three training formats: text-based, video-based, and VR-based (12 participants per group). Results indicated that while VR had the second-highest test scores among the groups, it provided more consistent and stable learning outcomes across participants. This consistency highlights VR’s potential as a reliable format for complex, skill-based training scenarios. Moreover, qualitative feedback showed high acceptance and preference for VR-based training. These findings suggest VR’s potential as an effective, immersive, and widely accepted tool for enhancing the safety and efficiency of EV emergency response training in real-world applications.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Extended Reality Session 1,120
405,5997.0,Impact of Variability of Haptic Feedback in Virtual Reality (VR) during Task Performance.,Practitioner,"Task performance is a significant focus within the Human-Computer Interaction (HCI) domain, particularly with the advent of technologies like Virtual Reality (VR). Understanding how individuals leverage VR for productive task execution is crucial as researchers continually seek ways to optimize human performance in digital spaces. Various studies have delved into factors influencing human performance in VR settings, including the engagement of primary senses - visual, auditory, and tactile (haptic). However, a notable gap exists in understanding how the intensity and availability of haptic feedback via VR controllers impact users cognitively during task performance. This research project addresses factors regarding the intensity and availability of haptic feedback, and how these factors interact to affect the completion of a VR puzzle task. The primary challenge lies in determining the most effective combination of haptic feedback to enhance user productivity and effectiveness. By tackling this challenge, users can achieve greater success in VR-based tasks, especially in less-than-ideal environments outside the lab. Addressing the research gap surrounding the utilization of varied haptic feedback intensities and availability via VR controllers is important to creating user experiences that are both more immersive and more effective. Such improvements can enhance engagement for VR users, particularly those performing in task-critical, cognitive-based applications. The findings from this research illustrated that participants demonstrated higher performance and reduced frustration when exposed to moderate and consistent haptic feedback. The study suggested that while haptics can be beneficial, their usage should be moderated to avoid overwhelming or discomforting users.",4,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Extended Reality Session 1,120
406,8727.0,Applying RASCI Tables to Communicate Allocation in Human-AI Agent Teams,Academician,"Traditional engineering design involves allocating functions to humans or machines, applying well known tools such as Fitts’ List. However, teaming often involves interdependent work in which teammates dynamically support each other as the situation demands. Thus, it is reasonable that the design of human- A rtificially I ntelligent (AI) Agents require the dynamic allocation of functions among AI agents and to human operators within human-AI Agent teams. This research discusses the fallacies of traditional function allocation which must be understood and avoided during human-AI Agent team design. We then discuss the understanding of work using a standard information processing model and application of a project management framework, referred to as R esponsibility, A ccountability, S upportability, C onsulting, and I nformation (RASCI) tables, to communicate static, and potentially dynamic, function allocation during the design of human-AI agent teams. This approach permits functions to be allocated among agents in human-AI agent teams, even when these teams include multiple human and AI agents. The paper will illustrate an approach for investigating and capturing the allocation in a Model-Based Systems Engineering framework. Furthermore, we will demonstrate the integration of the RASCI table and framework with a coactive design technique, referred to as Interdependence Analysis. The final approach illustrates the integration of these tools to support human-system interface design during the design of human-AI teams.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Human and AI,121
407,5692.0,Teaming Humans with Generative AI for Two Simulated Medical Emergencies: An Experimental Study,Academician,"Despite rapid adoption of generative AI, current knowledge regarding its role as a teammate during safety-critical events such as medical emergency remains limited. Thus, this study examined processes of human-AI teaming in two medical emergencies, choking and bleeding. Especially, the current study compared human-human teaming (HHT) and human-AI (ChatGPT) teaming (HAT) in terms of task performance, team trust, self-efficacy, and perceived workload. A between-subject experiment was conducted for 57 participants (31 for HHT and 26 for HAT). Analysis of variance was conducted to identify differences between two teaming conditions. Dependent measures include task completion time, NASA TLX, Global Team Trust, and Generalized Self-Efficacy. Results show a significant main effect of task type on task completion time, namely, choking task took significantly shorter time than bleeding task ( F (1,55)=86.012, p <0.001). Also, participants in HAT completed both tasks faster than those in HHT ( F (1,55)=4.774, p =0.033). Results show a significant interaction effect between task and teammate type on NASA TLX ( F (1,53)=9.594, p =0.003). Participants' team trust was significant lower on AI teammate than on human teammate ( F (1,54)=10.955, p =0.002). No significant effect was found for teammate type on Generalized Self-Efficacy ( F (1,55)=3.811, p =0.056). Findings indicate that AI provided effective support for medical emergency without increasing workload and lowering self-efficacy. Despite benefits of AI for task performance, lower trust on AI warrants further investigations on other factors that influence human trust on AI.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Human and AI,121
408,6899.0,Trust Calibration in AI: The Role of Transparency and User Performance,Academician,"A pilot study was conducted to investigate whether trust and reliability in an object detection system could be predicted by factors related to system transparency, user performance, and familiarity with AI technology. We examined the impact of perceived transparency—measured by the total number of clicks to access more information about how the system works and time spent on task—alongside the number of correct responses, AI knowledge, object detection AI familiarity, and perception of AI on users’ trust and reliability judgments. Twenty-seven undergraduate students interacted with an object detection system simulating AI functionality with varying interactive explainability. A multivariate analysis of variance (MANOVA) assessed the predictive value of these factors on trust and reliability outcomes. The omnibus MANOVA tests were insignificant for all predictors, indicating that the predictors did not collectively explain variance in trust and reliability. Correlational analysis revealed a significant negative correlation between accurately calibrated trust and the number of correct responses ( r = -0.45, p < .05), suggesting that as participants performed better, their trust was less accurately calibrated to the system’s performance. In other words, participants with higher accuracy tended to have trust levels that did not appropriately match the system's actual performance. Despite the limited sample size, these results highlight the need for further research to understand factors that influence trust in AI systems. Future studies with larger samples should investigate these dynamics to optimize explainability and foster appropriate levels of trust in such systems.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Human and AI,121
409,6470.0,Assessing Cognitive Workload in Dentistry,Academician,"In dentistry, every decision, movement, and interaction can impact patient care and clinician well-being, making it one of the most cognitively demanding professions in healthcare. This study presents a systematic literature review of cognitive load measurement methods within the framework of cognitive ergonomics. It evaluates their applicability in dentistry, where these methods remain underexplored. The review identified a range of subjective and physiological methods used to measure cognitive workload, such as the NASA-TLX, heart rate variability (HRV), and electroencephalography (EEG). Research suggests that the subjective tools are widely used due to their ease of implementation, while physiological measures provide more precise, objective data on mental load. In addition to the literature review, a survey informed from the findings, as well as elements of the NASA-TLX was administered to practicing dentists to identify specific cognitive load stressors in dental practice. It focuses on the mental challenges they face, including task complexity, decision-making, mental fatigue, and the integration of new technologies caused during routine procedures. The findings highlight critical areas where ergonomic interventions could reduce cognitive strain and enhance overall performance in dentistry. This study positions dentistry within the cognitive ergonomics field, highlighting its’ unique demands and laying the groundwork for possible improvements in the industry.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Cognitive Workload,122
410,6668.0,Cognitive Workload in Drone Operations: A Literature Review and Conceptual Framework,Academician,"The rapid advancement and increasing application of drone technology have raised concerns about the cognitive workload faced by operators, which can impact performance and safety. An extensive literature review was conducted on unmanned aerial vehicle (UAV) operation, operators’ workload assessment, and human factors issues. It identifies and synthesizes key factors that influence cognitive workload in drone operations and investigates the impact of cognitive workload on operator performance. The researchers propose a conceptual framework to guide future research aimed at reducing cognitive workload for drone operators, enhancing their efficiency and performance. The review categorizes influencing factors into working conditions (e.g., task complexity, number of drones, level of automation, environmental stressors) and individual operator characteristics (e.g., experience, demographics, personality traits, personal innovativeness). The cognitive workload during drone operation, caused by above mentioned factors, can be assessed through self-reporting mental workload surveys and objective measures for physiological stress. Finally, the framework examines the impact of cognitive workload on operator performance, focusing on key outcomes such as decision accuracy, response time, and situational awareness. This conceptual framework offers a structured approach to understanding the dynamics of cognitive workload in UAV operations, providing insights for developing human factors interventions that reduce workload and enhance operator effectiveness and safety in complex, high-risk environments.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Cognitive Workload,122
411,6397.0,"Smart Sensors, Smarter Workplaces: Multi-Modal Cognitive Load Assessment in Industry 4.0",Academician,"Cognitive load assessment is essential in manufacturing, where complex, repetitive tasks on assembly lines can affect operator safety, mental health, and overall well-being. This study presents a multi-modal data fusion approach for assessing cognitive load in sequential assembly tasks. It combines physiological data from the Empatica E4 wearable device and off-the-shelf biometric sensors with subjective tools, including the NASA Task Load Index (NASA TLX), and behavioral metrics. Physiological data is collected while participants are performing tasks of varying complexity across different stations on a lab-based assembly line, enabling the collection of real data. Half of the participants receive paper-based instructions, while the other use photo-based instructions on tablets at each station. The Empatica E4 collects six different physiological metrics, including heart rate variability, electrodermal activity, and skin temperature, which are utilized to objectively quantify cognitive load. Subjective data are collected using the NASA TLX and additional questionnaires to assess perceived mental workload. Behavioral data, including task completion time and error rates, provide insights into the relationship between cognitive load and performance. One hypothesis suggests that photo-based instructions will reduce cognitive load by enhancing clarity and processing efficiency, leading to faster task completion and fewer errors. The primary contribution of this study is the assessment of cognitive load using multi-modal data, integrating both subjective and objective measures. Additional contributions include validating wearable sensor data in manufacturing settings and comparing the impact of different instructional formats on operator well-being. This research highlights the need to examine cognitive load to create human-centered workplaces.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Cognitive Workload,122
412,6650.0,Application of Cognitive Load Theory in Designing Instructions for Assembly Tasks,Academician,"Assembly tasks often demand significant cognitive resources, as users must follow complex instructions while maintaining focus on accuracy and efficiency. Cognitive Load Theory (CLT) offers a framework for designing instructions that reduce mental load by structuring information in a user-friendly way. The objective of this study is to design assembly task instructions based on CLT principles to reduce cognitive workload. By focusing on CLT guidelines, this study aims to design an effective instructional framework that enhances user experience and minimizes cognitive workload. A LEGO model has been selected as the assembly task prototype for this study. This study has applied CLT principles, such as segmenting tasks, minimizing extraneous cognitive load, and using visual aids, to develop optimized assembly instructions. To evaluate the effectiveness of these instructions, we will conduct an experiment with 10 participants, assessing their cognitive workload using the NASA Task Load Index (NASA-TLX). We will compare NASA-TLX scores between participants following the standard instructions and those using the CLT-based instructions. We expect that the instructions designed with CLT principles will significantly reduce cognitive workload compared to standard instructions. By minimizing unnecessary mental demands, these optimized instructions aim to make the assembly process smoother and less mentally demanding for users. This study demonstrates the potential of CLT in designing user-centered assembly instructions. The findings could benefit both researchers and industry professionals by providing a framework for instructional design in various assembly contexts. This approach may improve assembly task efficiency and satisfaction.",4,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Cognitive Workload,122
413,5921.0,A scoping review of soft robot user interfaces,Academician,"Soft robots are made from compliant materials, showing great promise in rehabilitation, surgery, search and rescue, environmental inspections, and more, due to their advantageous properties such as flexibility and adaptability. To ensure safety during human-robot interaction (HRI), it is essential to design and evaluate intuitive and effective user interfaces and interaction strategies. However, the landscape of interfaces implemented to control the movement and behavior of soft robots is not well understood. The objective of this scoping review was first to identify what user interfaces control soft robots and then understand how the type of soft robot and its characteristics (e.g., actuation method, control method, motion capabilities) influence the type of user interfaces that are implemented. Following the Preferred Reporting for Systematic reviews and Meta-Analyses Extension for Scoping Reviews (PRISMA-ScR), we searched the literature for papers published from 2010 to 2024 in databases including Scopus, Web of Science, Compendex and Inspec via the Engineering Village platform, APA PyschInfo, and Ergonomics Abstracts. We included papers that described both a soft robot controlled by humans and a user interface or interaction strategy, resulting in 175 included papers.",1,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Interface Design and Usability,123
414,8964.0,Eye-Tracker Assisted Usability Evaluation of a Hunger-Relief Visualization Library,,"In recent years, major hunger-relief organizations have increasingly recognized the importance of quality data visualizations in enhancing evidence-based decision-making processes, particularly for their potential to improve understanding of the complex relationships between food insecurity and socioeconomic factors. While quality data visualizations can greatly reduce the cognitive and intellectual burden required to grasp information for decision-making, poorly constructed visualizations can do the opposite. Interviews conducted with various hunger-relief organizations revealed that resource constraints do not allow for a sufficient data science department, especially amongst those serving smaller communities, indicating a need for a tool facilitating the creation of data visualizations . Thus, an application localizing quality data visualizations for both reference and replication by hunger-relief employees was developed. Given the lack of specialists at hunger-relief organizations’ disposal, it is imperative that this application is highly usable even among non-expert personnel. To assess the usability of this application, an eye-tracker-assisted usability evaluation of a later-stage prototype was conducted. The results, derived from both quantitative and qualitative data, revealed usability concerns and identified design recommendations. Findings of the study provide valuable insights into user interaction patterns and preferences and will inform future application development proceedings to better serve the needs of domestic hunger-relief organizations.",2,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Interface Design and Usability,123
415,6822.0,Designing Provider-Patient Family Conversation Spaces in the Neuro-ICU,Practitioner,"Provider-Patient Family (PPF) conversations in the form of goals-of-care, transition-of-care or end-of-life meetings require families or surrogates of patients to make critical decisions on behalf of their loved ones, impacting a number of outcomes, including costs of care, type of intervention and the patient’s length of stay. These conversations involve various challenges for care providers, including coordinating the meetings, disseminating necessary information to family surrogates and care team members, and anticipating diverse family needs. The study uses the Systems Engineering Initiative for Patient Safety (SEIPS) 2.0 framework to identify how multiple systemic factors influence the effectiveness of the meetings, with a specific emphasis on coordination and decision-making. To this end, thirty meetings between providers and patient families will be observed in the Neurological ICU of a large hospital system in South Carolina. Following the meetings, family members will be asked to fill out a survey to gauge their satisfaction with the conversation. The provider team will also be asked to fill out two surveys to assess their perceived quality of communication, and their physical and cognitive workload. In addition, semi-structured interviews will be conducted with care providers and family members to delve deeper into the challenges and facilitators of effective coordination and decision-making. Data will be analyzed qualitatively and quantitatively to develop insights that will inform a multi-user-centered design framework to better support planning and coordination of PPF conversations in the ICU.",3,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Interface Design and Usability,123
416,6821.0,Heart Rate Variability Feature Selection for Mental Stress Detection,Academician,"The prevalence of stress-related disorders necessitates robust detection methods for effective management and intervention. This study explores the utility of heart rate variability (HRV) as a biomarker for mental stress detection, using data from the WESAD dataset collected from 15 individuals in a controlled lab setting. We evaluated 93 HRV features derived from electrocardiogram signals to differentiate between stress and non-stress states. Our methodology involved preprocessing, feature computation, and three feature selection approaches—Embedded, Filter-based, and Wrapper—to identify the most relevant HRV metrics. Results indicate that two HRV features, 'HRV_MCVNN' and 'HRV_LFn', consistently distinguish between stressed and non-stressed states across multiple classifiers. Using Recursive Feature Elimination combined with Nested Leave-One-Subject-Out Cross-Validation optimization, we achieved the highest Macro-F1 score of 0.76. These findings underscore the potential of targeted feature selection in enhancing the efficiency of stress detection models, especially for multidimensional datasets containing numerous potentially irrelevant features.",1,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Human Behavior Monitoring and Assessment,124
417,6826.0,Exploring the Impact of Task Complexity on Human Physiological Behavior in Virtual Reality Manufacturing Systems,,"Understanding the relationship between task complexity and human physiological responses is crucial for optimizing performance and well-being in manufacturing environments. This study explores the impact of task complexity on participants’ electrodermal activity (EDA), which can be used as a measure of human stress, in a virtual reality (VR) manufacturing assembly process. In one of the assembly tasks in the virtual factory, participants perform twelve different tasks with different levels of complexity. Participants’ physiological measures were recorded through wearable sensors. For each of the twelve tasks, EDA features such as the number of peaks, peak amplitude, and peak area were calculated for both the phasic (skin conductance response) and tonic (skin conductance level) components. A machine learning model was developed to predict task complexity levels based on the extracted EDA features. This research highlights the potential for VR to be used as a tool for stress management and assessment in manufacturing environments.",2,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Human Behavior Monitoring and Assessment,124
418,6537.0,Patient Trust in Digital Health Systems: A Human-Centric Analysis of Key Interaction Drivers,Academician,"With the integration of digital platforms into healthcare, establishing patient trust in these systems is crucial for enhancing engagement, adherence, and satisfaction. As digital health records, patient portals, and online health resources become increasingly prevalent, challenges emerge around usability, information transparency, and communication quality—factors that are essential to building and maintaining trust. Existing research has yet to comprehensively address how these factors interact to shape patient trust within digital healthcare contexts. Therefore, this study aims to address this gap by exploring factors across three categories: communication quality , information accessibility , and patient confidence in navigating health resources . Using data from the Health Information National Trends Survey (HINTS), we analyzed 4,504 respondents after excluding missing or invalid data from the initial 6,252 responses. Variables in this study include doctor-patient communication quality, ease of understanding health information, confidence in finding online health resources, perceived consistency in health recommendations, understanding of medical statistics, perceived quality of healthcare and demographic factors. To examine the complex relationships among these variables, we apply Hayes’ PROCESS modeling to test a multi-mediator model, identifying key pathways through which communication quality, usability, and information consistency influence patient trust in digital health settings. This research contributes to human factors and ergonomics by identifying factors that support patient-centered, trustworthy digital health systems. These findings aim to guide healthcare providers, digital system designers, and policymakers in enhancing patient trust across diverse digital platforms, such as wearable health devices, telemedicine applications, and patient engagement portals that prioritize patient confidence and effective health information accessibility.",1,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Experiences and Trust in Technology,125
419,6645.0,Empowering Children with Medical Complexity: The Role of Technology in Bridging Education and Healthcare,,"Children with medical complexity (CMC) are children and youth with multiple health conditions that require specialized medical care. They represent a unique and diverse patient group, ranging from congenital disorders and chronic illnesses to developmental disabilities and rare genetic conditions. In our study, we defined CMC as children aged 21 or below who have at least three medical conditions. CMC often face significant challenges in their education due to a lack of support systems designed to address their unique and diverse needs. As technology continues to shape both education and healthcare, this study addresses an important intersection of the two systems. It aims to explore the role of technology in education and healthcare for CMC. We recruited 12 participants from the SPAN Parent Advocacy Network who worked with parents of CMC. Interviews were conducted via Zoom, lasted approximately 45 minutes, and were audio recorded. Semi-structured, in-depth individual interviews were performed with each participant. Participants were asked to share their perspectives on challenges and care coordination between schools, parents, and healthcare providers. We conducted a thematic analysis to extract insights from the interview transcripts. The finding involved the challenges of virtual learning for CMC during COVID-19, and the suggested technologies to improve care coordination between healthcare providers, schools, and parents. The results presented the identification of patterns and key insights into how technology is used to support medical and educational needs. It also highlighted the challenges families face in ensuring continuity of care and communication between schools, healthcare providers, and families.",2,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Experiences and Trust in Technology,125
420,6311.0,Assessing Trust Dynamics and Performance Efficiency in Multi-Cobot Human Collaborative Assembly in a Simulated Manufacturing Environment,Practitioner,"This research investigates the interplay between trust and performance in human-cobot collaboration during manual assembly tasks. By integrating two collaborative robots (FANUC CRX-10iA and UR10e) in a simulated manufacturing workflow, the study focuses on how varying trust levels impact task efficiency and worker decision-making when alternating between human-only and cobot-assisted workflows. Unlike traditional survey studies, this research prioritizes understanding the psychological and behavioral aspects of trust, using physiological monitoring and trust surveys to map fluctuations in trust during simulated cobot errors and task transitions. The results are expected to highlight that trust development is not only pivotal for improving task efficiency but also for enhancing safety and error management. By systematically introducing failures and tracking participants' responses, the study intends to provide insights into the thresholds of human trust in cobots, particularly when faced with unexpected behaviors. This approach differs from our previous study that primarily emphasized productivity, instead offering a comprehensive view of how trust dynamics can influence the success of human-robot collaboration in complex assembly environments. The findings serve as a blueprint for future studies aiming to balance human factors with technological advancements in manufacturing systems.",3,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Experiences and Trust in Technology,125
421,5852.0,Predicting Physical Fatigue for Healthcare Workers During Mass Decedent Handling Using a Wearable Sensor-Based Machine Learning Approach,Academician,"Work-related injuries, such as musculoskeletal disorders (MSDs) among healthcare workers lead to substantial physical, mental, and financial costs with increased turnover rates. During the COVID-19 pandemic, these issues further intensified, with healthcare workers frequently engaging in prolonged mass decedent handling tasks. Performing these tasks caused the workers to exceed their physical limits, leading them to physical fatigue. Physical fatigue is one of the major factors and significant precursors of workplace injuries, underscoring the need for effective monitoring. By classifying and predicting physical fatigue automatically in real-time, we can proactively identify at-risk workers before fatigue leads to potential accidents. To address this issue, the study conducted a controlled laboratory experiment simulating decedent handling tasks. Twelve participants performed standardized logrolling tasks on a mannequin. Physiological signals, including heart rate (HR), skin temperature (ST), and electrodermal activity (EDA), were collected using a wristband-type wearable sensor. The Borg’s RPE scale (6-20) was used to assess subjective fatigue, categorizing it into four distinct levels. Subsequently, three machine learning (ML) models, namely Random Forest (RF), Support Vector Machine (SVM), and Artificial Neural Network (ANN), were developed to classify and predict these physical fatigue levels. The results showed that the ANN model achieved the highest accuracy rate at 80%, followed by the RF model at 78% and the SVM at 75%. Future research could prioritize the utilization of robust datasets to achieve higher accuracy in model performance. This study contributes to the body of knowledge on safer working conditions for healthcare workers in global health crises.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Physical Ergonomics #1,126
422,5059.0,Lumbar spinal creep as a function of individual flexibility,Academician,"This study aimed to explore the effects of lumbar flexibility and hamstring flexibility on the creep deformation of viscoelastic lumbar tissues. Sixteen human subjects, classified into two groups (GROUP: high flexibility, low flexibility) for each test, were asked to perform four 30-minute protocols combining two trunk flexion postures (POSTURE: Near-full, Full) and two load cycle duration (CYCLE: Short-cycle, Long-cycle). Before and after the protocols, the subjects were asked to perform controlled trunk flexion-extension motions to capture the change in peak lumbar flexion angle and the change in lumbar flexion angle at which erector spinae muscles achieve flexion-relaxation. Two ANOVA models were employed, once for the 2×2×2 design with lumbar flexibility and once for the 2×2×2 design with hamstring flexibility. Analysis of lumbar flexibility and hamstring flexibility individually revealed significant interactions between GROUP and POSTURE on the lumbar spinal creep. The simple effects analysis, sliced by GROUP, showed that low-flexible subjects exhibited significantly greater lumbar spinal creep responses in the Full condition compared to the Near-full condition, while high-flexible subjects were unaffected by POSTURE. The analysis of lumbar flexibility also revealed a significant interaction between GROUP and CYCLE on the lumbar spinal creep. The simple effects analysis, sliced by GROUP, showed that high-flexible subjects exhibited significantly greater lumbar spinal creep responses in the Short-cycle condition than in the Long-cycle condition, while low-flexible subjects were unaffected by CYCLE. These results suggest individual flexibility can play an important role in passive lumbar tissue loading with different trunk flexion postures and load cycle durations.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Physical Ergonomics #1,126
423,6618.0,Exploratory Ergonomic Risk Assessment Using Smart Insoles and Computer Vision,Academician,"Traditional observation-based ergonomic assessments, like the REBA (Rapid Entire Body Assessment), remain effective tools for risk assessment in the workplace. Existing automated methods that use IMU sensors or cameras/computer vision techniques to capture human motion have facilitated data collection but may not be ideal for various applications. For example, IMU signals can be easily distorted in the presence of metallic objects or electromagnetic fields, and computer vision techniques require workers to remain in a relatively fixed area. An emerging non-obtrusive tool that allows workers to remain mobile is the use of pressure insoles. In this exploratory study, we propose a novel approach that combines smart insoles with computer vision to assess ergonomic risk in a minimally invasive manner. Initially, we use computer vision algorithms to capture posture data, which provides the input needed to obtain REBA scores, serving as ground truth for training our model. Simultaneously, we collect plantar pressure data from smart insoles embedded with 16 sensors per foot. Our methodology aims to map the pressure distribution data from insoles onto a risk of injury, potentially enabling risk assessment based solely on plantar pressure distribution. We will use 60% of the data for training our machine learning model, 20% for validation, and apply k-fold cross-validation to refine the model. The remaining 20% will be reserved for an independent assessment of the model’s performance. This project may contribute to developing a less intrusive, data-driven model for injury risk prediction, facilitating field-based risk assessments of injury.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Physical Ergonomics #1,126
424,6644.0,User-Centered Evaluation of VR Training for Hybrid Manufacturing: A Cognitive Walkthrough Approach,Academician,"Virtual reality (VR) training modules offer immersive and interactive learning experiences. They are especially useful in complex fields like hybrid manufacturing, where detailed procedural knowledge is essential. Identifying user requirements early in development is essential to create cost-effective, user-centered training modules. The cognitive walkthrough is a structured usability evaluation approach that uncovers design flaws before broader implementation. This study aims to evaluate the usability of a VR-based hybrid manufacturing training module using cognitive walkthroughs. The study is focused on understanding user interactions and identifying potential design issues for each task involved in a metal repair process. A prototype VR training module was created in Unity for the Meta Quest app. Eight evaluators performed cognitive walkthrough sessions using this prototype. Data were also collected using task-based questions grounded in Nielsen heuristics and design principles to assess usability, identify design flaws, and observe user performance. Participants used the think-aloud method, providing real-time feedback. Early findings highlight key user requirements and usability issues in the initial stages of VR module development. After conducting this cognitive walkthrough, numerous design issues and usability problems have been identified, providing critical insights for enhancing the VR training module. These improvements will contribute to a more effective and user-friendly learning experience, ultimately supporting skill development in hybrid manufacturing and similar complex fields.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Human-Centered Approach,127
425,6615.0,Using Electroencephalography to Understand Learning Engagement with User-Centered Human-Computer Interaction in a Multimodal Online Learning Environment,Practitioner,"Multimodal learning environments (MMLA) use visual, auditory, and physical interactions to improve engagement in learning tasks. A recent study by Ma et al. (2023) demonstrated how biosensors such as GSR (Galvanic Skin Response), eye tracking, and facial expressions can track emotional and cognitive engagement. Building on that work, we are incorporating EEG (electroencephalography) data, focusing on Frontal Alpha Asymmetry (FAA), which measures differences in activity between the left and right prefrontal cortices. FAA is linked to engagement, with greater left-frontal activity associated with goal-directed behavior and positive emotional states. A recurring challenge in FAA research is inconsistent results, due to weak stimuli or overly smoothed EEG data that obscure transient patterns. To address this, we use wavelet transforms, which preserve both temporal and frequency details. This lets us detect subtle changes in alpha power, which is inversely related to cortical activity and often reflects changes in engagement, while linking these shifts to trends in time-synchronized biosensor data. We are exploring FAA analyses in 4 participants during a learning task in Quality Trainer (Minitab), and hypothesize that the stimuli will reliably trigger FAA shifts, aligning with eye tracking and facial expression responses. By integrating wavelet-enhanced EEG analysis with time-synchronized biosensor data, this approach offers a better understanding of engagement and cross-modal patterns in real time. These findings have significant implications for developing adaptive learning systems and human-computer interaction models. Furthermore, this work highlights the importance of rigorous, detail-preserving analysis to improve the accuracy and reliability of engagement metrics in multimodal environments.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Human-Centered Approach,127
426,6942.0,Addressing Information Needs for Human Sensemaking in Evolving Aerospace Environments,Academician,"Both aviation and spaceflight environments are approaching inflection points in the development of new systems capabilities and mission operations profiles affecting human operators in both ground-based and crewmember operations. Operational hazards such as environmental conditions and other aerospace entities (including non-human vehicles) challenge crewmember sensemaking and distributed expertise for safe operations. The research presented here examines information presentation and operator sensemaking in both mission planning and real-time mission operations conditions. In the aviation sector, pilots must content with dynamic weather conditions, the increase in aviation traffic, and the requirements to understand new sources of weather and status information during low altitude operations. In spaceflight, increasing density of operations and planned entries in low earth orbit through cislunar and lunar surface operations require new presentation of status and uncertainty of situation awareness for other vehicles and space debris. Our research addresses the development of tools and assessment of pilot sensemaking and situation awareness in these increasingly complex settings. A variety of desktop, small device, and even virtual reality interface designs are being considered to understand how, what, and when pilots or astronauts can make sense of dynamic information sources, and how these information sources support or distract from decision making and action execution tasks. System and interface design elements, and performance evaluation studies, are considered with data collection from domain experts, including pilots, astronauts, and mission operations personnel to inform future system development efforts.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Human-Centered Approach,127
427,5664.0,Cooling Innovation for Shipbuilder Safety,Practitioner,"Around the world, shipbuilders work in extreme environments, constructing vessels that are essential to national security and global stability. Rising temperatures pose an increasing threat to their safety and productivity, emphasizing the urgent need for advanced protective solutions. To address this challenge, Mississippi State University’s Athlete Engineering Institute partnered with Ingalls Shipbuilding to deploy CoolMitts—a portable device that rapidly cools core body temperature by siphoning heat from the body from the palms of workers’ hands through innovative water circulation technology. This case study explores the journey from concept to implementation, revealing how CoolMitts have transformed safety practices and improved working conditions for shipbuilders at Ingalls to provide an efficient, effective, immediate intervention for heat illness and injury. Along the way, valuable lessons emerged, highlighting the importance of adaptability, collaboration, and user-centered design in deploying wearable technology in high-stress industrial settings. CoolMitts exemplify a proactive approach to occupational safety, showcasing how industry-academic partnerships can drive effective solutions to global challenges. By focusing on the needs of these “industrial athletes,” this presentation offers insights into cutting-edge innovations, underscoring the broader benefits of prioritizing worker protection across industries worldwide.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Safety,128
428,5960.0,Assessing the Impact of Flash Flood Risk Communication on Drivers,Practitioner,"Driving during floods poses significant risks to drivers, often leading to dangerous and potentially fatal situations such as drowning, loss of control, and vehicle damage. To reduce the risk of flash floods while driving, this study evaluates various risk assessment tools and seeks to understand driver preferences for route selection. The research tested three methods of communicating flood risk: providing a percentage likelihood of flooding, describing the risk, and recommending alternative routes. The participants included a diverse group of drivers to ensure a representative sample. The survey explored drivers' decision-making processes when faced with potential flash flood scenarios. It included quantitative and qualitative questions to gather comprehensive data on driver preferences and perceptions of the different risk communication methods. The results were analyzed using statistical techniques such as ANOVA and regression. Findings indicate that drivers are more likely to choose safer routes when recommended alternatives are offered, even if these routes are longer. The results have significant practical implications for future research and policy decisions, as they underscore the importance of clear and actionable guidance in helping drivers avoid hazardous conditions and enhancing road safety during flash floods.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Safety,128
429,6374.0,Challenges in implementing stop-work authority: Findings from interviews with offshore workers,Practitioner,"Deepwater Horizon disaster that occurred in 2010 has mandated new offshore safety programs, such as stop-work authority (SWA), ultimate work authority (UWA), reporting unsafe conditions, and employee participation, for energy activities in the US Outer Continental Shelf. Although these safety programs were intended to facilitate the identification, communication, and reduction of offshore risks, actual implementation of the programs remains limited. To address this problem, the current work aims to identify barriers against full implementation of the offshore safety programs. The research team has conducted 17 virtual one-on-one interviews with offshore workers, supervisors, and managers from multiple offshore energy companies. The interviewees were asked questions regarding the nature of their offshore activities, overall experience with the safety programs, reasons behind stop-work, factors that influenced their implementation of SWA, or lack thereof, and recommendations for future improvements. Findings from qualitative data analysis include offshore energy workers' fear of delaying production by stopping work, fear of getting reprimanded, lack of empowerment for SWA, and conflicts in stop-work process. In addition, participants indicated that the safety reporting process was cumbersome, requiring excess time to fill out the reporting form and that contractors face discrepancies of SWA, UWA, and safety reporting policies and processes between companies they work for. Further, participants suggested recommendations such as anonymous reporting system and strong support and empowerment from the management. These findings reveal limiting factors of the offshore safety programs and thus inform the development of future interventions to further improve offshore safety.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Safety,128
430,6552.0,Effects of virtualization in the evaluation of risk and complexity,,"This paper examines the relationship between complexity and risk for real-money poker players in both virtual gaming platforms and traditional casino environments. Although the rules and structure of the game remain consistent in both game contexts, players respond to risk and uncertainty in different ways depending on the setting in which the game takes place. The analysis involves an assessment of numerous poker hands across both virtual and casino games and the player responses observed in each setting. Each poker hand was evaluated according to two dimensions: risk, quantified by the probability of a successful wager, and complexity, quantified by the number and type of potential opponent outs that must be accounted for to determine a wager. The research findings suggest that environmental factors in virtual settings may influence risk perception and decision-making processes. These insights may help contribute to a broader understanding of how virtual platforms may impact the behavior of users, with implications for designing systems where risk-based decision-making and behavioral consistency are important.",4,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Safety,128
431,6473.0,Developing Affordable Tactile Gloves for Real-Time Musculoskeletal Strain Monitoring in Heavy Lifting Using Machine Learning,Academician,"Preventing workplace injuries caused by musculoskeletal strain during heavy lifting is crucial, as such injuries impact both worker health and organizational efficiency. Effective monitoring tools, such as tactile gloves, are needed to assess exertion levels and prevent overexertion. However, the high cost of commercial tactile gloves limits their accessibility in many industries, creating a need for affordable alternatives. To address this gap, this research develops a cost-effective prototype that combines force sensitive resistor tactile gloves with electromyography sensors to measure applied pressure and muscle activity in real time. The system uses bluetooth low energy technology to transmit data wirelessly, enabling continuous monitoring of fatigue and exertion levels without hindering worker mobility. This prototype will be tested in simulated lifting and carrying tasks to gather data on pressure distribution and muscle activation patterns, which will then be processed through a machine learning model to detect patterns indicative of injury risks. The machine learning component enables real-time assessment and supports predictive insights that can prompt timely ergonomic adjustments and interventions to enhance safety. By making advanced ergonomic monitoring more affordable and accessible, this research contributes a scalable tool for industries with high physical demands, such as manufacturing, logistics, and construction. Ultimately, this solution aims to advance occupational health by reducing injury risks and supporting data-driven ergonomic improvements, aligning with the broader goals of human factors and ergonomics in enhancing worker well-being and system performance.",1,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Machine Learning in Human Systems,129
432,5341.0,Predictive Modeling of Joint Angles Using Machine Learning: A Comparative Study of Random Forest and Support Vector Machine,Academician,"In biomechanics, accurate prediction of joint angles under varying conditions is essential for the optimization of rehabilitation protocols, orthopedic design, and athletic performance. Joint movements are inherently complex, which are influenced by various external factors and the specific joint being observed. Previous studies have employed machine learning techniques, such as Random Forest (RF) and Support Vector Machines (SVMs), to accurately predict joint angles under conditions of simple movements and gait analysis, which used data from wearable sensors such as Inertial Measurement Units (IMUs) and Surface ElectroMyoGraphy (SEMG). While these models have been effective in predicting joint angles in real-time applications and movement analysis, no prior research has focused on the prediction of joint angles under varying brace conditions, such as unbraced, knee brace, and ankle brace. This limits the understanding of how external supports affect joint movement dynamics. To address this limitation, this study investigates the use of RF and SVM in predicting joint angles under three specific conditions: unbraced, knee brace, and ankle brace. Feature engineering techniques are applied to transform the sequential data; the models are evaluated based on accuracy, robustness, and computational efficiency. The results will offer insights on the impact of braces on joint movement, which contribute to the literature on machine learning in biomechanics. This research has practical implications for rehabilitation, the design of wearable orthopedic devices, athletic training, which advances the predictive modeling of joint biomechanics.",2,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Machine Learning in Human Systems,129
433,6480.0,Optimized Machine Learning Approach for Assessing Impact of Back-Support Exoskeletons on Muscle Strain and Fatigue in Manual Material Handling,Academician,"This study evaluates the efficacy of back-support exoskeletons in alleviating muscle strain and fatigue during physically demanding manual material handling tasks, addressing a critical need in occupational ergonomics. Musculoskeletal injuries due to repetitive strain are common in industries reliant on manual labor, affecting worker health and productivity. Back-support exoskeletons, as assistive devices, offer potential benefits for reducing physical demands on workers, but their real impact on muscle activation and fatigue requires further investigation. To quantify these effects, we collected electromyography (EMG) data from five participants as they performed lifting and carrying tasks, both with and without exoskeleton support. Wearable EMG sensors attached to seven muscle groups in the lower limbs and back provided detailed data on muscle activation and exertion patterns. Data from these sessions was processed using machine learning models optimized through feature extraction and parameter tuning, allowing for precise analysis of how exoskeleton use influences muscle strain. Our initial findings indicate that exoskeletons may significantly reduce muscle strain and lower fatigue levels, potentially decreasing injury risks and enhancing long-term worker well-being. This machine learning approach enhances predictive accuracy and provides interpretable insights into the biomechanical effects of assistive technology, offering a scalable assessment tool for workplace safety. By integrating advanced data analytics with wearable sensing technology, this research contributes to human factors and ergonomics by providing actionable insights for ergonomic interventions. It supports the adoption of cost-effective, data-driven solutions in high-risk industries, promoting sustainable practices that prioritize worker safety and productivity.",3,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Machine Learning in Human Systems,129
434,8928.0,Investigating Driver Behavior and Decision-Making in Defensive and Competitive Scenarios Using Tobii Glasses3 Eye Tracker and Machine Learning in a Vehicle in the Loop Driving simulator,Academician,"An understanding of driver behavior and decision-making processes is essential for efficient human-vehicle collaboration as driving duties are progressively transferred from human operators to automated systems. This study employed a vehicle-in-the-loop driving simulator along side with Tobii Glasses 3 Eye Tracking technology to investigate drivers’ behavior under defensive and competitive driving scenarios. Machine learning models have been developed to predict driving strategies by analyzing key characteristics, including pupil diameter, dilation, fixation duration, saccade length, and gaze. These models revealed significant trends in visual attention, cognitive load, and hazard identification, providing valuable insights into driver decision-making processes. These findings emphasize the importance of understanding driving habits to enhance traffic safety, support automated vehicle systems, and provide personalized feedback. Insights from this study can guide the development of advanced driver-assistance systems (ADAS) featuring adaptive sensors and predictive technologies to minimize avoidable collisions and improve road safety. Additionally, the results have broader applications in traffic management, driver education, policymaking, traffic flow modeling, and insurance risk assessment.",4,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Machine Learning in Human Systems,129
435,8564.0,Field-Applicable Ground Reaction Force Estimation Through Deep Learning: Enhancing Biomechanical and Ergonomic Assessments,Academician,"Ground reaction forces are critical inputs in biomechanical analysis for estimating joint kinetics, essential in ergonomic assessment and musculoskeletal disorder evaluation. Traditional ground reaction force measurement relies on force plates in laboratory settings, which limit applicability to real-life conditions, while wearable force sensors are constrained by battery life and user comfort. This study proposes a machine learning-based model to estimate ground reaction forces using joint coordinate data captured via motion tracking systems. Twelve healthy subjects performed bending and squatting tasks, with kinematic and kinetic data collected from a motion capture system and force plates. These data were then used to train and validate a deep-learning neural network model for ground reaction force estimation in practical, field applications. Model performance will be assessed using root mean square error. This approach aims to deliver a handy tool for ground reaction force estimation, potentially integrable with computer vision techniques to facilitate on-the-field biomechanical assessments for ergonomic and related applications.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - AI and Data-Driven Research,130
440,6267.0,Survey of AI-Based Tools for Enhancing Human Factors and Ergonomics in the Workplace,Academician,"The integration of Artificial Intelligence (AI) in human factors and ergonomics (HFE) has introduced transformative tools for enhancing workplace safety, optimizing ergonomic design, and reducing injury risks. This literature review surveys AI-based products currently used in HFE, focusing on their roles in ergonomic risk assessment, real-time monitoring, and early intervention strategies. AI tools leverage machine learning algorithms, computer vision, and data analytics to identify potential ergonomic hazards proactively, enabling targeted interventions before injuries occur. Key applications include using AI-driven image recognition for postural analysis, wearable sensors for continuous monitoring, and predictive analytics for assessing injury risk. These technologies allow for a systems-thinking approach to ergonomics, providing comprehensive insights into worker movements, task demands, and environmental factors. By examining the functionality, efficacy, and accessibility of these AI tools, this review highlights how they contribute to a safer and more productive workplace. Additionally, it identifies emerging trends, challenges, and future directions for AI in HFE, emphasizing the importance of integrating these tools into organizational practices. As AI technology advances, its potential to improve workplace ergonomics and enhance worker well-being continues to grow, paving the way for innovative, data-driven solutions in occupational health.",2,118 | Cobb Galleria Centre,Technology and Psychology in Ergonomic Research,131
441,6947.0,How Surgeon Personality Influences Non-Verbal Communication: A Case Study,Academician,"The non-technical skills (NTS) of a surgeon are critical for surgical outcomes, with non-verbal communication being an essential yet underexplored aspect of team dynamics. This study investigates how surgeon personality traits, extraversion, agreeableness, conscientiousness, emotional stability, and dominance, impact non-verbal behaviors in robotic-assisted urology surgeries to enhance team cohesion and performance. Thirty-four surgeries were analyzed, with three coders identifying and categorizing non-verbal behaviors as effective (aligned and supportive of verbal communication), ancillary (unrelated to task goals but present in the setting), repair (addressing communication breakdowns), or upgrade (enhancing verbal communication). Surgeon personality was assessed using the Ten-Item Personality Index (TIPI). Preliminary analysis of 11 surgeries revealed that extraversion and agreeableness negatively correlated with the frequency of non-verbal events, while conscientiousness showed a positive correlation. Emotional stability had no significant effect on overall event count. Specific behavior analysis revealed that effective non-verbal communication correlated positively with emotional stability but negatively with extraversion, agreeableness, conscientiousness, and dominance. Ancillary behaviors were positively associated with dominance and emotional stability, while negatively correlated with extraversion, agreeableness, and conscientiousness. Upgrade and repair behaviors showed positive relationships with dominance, conscientiousness, and emotional stability, but negative correlations with agreeableness. These findings suggest that emotionally stable and conscientious surgeons demonstrate more purposeful and adaptive non-verbal strategies, especially in repair and upgrade contexts. Conversely, extraverted surgeons may rely more on verbal interactions. Insights from this study could guide training to optimize communication strategies tailored to individual personality traits, fostering stronger teamwork and better surgical outcomes.",3,118 | Cobb Galleria Centre,Technology and Psychology in Ergonomic Research,131
442,8995.0,Comparison of Different Weighting Scales when Measuring Mental Workload,Practitioner,"Assessing mental workload is significant in designing occupational tasks, as inappropriate levels can impact job performance, employee well-being, and even workplace safety. By assessing and decreasing mental workload, organizations can enhance productivity, reduce errors, and foster a healthier work environment. The NASA Task Load Index (NASA-TLX) and the Surgery Task Load Index (SURG-TLX) are tools used to assess mental workload subjectively. Both tools offer unweighted and weighted mental workload measurements. Researchers infrequently employ weighted TLX assessments due to their strong correlation with unweighted TLX measures and the additional time needed to collect pairwise weightings during experiments. This study aims to compare weighted and unweighted TLX scores. Different weighting scales (weighted, unweighted, rating, and ranking) were used to estimate the TLX scores. Pearson correlations and paired t-test analysis were used to compare the unweighted and weighted TLX scores across tasks and conditions. The results will allow us to know if there is any significant difference in the scores between weighting alternatives; therefore, improving the process of selecting the appropriate scale when using the TLX surveys. The goal is to identify a technique that will produce accurate results but does not require too much time or resources and especially will not generate fatigue, stress, or boredom for those filling out the surveys.",4,118 | Cobb Galleria Centre,Technology and Psychology in Ergonomic Research,131
443,6569.0,Multi-stimulus cognitive workload measure using skin-integrated triboelectric pulse sensor for various environments,,"Triboelectric nanogenerator have been developed to create self-powered devices for power generation or sensing applications. There are many sensor applications for triboelectric nanogenerators in systems where there may be any residual mechanical energy that would have otherwise been unutilized. One such system is the human body; hence, a prominent application of triboelectric sensors is skin-integrated health monitoring sensors. Due to the self-powered, easy to operate and bio-compatible nature of these sensors, there is growing interest in more widespread adoption of these sensors in real-world applications. This can be achieved by developing new methods of enhancing the triboelectric performance and investigating reliable and scalable manufacturing of these sensors. Moreover there is a need for development with a holistic approach combining device performance and scalable manufacturing. Here we present a novel water-based ink formulated for skin-integrated, highly sensitive triboelectric sensors. The ink synthesis utilizes non-toxic, bio-compatible polymers that enable safe and direct skin contact, ensuring reliability in triboelectric output. With tunable viscoelastic shear-thinning properties, the ink can be optimized for various printing techniques, allowing for the production of uniform, thin films with minimal defects. Additionally, the ink features recoverable cross-linking, which ensures it reverts to its original structure once the shear forces applied during the printing process are removed. This design framework provides a versatile, safe, and efficient approach for fabricating next-generation wearable sensor systems.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Workload Measurements,132
444,6583.0,Wearable Chemical Nanosensors for Real-time Human Performance Monitoring and Workload Assessment,Academician,"Wearable sensors present a breakthrough in human factors engineering and ergonomics by offering a non-invasive alternative to conventional performance monitoring methods, enabling real-time, continuous assessment of worker physiological states and cognitive workload. The capability to continuously analyze and predict dynamic changes in various biomarkers in easily accessible body fluids (e.g., sweat) would significantly enhance workplace safety, optimize human-system integration, and improve overall operational efficiency in industrial settings. We develop novel wearable biosensors utilizing nanoscale semiconductors, specifically atomically thin tellurene, capable of detecting multiple biomarkers in sweat that correlate with mental workload and fatigue. The unique p-type characteristics of tellurene, combined with our ergonomic wearable design, enable sensitive and selective longitudinal monitoring of human performance metrics. The sensor facilitates real-time workforce analytics and predictive modeling of human performance variables by providing continuous, multi-analyte data streams. Integrating physiological monitoring with digital twin simulation creates new opportunities for workplace safety enhancement, process optimization, and data-driven decision making in industrial engineering applications.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Workload Measurements,132
445,6827.0,"Bioinspired, Skin-Like Laser-Induced Graphene Wearables for Continuous Mental Workload Monitoring",Practitioner,"Wearable sensors are crucial in monitoring physiological responses to mental workload (MWL), offering real-time insights into cognitive and emotional states. Accurate assessment of MWL is essential in fields like aviation, automotive industries, and high-stress work environments where cognitive overload can lead to decreased performance and increased risk of errors. However, existing devices often prioritize performance metrics over user comfort and ergonomics, leading to poor wearability and reduced long-term compliance. This compromises the effectiveness of continuous monitoring, as discomfort can cause users to abandon or misuse the devices. Furthermore, human skin's stretchable and breathable nature presents challenges for integrating electronic architectures into skin-interfaced devices, often resulting in poor sensor-skin contact and signal degradation. Human factors engineering and design-for-wearability principles provide guidelines to address these limitations by engineering devices that are not only functional but also comfortable, adaptable, and unobtrusive. Focusing on the skin-sensor interface can enhance signal quality while maintaining user comfort. We have developed a skin-like laser-induced graphene-based triboelectric sensor through a bioinspired interfacial cross-linking approach in this context. This results in skin-like mechanical properties, breathability, and self-healability, ensuring sustained comfort and durability during prolonged use. Capable of accurately tracking cardiovascular indicators such as heart rate and heart rate variability, this sensor provides a reliable, high-performance, and user-centered solution for monitoring key physiological markers of MWL.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Workload Measurements,132
446,6417.0,Leveraging Retrieval-Augmented Generation and Extended Reality for Enhanced Accessibility of Alternative Fuel Vehicle Emergency Response Guides,,"As the number of alternative fuel vehicles (AFVs) and their related accidents continue to rise, the need for effective emergency response guidance becomes more critical. First responders often face challenges accessing essential procedures quickly, especially when dealing with various vehicle models and complex scenarios. These delays in accessing accurate information can lead to increased risks during emergency response situations, impacting both responder and public safety. With the development of large language models (LLMs), it is now possible to provide quick responses. However, these models often lack the ability to deliver context-specific and accurate information. To address this, we developed a framework that integrates Retrieval-Augmented Generation (RAG) with LLMs and extended reality (XR) to enhance the usability of emergency response guides for AFVs. The proposed system processes digital inputs from emergency response guides, retrieves relevant information, and visualizes the data through XR devices such as the HoloLens 2. This approach provides first responders with rapid, hands-free access to vital emergency procedures, enhancing situational awareness and decision-making efficiency. Evaluation results demonstrated that the system could effectively extract and present relevant emergency response content across various vehicle models, significantly reducing response times and improving user satisfaction. These findings show the potential of combining RAG, LLMs, and XR to enhance emergency response, making vital information more accessible for first responders.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Extended Reality Session 2,133
447,8761.0,AI-Integrated Augmented Reality for Context-Aware Guidance in Procedural Tasks,Academician,"This talk presents recent progress and outlines a future research agenda on transforming augmented reality (AR) into an intelligent coach and assistant for industrial workers performing complex psychomotor tasks. Our research leverages head-mounted AR devices as input/output platforms capable of streaming multiple data modalities (e.g., RGB, depth, gaze, speech, hand tracking, head pose) to an AI server for inference, with the results streamed back to the headset to deliver context-aware interventions. The talk will begin by introducing a scalable, state-of-the-art pipeline for data streaming and AI-driven inference. It will then explore three key use cases showcasing various applications of AI-integrated AR: (1) Activity understanding—a novel transformer-based deep learning model will be introduced, designed to leverage egocentric RGB and hand-tracking data for detecting task steps in procedural tasks. (2) Expertise estimation—a new algorithm will be presented that utilizes gaze and hand-tracking data to estimate proficiency levels in skill-based tasks. (3) Human-robot interaction—an innovative AR interface and a 6D pose tracking model will be discussed, enabling intuitive intent recognition through visual attention tracking, hand tracking, and RGB-D data in human-robot collaborative tasks, such as assembly. The talk will conclude with a research agenda outlining directions for future research on next-generation AI-integrated AR guides in diverse industrial contexts.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Extended Reality Session 2,133
448,6706.0,Investigating the Impact of Stress on Shared Mental Models and Team Performance in Collaborative Mixed Reality Environments,,"In high-paced work environments such as manufacturing, teams are often under intense stress while working to deliver quality outputs. The concept of shared mental models (SMMs) where team members have a common understanding of tasks, processes, and goals , has been suggested to enhance team performance. However, human stress can impact the development of these SMMs. In this study, we explore the relationship between stress, SMMs, error occurrence, and teamwork quality within a collaborative mixed reality (MR) manufacturing environment. The study engaged 110 participants in an undergraduate course, where they used a collaborative MR module to first learn about the design and then assemble a hydraulic bike and an excavator arm. The MR environment offers an immersive platform that simulates real-world conditions while allowing variable manipulation and observation of interactions in ways that traditional settings might not permit. The experiment included two experimental conditions: with or without stress, using time pressure as the stressor. Both survey-based and sensor data (e.g., electrodermal activity and heart rate) are collected to analyze the impact of stress on the development of SMMS, error occurrence, and teamwork quality. The study aims to determine how time pressure as a stressor affects team interactions and performance within the MR environment. By integrating these findings into training programs and operational strategies, organizations can enhance team outcomes in high-paced environments.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Extended Reality Session 2,133
449,6781.0,How to Effectively Implement eXtended Reality into Higher Education Curricula? A Report on Stakeholder Analysis and Diverging Needs,Practitioner,"Immersive technologies like eXtended Reality (XR) are becoming more mature and prevalent, presenting significant opportunities to increase learners' motivation, enhance engagement, and improve long-term knowledge retention and comprehension. Consequently, this applies pressure on higher education institutions (HEIs) to adopt and utilize emerging technologies like XR. However, HEIs are bureaucratic organizations that are notoriously difficult to change, with a large number of decision-makers distributed across various levels of the organizational hierarchy. Thus, understanding the roles and needs of associated stakeholders is essential for successful XR integration into curricula. To that end, this work presents findings from a case study on XR implementation processes at Virginia Tech (VT), documenting the stakeholders involved in the process, along with their specific roles, nuanced needs, and the influence these factors have on XR implementation and its long-term sustainment. Findings indicated that the implementation of a relatively simple project requires the involvement of various stakeholders, each representing key aspects of the process. This results in unpredictable latencies between stakeholder actions to overcome barriers. Moreover, the initial development of XR platforms is risky and time-consuming, often falling onto the shoulders of “champions” who may struggle and ultimately fail due to the absence of long-term sustained support mechanisms. Furthermore, surface-level interest and lack of commitment from administrative entities hinder the advancement and implementation of XR initiatives. Although the case study focuses on VT’s socio-technical system, the findings contribute to a broader understanding of the complexities associated with integrating XR technologies into HEIs, ultimately providing guidance for all stakeholders.",4,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Extended Reality Session 2,133
450,6001.0,Improving Human-Exoskeleton Coordination through Modification of the Torque’s Rise and Fall Time Control Parameters in Powered Ankle Exoskeleton,Academician,"Exoskeletons have become increasingly popular due to their potential to enhance or assist human capabilities across various industries, such as manufacturing and healthcare. Lower-extremity powered exoskeletons are driven by controllers that store commands to assist the user. However, they still have their own unique set of complexities and unknowns. It needs to be determined how changes in exoskeleton control parameters can affect human-exoskeleton coordination. Changes in exoskeleton parameters, such as ankle timing and quasi-stiffness, can influence gait characteristics and muscle activation, affecting human performance. This research aims to develop a new exoskeleton controller for the Dephy ExoBoot to modify the torque profile's rise and fall time (exoskeleton parameters) and evaluate people’s sensitivity to the changes and associated changes in gait characteristics. The Dephy ExoBoot provides torque/assistance at the push-off during the stance phase of the gait cycle. To provide the assistance, the actuator produces a torque about the ankle joint by spooling an inelastic belt that is rigidly attached to the exoskeleton’s lever arm. The results from this pilot study will provide a preliminary understanding of human perception towards changes in exoskeleton control parameters, which offers insight into individual preferences and differences in exoskeleton usage and informs exoskeleton precision requirements to maximize human-system interaction, improving human performance.",1,118 | Cobb Galleria Centre,Human Factors & Ergonomics - Physical Ergonomics #2,134
452,8611.0,Assessment of Ergonomic Surgical Intervention Table (E-SIT) for Transcatheter Aortic Valve Replacement,Practitioner,"Within Interventional Cardiology, procedures (diagnostic, interventions, and surgical) have free-standing tables behind the operators holding materials (catheters, wires, etc.) while the operators face the procedure table. This workflow setup requires operators to perform 50-100+ turns to access materials. This can lead to repetitive awkward turning motions that can lead to ergonomic issues like localized fatigue and Work-Related Musculoskeletal Disorders (WMSDs) in the back, and neck. Space constraints impact workflow and may exacerbate WMSDs. An Ergonomic Surgical Instrument Table (ESIT) will be attached to the procedure table, positioned over the patients’ body as a work environment and workflow intervention. The primary aim of this work is to evaluate the workflow and ergonomic impacts of the ESIT intervention with the hypothesis that the ESIT will allow for direct tool access, improved workflow, reduced turning, and improved ergonomics. To evaluate workflow and ergonomic changes pre- and post-intervention, the Transcatheter Aortic Valve Replacement (TAVR) procedure was chosen due to it being relatively standardized and low-risk. Around 15 operators of the TAVR procedure in the Mayo Clinic Catheter Lab will be recruited. Each operator will be evaluated for six procedures minimum using the existing workflow setup, and for same number of procedures with the interventional workflow setup after a period of acclimatization. To evaluate workflow and ergonomics, postural data will be collected using an array of Inertial Measurement Units (IMUs), movement patterns and workflow task-analysis will be evaluated using video recordings, and perceived workload and workflow will be collected using a self-reported survey.",1,112 | Cobb Galleria Centre,"Human Factors & Ergonomics - Applied Ergonomics for Healthcare, Education, and High-Stakes Work",135
453,6491.0,"Preventing WMSDs in Healthcare: A Study of Mental Workload, Psychosocial Factors, and Body Biomechanics",Academician,"Recognizing that physical, psychosocial, and individual factors interplay in WMSDs, this research focuses on how heightened mental workload might influence body biomechanics, potentially increasing WMSD risk. The aim is to investigate the effects of mental workload on body behavior and its potential contribution to work-related musculoskeletal disorders (WMSDs) among healthcare professionals. Conducted in a controlled laboratory setting, the study engages nursing students and professional nurses to simulate a realistic healthcare task, the insertion of a nasogastric tube, under varied mental workload conditions—baseline, auditory distractions, and interruptions. Using a cross-sectional, repeated-measures design, this study incorporates both quantitative and qualitative methodologies. Mental workload perceptions will be measured via the NASA Task Load Index and cortisol levels from saliva samples, while body biomechanics will be tracked through motion capture and ergonomic assessment tools. Statistical models will analyze how specific psychosocial stressors and individual factors (e.g., age, personality traits) affect mental workload and biomechanical responses. Expected outcomes include identifying key psychosocial factors impacting mental workload, understanding their relationship with body posture and movement, and highlighting individual characteristics that may exacerbate WMSD risk. These insights will inform ergonomic interventions in healthcare, promoting safer workplace design by accounting for the mental and physical demands of healthcare workers. This research contributes to a holistic understanding of WMSDs and fosters proactive strategies for minimizing these disorders in high-stress healthcare environments.",2,112 | Cobb Galleria Centre,"Human Factors & Ergonomics - Applied Ergonomics for Healthcare, Education, and High-Stakes Work",135
454,6188.0,Enhancing Human-Robot Interaction through Ensemble Intention Recognition and Trajectory Tracking,Academician,"Recognizing intentions and tracking trajectories in Human-Robot Interaction (HRI) is crucial for enabling robots to anticipate and support human actions effectively. This study explores the application of ensemble learning to classify and track human intentions and locations in HRI, utilizing data collected from Virtual Reality (VR) environments. By leveraging VR, we establish a controlled and immersive setting that allows for precise monitoring and recording of human behavior. We developed and trained ensemble deep learning models—combining Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Transformers—on this comprehensive dataset to achieve high accuracy in recognizing human intentions and predicting trajectories. Ensemble learning proved beneficial by merging the strengths of individual models. While CNN and CNN-LSTM models demonstrated high accuracy, they struggled with identifying specific intentions under certain conditions. The CNN-Transformer model, however, excelled, yielding superior precision, recall, and F1 score for intention classification, along with robust trajectory tracking. By aggregating diverse models, the ensemble approach increased overall predictive performance, offering enhanced robustness and adaptability to complex human behaviors. This method highlights the potential for significantly improving HRI by enabling robots to understand and respond to human intentions in real time, ultimately fostering more intuitive and effective human-robot collaboration. Our findings underscore the value of ensemble learning in advancing intention recognition and trajectory tracking within HRI.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Human-Robot Interaction,136
456,6673.0,Developing a Virtual Reality Platform for Full-Body Human-Robot Interaction,Academician,"As robots become increasingly integrated into daily life in sectors like healthcare, education, and manufacturing, close human-robot collaboration is expected. In these shared environments, unintended physical contact between humans and robots is inevitable, akin to everyday human interactions. Such contact is significant for building teamwork, developing trust, and enhancing collaboration. To prepare humans for these full-body interactions and ensure safety and comfort, a controlled environment for learning is needed. We have developed a Virtual Reality (VR) platform that enables safe, immersive, full-body human-robot interaction. This platform provides realistic simulations where users engage with virtual robots, experiencing various physical interactions without real-world risks. Through repetitive practice in different scenarios, users become accustomed to the dynamics of human-robot interactions, learn appropriate responses, and build confidence. The VR system integrates advanced force estimation and motion feedback technologies, including wearable sensors, VR headsets, controllers, and an omnidirectional treadmill to capture movements and provide realistic feedback accurately. Multimodal feedback mechanisms, encompassing visual, auditory, and haptic cues, are designed to enhance user awareness and understanding of physical contacts. The platform is highly customizable to accommodate individual preferences and sensitivities, making it accessible to different users. Through iterative testing and user collaboration, we have refined the platform to optimize effectiveness and user satisfaction. This VR environment serves as a valuable tool for training, education, and research, facilitating the safe exploration of full-body interactions with robots. By preparing humans for close collaboration with robotic counterparts, our platform contributes to the seamless integration of robots into daily life.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Human-Robot Interaction,136
457,6371.0,Multi-Sensory In-Vehicle Experiences Alleviate Driver Stress,Practitioner,"Nearly 70% of Americans report experiencing daily stress, with driving frequently cited as a significant contributor. Given the widespread reliance on personal vehicles, incorporating relaxation techniques during driving breaks could provide a practical way to alleviate stress. However, stimuli introduced while driving could distract drivers, making short breaks a safer alternative. Despite the potential benefits, there is limited research on how to optimize these breaks for stress relief. This study aims to address this gap by developing and evaluating a multi-sensory in-vehicle relaxation method designed to reduce mental stress during short driving breaks, based on subjective questionnaire data. Forty-eight participants experienced three break conditions: a control (no relaxation), multi-sensory relaxation without scent, and multi-sensory relaxation with scent. Participants’ subjective stress levels were assessed after each condition to measure the method’s effectiveness. The results indicated that both multi-sensory relaxation experiences (with and without scent) significantly reduced stress compared to the control condition. Specifically, stress levels in the control were 0.28 points higher than in the relaxation with scent condition (p = 0.04). Post-study feedback reinforced these results, with 98% of participants preferring the relaxation experience and 62% expressing a willingness to use the relaxation method regularly during driving breaks. These findings suggest that multi-sensory relaxation techniques are effective in reducing stress during short driving breaks and are well-received by users. Incorporating such relaxation experiences into increasingly automated vehicles could enhance driver well-being, lower stress levels, and contribute to long-term health improvements.",1,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Driver Behavior and Automated Vehicle,137
458,8943.0,Analyzing Driving Behavior Distraction During Animal Crossing in Rural Road; Insights From Eye Tracking and Vehicle-in-the-Loop Driving Simulator,Academician,"Rural roads continue to be among the riskiest and least understood driving conditions. Using a vehicle-in-the-loop driving simulator and Tobii Glasses 3 eye-tracking technology, this study examined driving performance and behavior on rural roads in three different visibility scenarios: high (during the day), medium and low(during the night). A comparison of driver attention and decision-making in manual and Advanced Driver Assistance System (ADAS) modes was made possible by the inclusion of actual risks in the simulations, such as roadside distractions and animal crossings considering both animal walking and running cross the road. Important findings showed that drivers' ability to notice hazards was seriously lacking in areas including failing to keep safe distances and ignoring animal crossings. The unpredictable and low visibility of rural areas, where cognitive load and attention demands were higher, made it difficult for ADAS systems to prevent crashes, even though they were successful in controlled circumstances. These findings highlight the necessity of more sophisticated sensors, cameras, and radars to improve ADAS functionality in rural areas. This work offers important insights into how visual attention, cognitive functions, and driving performance interact. These insights can be used to develop adaptive ADAS systems, improve road safety, and solve the particular difficulties associated with driving in rural areas.",2,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Driver Behavior and Automated Vehicle,137
459,8944.0,Harnessing Driver Metacognition to Advance In-Context Learning for Autonomous Vehicle Social Intelligence,Academician,"This conceptual paper explores the integration of driver-based social metacognition experiments with prompt engineering for in-context learning, aiming to enhance the social intelligence of autonomous vehicles (AVs) in pedestrian interactions. We propose a novel framework that leverages human metacognitive processes to address limitations in the social cognitive capabilities of large language models (LLMs) when applied to AV decision-making systems. While AVs have made significant technological strides, their ability to navigate interactions with vulnerable road users remains a critical challenge in urban environments. These scenarios demand advanced reasoning capabilities that traditional AI systems often lack. Foundation models like LLMs offer a promising solution for equipping AVs with enhanced common sense and social reasoning skills. However, despite their remarkable language understanding and generation abilities, LLMs are trained on complex language patterns and lack the nuanced social intelligence derived from real-world interactions and biological foundations. To overcome this gap, we propose using driver-based metacognition experiments to generate effective prompts for in-context learning. These prompts aim to capture the core of human social cognition, focusing on two critical aspects: improving the accuracy of social cognitive estimations and calibrating confidence levels in these estimations. By distilling drivers' implicit and explicit metacognitive processes into structured prompts, our approach guides LLMs in modeling pedestrian intentions and beliefs across diverse traffic scenarios. This framework represents a step forward in advancing AV systems, enabling safer and more intelligent navigation of complex social environments by synthesizing human cognition with AI reasoning.",3,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Driver Behavior and Automated Vehicle,137
460,8647.0,Evaluating attentional demands of in-vehicle interfaces,Academician,"Touchscreens have become widely adopted in vehicles because they are flexible interfaces that enable easy updates and accommodate advancing technology. However, in-vehicle touchscreens may have detrimental effects on driver attention. As a result, incorporating some number of physical controls into in-vehicle interfaces may better support driver attention to the driving task. However, there is currently no unified framework for determining how to optimally allocate functions (e.g., navigation control) between different types of inputs like touchscreens or traditional buttons. The allocation of functions between touchscreen and button inputs has substantial implications for how drivers interact with in-vehicle interfaces (i.e., the interaction model). Interaction models are influenced by factors like input type (touchscreen vs. button), input feedback (visual, haptic), and display behavior which may be highly dynamic (where display content may change, like on touchscreens) or primarily static (where information format and location are consistent, like on many vehicle dashboards). To support the design of next-generation vehicle interfaces, we conducted a systematic literature review of studies that collected empirical data about driver interactions with in-vehicle interfaces. We classified the studies based on the characteristics of the interaction models used, specifically examining input type, input feedback, and output behavior. We identified specific measures of attentional demand and driver performance collected within each study. Finally, we identified characteristics of interaction models associated with higher attentional demand and the resulting impact on performance. We used those findings to identify research gaps, make recommendations for future research, and apply these insights to the driving environment.",4,112 | Cobb Galleria Centre,Human Factors & Ergonomics - Driver Behavior and Automated Vehicle,137
461,6307.0,"Analysis of Stationary Lithium-Ion Battery Energy Storage System Accidents: Causes, Patterns, and Insights",Practitioner,"This study provides a comprehensive analysis of accidents involving stationary lithium-ion battery energy storage systems (BESS) over the past 13 years, from 2011 to 2024, revealing critical trends in incident frequency, risk factors, and operational phases. Using data from the Electric Power Research Institute (EPRI), we systematically examine the causes, timing, and environments of BESS incidents, normalizing incident rates per installed megawatt-hour (MWh) to enhance the accuracy of the risk profile. Findings indicate that 60% of incidents occur during the pre-installation phase, while 25% occur within the first year of operation, with prefabrication sites and assembly areas representing 40% of reported incidents. These results underscore an urgent need for enhanced safety protocols during pre-installation and early operational phases, particularly within assembly environments. This study offers essential data for developing targeted risk mitigation strategies and improving safety standards across the BESS lifecycle.",1,109 | Cobb Galleria Centre,Advanced Data Analytics for Building and Battery Energy Systems,138
462,6598.0,A comprehensive comparison of Time-Series Models for Battery health diagnostics and prognostics,Academician,"Accurate monitoring of battery health is crucial for ensuring the reliability and longevity of energy storage systems, particularly in applications such as electric vehicles and renewable energy. Traditional methods, like empirical formulas, often struggle to capture the complexities of battery degradation in dynamic systems. This paper investigates the use of advanced time-series forecasting models to predict the State of Health (SOH) and Remaining Useful Life (RUL) of lithium-ion batteries. We examine three sophisticated models capable of capturing both long-term dependencies and short-term fluctuations: (1) Prophet, a statistical model known for its flexibility in handling non-linear trends and complex seasonality and outliers, (2) DeepAR, a machine learning model that excels at modeling high-dimensional dataset and can delivers probabilistic predictions to account for uncertainty, and (3) Temporal Fusion Transformers (TFT), a hybrid model that combines transformer-based architectures with specialized components to enhance forecasting accuracy. These models are applied to a publicly available battery dataset, and their performance is evaluated based on key criteria such as accuracy, robustness, scalability, and interpretability. Additionally, we explore the impact of group training (multi-task learning), supported by both DeepAR and TFT, which allows the models to learn shared patterns across multiple battery systems. By leveraging data from different battery types or systems, group training improves generalization and enhances model accuracy, particularly when individual data is sparse. This study offers valuable insights into the strengths and weaknesses of each approach, helping identify the most effective model and optimal strategies for applying these models to real-world battery health management.",2,109 | Cobb Galleria Centre,Advanced Data Analytics for Building and Battery Energy Systems,138
463,6566.0,Is Transformer all you need? Answer is no to Building Energy Forecasting in High Volatility,,"Modeling and predicting building energy consumption is crucial for addressing energy efficiency issues in buildings and tackling the challenges posed by urban expansion and so on. Precisely forecasting energy usage at a smaller time interval opens up possibilities to address a wider range of issues in several domains, including urban planning and the energy market. The Transformer has been widely utilized in time series forecasting. The Transformer has been criticized to have limited scalability regarding the long dependency among long sequences and to lose temporal information. Some MLP models can also achieve comparable performance as Transformers. Motivated by these findings, we wonder if recurrent neural networks (RNNs) based models can also match or even outperform the Transformers and MLP models in time series forecasting. This study compares several Transformers, MLP and GRU-based models trained on power consumption data in Massachusetts, showing that the Transformer and MLP model achieve comparable performance and the GRU-based model outperforms the other types of models.",3,109 | Cobb Galleria Centre,Advanced Data Analytics for Building and Battery Energy Systems,138
464,5937.0,Replacement Policies for Fixed and Floating Wind Farms,,"We present a model to explore replacement and ordering policies in offshore wind farms that contain both fixed-bottom and floating wind turbines situated in shallow and deep waters, respectively. The model incorporates two types of turbines that experience distinct, yet possibly correlated environments. That is, the distance between the two types of turbines is sufficient to induce different degradation rates, yet their evolutions are correlated because they reside in the same overall region. Our goal is to optimally prescribe policies for replacing turbines and ordering jackup vessels (JUVs) to reduce overall maintenance costs, while accounting for the stochastic dependence arising from correlated environmental conditions and the resource dependence associated with a shared JUV. By capturing these dependencies, we provide insights into optimal policies, enhancing the cost-effectiveness of maintenance activities in offshore wind farms.",1,109 | Cobb Galleria Centre,Optimization and Decision Models for Wind Farm Operations and Maintenance,139
465,8879.0,Leasing Land for a Wind Turbine: A Decision Model for a Landowner,Academician,"Wind energy and wind turbines are increasing throughout the United States and especially in Midwestern farming states. A landowner or farmer who is thinking about leasing their land for wind turbines may encounter opposition from the community, however. The landowner needs to determine if the land should be devoted to wind energy or used for agricultural purposes (e.g., grazing or growing crops). If the landowner is going to lease the land for a wind turbine, the landowner may need to decide between a fixed annual payment or an annual royalty based on the amount of wind energy produced by the turbine. We create a decion model for a landowner who is considering leasing land for a wind turbine and exploring different leasing alternatives. The model accounts for uncertainty in the county's reaction, energy prices, and the revenue gained from using the land for growing corn. The model examines how the decision might change based on the landowner's risk attitude. We find that leasing the land for a wind turbine is financially better than using the land for agricultural. Deciding between the type of lease depends upoin the terms of the lease agreement and the landowner's risk attitude.",2,109 | Cobb Galleria Centre,Optimization and Decision Models for Wind Farm Operations and Maintenance,139
466,8905.0,Attention is All You Need to Optimize Wind Farm Operations and Maintenance,Academician,"Operations and maintenance (O&M) is a fundamental problem in wind energy systems with far reaching implications for reliability and profitability. Optimizing O&M is a multi-faceted decision optimization problem that requires a careful balancing act across turbine level failure risks, operational revenues, and maintenance crew logistics. The resulting O&M problems are typically solved using large-scale mixed integer programming (MIP) models, which yield computationally challenging problems that require either long-solution times, or heuristics to reach a solution. To address this problem, we introduce a novel decision-making framework for wind farm O&M that builds on a multi-head attention (MHA) models, an emerging artificial intelligence methods that are specifically designed to learn in rich and complex problem settings. The development of proposed MHA framework incorporates a number of modeling innovations that allows explicit embedding of MIP models within an MHA structure. The proposed MHA model (i) significantly reduces the solution time from hours to seconds, (ii) guarantees feasibility of the proposed solutions considering complex constraints that are omnipresent in wind farm O&M, (iii) results in significant solution quality compared to the conventional MIP formulations, and (iv) exhibits significant transfer learning capability across different problem settings.",3,109 | Cobb Galleria Centre,Optimization and Decision Models for Wind Farm Operations and Maintenance,139
467,8952.0,"A Hierarchical Optimization Framework to Integrate Turbine Failure Risks and Accessibility Forecasts to Offshore Wind Farm Operations, Maintenance, and Logistics",Academician,"This paper introduces an innovative hierarchical optimization framework to optimize operations, maintenance, and logistics decisions in offshore wind farms. Operating at multiple levels, the framework leverages a monthly dynamic programming model at a higher tier to determine optimal vessel rental strategies, as a function of turbine statuses and future accessibility conditions. At a more granular level, a detailed model is employed to optimize operations hourly over a 60-day timeframe with a rolling horizon fashion. Integrating various data-driven predictions such as weather forecasts, production levels across wind farm locations, and turbine failure risks, this framework offers a comprehensive approach to asset management. The proposed approach provides significant advantages over benchmark policies in terms of both efficiency and reliability.",4,109 | Cobb Galleria Centre,Optimization and Decision Models for Wind Farm Operations and Maintenance,139
468,6198.0,Optimal CCUS Subsidies with perfect and imperfect information,Academician,"Carbon capture, utilization, and storage (CCUS) has been widely recognized for its role in many decarbonization pathways. Emerging technologies like CCUS are often adopted slowly until the technology matures and the implementation costs fall enough for firms to profit. Government planners can induce earlier adoption of new technologies through direct and indirect subsidies. In this paper, we use a stylized Stackelberg game framework involving a government planner and CO2-emitting firm to study the impacts of different subsidy levels on CO2 emissions, firm profit, and social welfare. We analytically derive optimal CCUS subsidy levels under a variety of assumptions and study the conditions under which it is optimal to induce the firm to invest in CO2 capture, and under which emissions could actually increase due to CCUS adoption. We extend our model further by adding uncertainty in the firm’s CCUS investment costs, which leads to a game of incomplete information from the government’s perspective. From this we study the effects of uncertainty on the optimal subsidy level and CO2 emissions. We also apply our findings to a case study of a thermal power plant and make policy recommendations based on the outcome.",1,109 | Cobb Galleria Centre,"Energy Policy, Incentives, and Strategic Implementation for a Sustainable Future",140
469,6257.0,Cost and Production Tax Credit Analysis for Hydrogen Production Technologies,Practitioner,"This study evaluates the cost and lifecycle of greenhouse gas emissions of low-carbon hydrogen production technologies. We explore how policy requirements affect the methodology of life cycle assessment and the results of the greenhouse gas emissions assessments. We evaluate hydrogen production by electrolysis using solar, wind, hydro, or nuclear power, and hydrogen produced by biomass can achieve emissions intensities of less than 4kg CO 2 e/kg of H 2 produced. Both steam methane reforming and autothermal reforming processes achieve low emissions with carbon capture and storage applied. Methodological requirements for incrementality, deliverability, and temporal matching vary over time, resulting in different emissions calculations for different time-periods and in different jurisdictions. The results show that electrolysis with solar and wind achieves carbon credits of $0.75/ kg H2. Low-temperature electrolysis and high-temperature electrolysis with nuclear, and hydropower attract the highest carbon credit of $3/kg H2. Biomass energy can yield the US production tax credit of $1/kg of H2 while steam methane reforming and auto thermal reforming processes can achieve the US production tax credit of $0.6/kg of H2 and of $3/kg of H2, with carbon capture and storage at 96.3% and 98% respectively. We calculate the levelized cost of hydrogen for each technology considering production tax credits from the US Inflation Reduction Act. We identify cost reduction targets to achieve parity with fossil-derived H 2 and to meet the US hydrogen cost reduction goals.",2,109 | Cobb Galleria Centre,"Energy Policy, Incentives, and Strategic Implementation for a Sustainable Future",140
470,6968.0,Achieving Sustainable Energy Transition: A System Analysis Approach to Energy Policy Implementation,Academician,"The dynamics of energy transition from fossil fuel to alternative energy sources, though a seeming lofty height towards revitalizing the environment and human wellbeing at large, has without fail continue to have unintended disruption on economic and other areas. The crux of energy transitioning lies in the dynamism of the energy sector and other dependent sectors of the economy – turbulence and restructuring of the entire energy industry to seamlessly integrate new economic- and environment-feasible energy alternatives with minimum disruption to current and future energy demands. Often government energy policies are proposed and implemented without due consideration for the entire energy and energy-dependent industries, this paper is an attempt to holistically simulate the impact of government energy policies on the energy industry and its resulting impact on the environment (GHG emissions and air pollution), economic, health impact, and energy prices. GLIMPSE, a computational framework and support decision tool developed by the US Environmental Protection Agency, was used to evaluate the system-wide impact of Texas energy policies. Further sensitivity analysis was embarked on to ascertain optimal decision levels for various alternative energy sources to achieve predetermined desirable goals set at strategic administrative levels by government for the entire ecosystem directly or indirectly impacted by energy.",3,109 | Cobb Galleria Centre,"Energy Policy, Incentives, and Strategic Implementation for a Sustainable Future",140
471,5935.0,Needs of manufacturing innovation for addressing increasing sizes of wind turbine blades,Academician,"Modern wind turbine rotor blade is a sophisticated component made from advanced composite materials. Driven by the need to ramp up clean energy production substantially and meet the global sustainability goal, one resulting phenomenon is the so-called “rotor race,” a seemingly never-ending appetite for making ever bigger rotor blades. Supersized monolithic blades, regularly exceeding 100 meters, create a lot of challenges, from difficulty in manufacturing and transportation, inability for on-site assembly, to limited options for operation and maintenance. This talk will present the author's thoughts on the options to address the challenges brought on by the increasing size of wind turbine rotor blades built for megaturbines.",1,109 | Cobb Galleria Centre,"Innovative Design, Optimization, and Analytics for Emerging Renewable Energy Technologies",141
472,5785.0,Optimizing Tidal Stream Farm Designs: Solution Accuracy-Compute Resource Trade-off in Infinite Dimensional Optimization,Academician,"The design of tidal stream energy farms can be formulated as an infinite dimensional optimization problem, where tidal stream behavior is represented through the shallow water equations. For numerical simulations, we discretize this infinite dimensional problem. In order to obtain accurate solutions through computationally solving discretized problems, we have to balance the discretization accuracy with the use of computational resources. Inspired by this challenge, we consider a broad class of infinite dimensional problems, and develop a theoretical framework that yields accuracy bounds to optimally guide this trade-off. Numerical simulations for the tidal stream energy farm design problem validate our theoretical results.",2,109 | Cobb Galleria Centre,"Innovative Design, Optimization, and Analytics for Emerging Renewable Energy Technologies",141
473,8671.0,Optimization of Thermoelectric Cooler-Based Atmospheric Water Generation: Insights from Controlled and Outdoor Experiments,Academician,"Atmospheric water generation (AWG) is a sustainable solution for producing drinking water by extracting moisture from the air, and this study evaluates the performance of a thermoelectric cooler (TEC1-12706) for this purpose. Experiments were conducted across a voltage range of 3V to 14V under controlled laboratory conditions, with a 12V DC fan employed to optimize cooling on the hot side and prevent overheating, as trials without a fan yielded suboptimal results due to excessive heating. Trials were performed for different time intervals to assess water generation rates, with the controlled environment ensuring reliable comparisons unaffected by outdoor factors like evaporation and air velocity. Results indicated that the optimal voltage range for water collection was 9V to 11V, with maximum yields observed at 9V for 1-hour experiments and 10V for 2-hour experiments. Voltages above 11V led to reduced water generation due to freezing temperatures that inhibited condensation. Outdoor experiments showed significantly lower water yields, primarily due to evaporation and air velocity, reinforcing the importance of controlled conditions for performance evaluation. This research highlights the potential of thermoelectric coolers in AWG systems, demonstrating their practicality and scalability for sustainable water harvesting. The findings provide valuable insights into optimizing AWG technologies for diverse environmental settings, particularly in water-scarce regions, by emphasizing the need for efficient cooling mechanisms and controlled operation to maximize water yield.",3,109 | Cobb Galleria Centre,"Innovative Design, Optimization, and Analytics for Emerging Renewable Energy Technologies",141
474,4869.0,Design and Feasibility Analysis of Origami Agrivoltaic Systems with Transparent Solar Panels,Practitioner,"Given the current trend towards sustainability, systems such as agrivoltaics, which serves as the dual use of land for agriculture and solar energy, are valuable assets. Considering 52% of US soil is used for agriculture, agrivoltaics have a substantial potential contribution to renewable energy harvesting and utilization. However, large-scale implementation of agrivoltaics is difficult because their stationary nature prevents large machinery from being used within fields, deteriorates soil health, and blocks sunlight exposure. To combat these issues, this research focuses on the design and analysis of the feasibility of an agrivoltaic structure with transparent solar panels. The design was based on rigid origami techniques, resulting in a fully functional and collapsible origami solar array and a complimentary housing tower. To model the application of the transparent agrivoltaic systems, Agricultural Production Systems Simulation (APSIM) and customized weather data were used to simulate the effects of the transparent agrivoltaic systems on crop growth. HOMER software was utilized to estimate energy production based on the specification of Brite Solar’s transparent PV panels and the system design. Both models allowed simulated comparison between implementing transparent, generic, and no agrivoltaics. The results of the study show that transparent PVs are viable for energy production and have minimal effects on crop yield.",1,109 | Cobb Galleria Centre,Innovative Approaches to Sustainable Energy and Environmental Impact,142
475,5820.0,Potential Environmental Impacts of Spent LIBs Management,Academician,"Lithium-ion batteries (LIBs) have been widely used in various electronics and electric vehicles. Globally about 5-95% of spent LIBs are recycled while the majority end up in landfills which raise environmental and economic concerns. Recycling of LIBs has significant economic and environmental benefits by recovering valuable metals and reduce energy consumption, however, LIBs recycling generates gas emissions and liquid wastes which pose potential environmental risk. LIBs and the next generation batteries solid-state batteries (SSBs) –which offer improved battery safety and performance– contain different fluorinated compounds including persistent chemicals such as per- and polyfluoroalkyl substances (PFAS). The decomposition of PFAS during LIBs recycling processes is not well understood. The objective of this research is to bridge the knowledge gap of fluorinated compounds behavior and chemical breakdown during spent LIBs management including landfilling and recycling processes (pyrometallurgy, hydrometallurgy and direct recycling) with focus on pyrometallurgy and thermal treatment under different conditions, in order to develop sustainable recycling practices to mitigate environmental risk while supporting circular economy.",2,109 | Cobb Galleria Centre,Innovative Approaches to Sustainable Energy and Environmental Impact,142
476,9083.0,How well will the power grid hold up in the face of future heatwaves in a changed climate? A high-resolution analysis based on climate-shifted temperatures,Academician,"Extreme weather events such as flood-causing storms, cold snaps, and heat waves impact lifeline infrastructures, particularly the power grid. Scientists project that these events will likely be more frequent and/or intense in the future due to climate change, potentially impacting the power grid at unprece- dented levels. It is essential, in fact, a matter of life-and-death, to adapt the grid to risks of future climate-impacted weather. Realizing the importance of connecting such climate projections with decision support models for grid adaptation that go beyond reactionary mitigative measures, we need mathematical and computational models that (1) consider future climate-impacted weather scenarios and their uncertainties, (2) evaluate their network-wide impacts on power supply and demand, both spatially and over time, capturing the weather-induced uncertainty and variability of both power supply and demand, and (3) optimize the adaptation decisions for long-term reliability and resilience of the power grid balancing the grid’s normal operations and extreme weather performance. With this study based on heatwaves expected to occur in potential climate futures affecting a synthetic grid on the footprint of Texas, we present our preliminary results in seeking answers to the following questions: How does the climate-impacted extreme heat affect the power demand? How does the climate-impacted extreme heat affect the power supply (particularly generation and transmission capacities)? How does the climate-impacted power demand and supply affect the grid performance? How should we adapt the grid to climate change given the analysis of the expected impacts?",3,109 | Cobb Galleria Centre,Innovative Approaches to Sustainable Energy and Environmental Impact,142
477,6326.0,A Cutting-Plane Approach for Transmission Expansion Planning with Integration of Remote Generation Sources,,"The growing integration of renewable energy sources into power systems underscores the need for effective solution methods in Transmission Expansion Planning (TEP). This paper addresses underexplored aspects of the DC Optimal Power Flow-based TEP (DC-TEP) problem, particularly in expanding networks to accommodate new generation sources. We leverage the graph-based Longest Shortest-Path Connection (LSPC) algorithm to enhance the mixed-integer linear programming disjunctive formulation of the DC-TEP. By identifying plausible power flow paths between disconnected buses, this method efficiently generates tighter cutting planes than those derived from the traditional, intractable Longest Path Problem (LPP). To handle the large number of cutting planes that LSPC can generate for large-scale transmission networks, we also propose an efficient scheme to select a subset of effective cuts for the model. The computational advantages of this algorithm are demonstrated through application to real-world cases.",1,109 | Cobb Galleria Centre,Optimization Algorithms in Power System Operations and Planning,143
478,8695.0,Dual Conic Proxy for Semidefinite Relaxation of AC Optimal Power Flow,Academician,"The nonlinear, non-convex AC Optimal Power Flow (AC-OPF) problem is fundamental for power systems operations. The intrinsic complexity of AC-OPF has fueled a growing interest in the development of optimization proxies for the problem, i.e., machine learning models that predict high-quality, close-to-optimal solutions. More recently, dual conic proxy architectures have been proposed, which combine machine learning and convex relaxations of AC-OPF, to provide valid certificates of optimality using learning-based methods. Building on this methodology, this paper proposes, for the first time, a dual conic proxy architecture for the semidefinite (SDP) relaxation of AC-OPF problems. Although the SDP relaxation is stronger than the second-order cone relaxation considered in previous work, its practical use has been hindered by its computational cost. The proposed method combines a neural network with a differentiable dual completion strategy that leverages the structure of the dual SDP problem. This approach guarantees dual feasibility, and therefore valid dual bounds, while providing orders of magnitude of speedups compared to interior-point algorithms. The paper also leverages self-supervised learning, which alleviates the need for time-consuming data generation and allows to train the proposed models efficiently. Numerical experiments are presented on several power grid benchmarks, which demonstrate significant speedup compared to a commercial interior-point solver. The proposed method also outperforms a state-of-the-art learning approach based on the second-order cone relaxation of AC-OPF, providing higher-quality dual bounds.",2,109 | Cobb Galleria Centre,Optimization Algorithms in Power System Operations and Planning,143
479,8951.0,Decentralized Operations Planning of Decarbonized Chemical Plants with Renewable-driven Transmission Systems,Academician,"As the electrification of industrial process heating becomes common it is increasingly important to integrate clean energy from power systems into chemical plant processes. However, integrating private chemical plant operations with the public power grid raises privacy concerns. Decentralizing large-scale unit commitment in power systems and electrified chemical process heating in a mixed-integer problem can maintain this privacy, encouraging stakeholders to adopt renewable energy use, thereby aiding decarbonization efforts. In this talk we will discuss decentralized optimization techniques that implement iterative ADMM along with a relaxation-to-integer transition to reach consensus. We demonstrate that by limiting shared information through decentralization, it is possible to satisfy demand needs of the chemical plant while maintaining the privacy of its operations, while maintaining good computational results in comparison to solving in a centralized manner. This approach eliminates the need to move data while considering operational constraints of the power system and chemical plant, while procuring expected results and retaining the privacy of operations.",3,109 | Cobb Galleria Centre,Optimization Algorithms in Power System Operations and Planning,143
480,5145.0,"Taking Heavy Industry ""Off the Grid""?",Academician,"Energy consumption for Heavy Industry (HI) operations is a major cost. As such, heavy industry firms are constantly seeking opportunities to reduce the high cost of energy such as electricity, gases, and fuels. In most cases, HI firms are high demand consumers of energy. As such, they are burdened with paying the prevailing market rates set by electric, gas, and fuel companies that are beyond their control that often leads to unforeseen price escalations. In response, heavy industry firms are seeking alternatives to offset or even replace their current sources of energy supply. The paper explores the potential of taking HI off the grid. Specifically, showcase a case study of a HI firm’s self-sustaining energy supply model. Based in Los Angeles, California, this firm is burdened with high-cost electricity, gases, and other sources of energy. Further, this firm is subjected price fluctuations with little or no warning making it difficult to plan and manage the risk of cost escalation. On the average, the cost of energy is x% of the firm’s revenue. To become less reliant on energy providers and manage cost, this firm is exploring alternatives and options such as generating and storing energy so it can become self-reliant and literally come “off the grid”. The case provides insights into feasibility studies, initial concepts, trade studies, business case (including funding streams), systems integration, and system lifecycle assessments. Students, faculty, and professionals of Industrial Engineering and Systems Engineering may find the particulars of this case beneficial.",4,109 | Cobb Galleria Centre,Optimization Algorithms in Power System Operations and Planning,143
481,5508.0,Co-ordinated Operation of Independent Water and Power System with Limit Information Sharing,Academician,"Water systems can be significant demand-response sources for electricity systems. Traditionally, electricity and water infrastructures are operated independently of one another, but their operations are inherently interconnected due to the energy consumption requirements of water systems and the flexibility that water tanks and variable speed pumps can provide to power systems. In this paper, we develop operational models for independently operated yet interrelated water and electricity systems to explore their coordinated operation. By simultaneously solving the Karush-Kuhn-Tucker (KKT) conditions of both system models, we explored how price and information exchange between the two systems can facilitate their coordinated operation, and we characterize the equilibrium states under varying levels of temporal and spatial granularity in information exchange. Using the case study, we show that fully efficient price and information exchange yields optimal outcomes, some coordination gains are lost if the price signal lacks spatiotemporal granularity. The results highlight the feasibility and benefits of our proposed coordinated operations approach between water and power systems.",1,109 | Cobb Galleria Centre,Advanced Optimization Methods for Critical Energy Systems,144
482,8909.0,"A Stochastic Optimization Framework for Condition-Based Maintenance, Crew Management, and Spare Parts Logistics in Solar Energy Systems",Academician,"This study presents a stochastic optimization framework for condition-based maintenance scheduling in solar energy systems, considering production, spare parts logistics, and crew management. The proposed framework leverages a detailed model to optimize hourly operations over a 30-day timeframe with a rolling-horizon fashion, integrating maintenance signals derived from the remaining lifetime distributions of each photovoltaic (PV) unit across various locations. The objective of the model is to minimize the expected cost of preventive and corrective maintenance activities, inventory holding and order, crew routing, and penalty for deviation in the crew routing plans while maximizing expected production revenue. Expected revenue is calculated through hourly and daily electricity prices and production scenarios generated from real-world data for each location, while the expected costs are obtained using uncertain maintenance duration. This study offers a comprehensive approach to asset management through efficient crew management and spare parts logistics, leveraging opportunistic maintenance and optimal crew routing between different asset locations, while balancing costs and reliability. The proposed approach provides significant advantages over the benchmark policies regarding efficiency and reliability.",2,109 | Cobb Galleria Centre,Advanced Optimization Methods for Critical Energy Systems,144
483,6309.0,Characterizing Price-Responsive Demand Flexibility from Electrified Supply Chains,Academician,"A target has been set to have 100% carbon pollution-free electricity by the year 2035 in the US, requiring an increased reliance on renewable sources. However, as renewable energy sources become integrated into power systems, they introduce fluctuations in power supply due to their stochastic nature. Battery Energy Storage Systems (BESS) are typically used to address these fluctuations but are unsuitable for longer timescales due to the associated costs. As of now, electric freight transportation in supply chains is in the early stage of deployment. This paper presents a solution to help mitigate the challenge associated with renewables and reduce the variation in supply by leveraging the flexibility of supply chains and integrating electrified supply chains into power systems planning and operation. We propose a model for the coupled system containing constraints from both the system's transport and manufacturing processes. The model utilizes the batteries of the electrical trucks in the supply chains to help manage fluctuations from renewables on the power system side. A power penalty is implemented for non-renewables to incentivize the use of renewables. The proposed model incorporates Vehicle-to-Grid (V2G) technology and smart charging. To demonstrate the effects of our model, we conduct a realistic case study of the electrified cement supply chain in the Southeast of the United States. Different levels of power penalties are applied to show the impact of varying incentives. We demonstrate that, unlike BESS, our proposed model combined with supply chain flexibility can increase the utilization of renewables for lower incentives.",3,109 | Cobb Galleria Centre,Advanced Optimization Methods for Critical Energy Systems,144
484,8891.0,Hybrid Quantum Classical Computing for Energy System Management,Academician,"In this talk, we will discuss how to utilize the hybrid quantum classical computing techniques to solve the energy management challenges. We will introduce the hybrid quantum classical Benders’ decomposition algorithms and their application in energy management systems. We will show the simulation results to verify our proposed algorithm and the advantages of quantum computing in the cloud.",4,109 | Cobb Galleria Centre,Advanced Optimization Methods for Critical Energy Systems,144
485,6373.0,A Data-Driven Optimization Approach for Community Resilience Enhancement During Disasters Leveraging Electric Vehicles,Academician,"Natural disasters pose a significant risk to critical infrastructure, leaving essential facilities such as hospitals, and fire stations vulnerable to power outages. Traditional backup power solutions, such as diesel generators, often lack the capacity for prolonged outages, while repair crews face substantial delays in restoring the power grid. This study presents an innovative optimization approach using electric vehicles (EVs) to restore critical isolated loads, ensuring that vital services remain operational during disasters. This problem’s complexity surpasses traditional vehicle routing problems due to several unique features: (1) it combines elements of the traveling salesman problem, timely demand, load shifting, and split delivery; (2) it requires energy and transportation layers with state-of-charge (SOC) tracking; (3) EVs may need to make multiple back-and-forth trips between shelters and charging stations; and (4) route interdependency of vehicles. To address these complexities, we developed an efficient mixed-integer-linear programming model and employed a branch and price solution approach that efficiently manages the routing and scheduling of EVs to meet the demand of critical isolated loads. Realistic case studies, based on data from Florida hospitals, demonstrate the effectiveness of our model and solution approach, solving the problem accurately within a one-hour time limit. Specifically, our proposed branch and price algorithm can solve the problem up to 120 times faster than Gurobi. This study highlights the potential of EV-powered backup systems to enhance community resilience, offering a sustainable, adaptive alternative to traditional generators in supporting critical infrastructure during extended power outages.",1,114 | Cobb Galleria Centre,"Smart Microgrids for Resilience, Sustainability, and Clean Energy Transition",145
486,6522.0,From Diesel-Based to Hybrid Microgrids in Developing Countries,Academician,"Many developing countries suffer from conflicts, instability and corruption, causing the national electricity grid to be unreliable or with limited geographical coverage. While previous literature has covered the cases of poor regions with very low demand levels and purchasing power, other developing countries with better economic conditions yet dysfunctional governments such as Lebanon, Iraq and Nigeria remain understudied. Over the last years, such countries witnessed a proliferation of neighborhood diesel generators (DG) where each DG owner fully controls a parallel distribution grid, independent from the national grid and sells electricity unidirectionally to a nearby customer base. Hoping to achieve a level of energy independence, many households and industries recently installed their own solar photovoltaic (PV) systems. Despite their competitive economics, these independent setups result in large losses of PV-generated energy during sunny hours, and intermittent availability otherwise. Our key research question is: under what conditions is the diesel generator owner incentivized to allow these PV owners to exchange electricity with this parallel grid, transforming it into a microgrid? The DG owner's objective is to maximize profit while satisfying the demand of its customer base. Key decisions that the DG owner has to make are: (1) the size of PV capacity allowed into the microgrid, and (2) the market conditions that enable the PV system owners to accept joining the microgrid. We model the problem as a nonlinear mixed integer program, then we apply it on a concrete case study from Lebanon generating relevant policy insights.",2,114 | Cobb Galleria Centre,"Smart Microgrids for Resilience, Sustainability, and Clean Energy Transition",145
487,6807.0,Optimizing Voltage and Frequency Stability in Renewable Microgrids Using Reinforcement Learning,Academician,"Microgrids come with high penetration of renewable energy like solar become obvious to meet sustainable development goals. However, renewable energy has no rotational inertia like the conventional power grid, which shows resilience against voltage and frequency change to maintain grid stability. To establish a microgrid as an independent and reliable power system a virtual inertia must be mounted. In this paper, a reinforcement learning-based droop-virtual inertia is incorporated into the microgrid system based on a vehicle-to-grid (V2G) control approach. The voltage magnitude and frequency at the point of common coupling constitute the microgrid states. The Q-learning algorithm is employed to enhance activities according to the status of the microgrid system. To validate the algorithm in one case at fixed 1 kW/m 2 solar irradiation, a load of 180 kW and 40 kVAR was added for 1.5 s. In another, solar irradiation is varied for 2-6 seconds at the power generation end. In both cases, upon activation of the devised control mechanism, the output voltage and current are effectively regulated around their reference values. The suggested microgrid architecture and its associated control mechanisms have been rigorously tested in OPAL-RT to confirm their efficacy, and the real-time simulation outcomes suggest several possible applications in the advancement of the smart grid.",3,114 | Cobb Galleria Centre,"Smart Microgrids for Resilience, Sustainability, and Clean Energy Transition",145
488,6973.0,Optimal Scheduling of Decarbonized Microgrid of Steam Cracking for Clean Olefins Production,Academician,"Olefins are essential building blocks in producing various chemical products, including plastics, detergents, adhesives, rubber, and food packaging. Ethylene, the most significant olefin with global production exceeding 200 million metric tons annually, is almost exclusively produced via steam cracking of hydrocarbon feedstocks like ethane, propane, and naphtha. However, steam cracking is among the most energy- and carbon-intensive processes in the chemical industry. As the U.S. shifts toward renewable energy, electric cracking technology offers a promising route to decarbonize this process. Yet, the large scale of ethylene plants, continuous operation needs, and the intermittent nature of variable renewable electricity (VRE) make it challenging to rely fully on electrified cracking, rendering large-scale storage and plant reconfiguration impractical. To address these challenges, we propose a gradual electrification approach for steam cracking, integrating various energy sources. This includes a hybrid setup where both electrified and conventional crackers are incorporated into the plant’s operation. Battery, electrolyzer, and hydrogen storage support continuous operation alongside onsite VRE. Electrified crackers utilize electricity from the main grid, in-house generators, fuel cells, and batteries, while conventional crackers operate on natural gas and methane byproducts from both cracker types. This transforms the ethylene plant into a microgrid capable of operating in grid-connected or islanded mode, offering enhanced resilience, economic efficiency, and flexibility. Our approach is built upon a differential-algebraic model to determine energy requirements of crackers, followed by a MILP model to explore economic and environmental trade-offs. Insights are drawn from a hypothetical ethylene plant on the Texas Gulf Coast.",4,114 | Cobb Galleria Centre,"Smart Microgrids for Resilience, Sustainability, and Clean Energy Transition",145
489,6145.0,Net-Zero Charging Infrastructure for Electric Aircrafts,Academician,"The objective of this paper is to address two design questions pertaining to aviation sustainability: First, how to design a solution for the establishment of a net-zero electric aircraft (EA) charging infrastructure for airports similar in size to Austin-Bergstrom International Airport (ABIA)? Second, what is an optimal solution for sizing spare batteries, mega-chargers, wind turbines and photovoltaics to promote net-zero energy operations for small- and medium-capacity EA fleets? We propose a queueing decision-making model to de-carbonize airport battery charging and swapping in the presence of demand-supply uncertainty under EA turn-around time constraints. We develop a data analytics model to characterize and predict battery swapping (BaS) and on-board mega-charging (MC) service operations based on aircraft arrival rate and layover wait times from Southwest Airlines. This optimization problem is formulated as a two-stage programming model: first, optimizing the spare battery inventory and mega-charger installation; then determining the aircraft capacity design with onsite renewable generation technology and capacity optimization. This battery service modeling will yield throughput information at 1000kW charging levels and provide insight into BaS processes for commercial EA services. The first managerial implication is to minimize the annualized battery MC facility cost from aircraft turn-around time to establish a foundation for the net-zero charging infrastructure design. Second, the intermittent power onsite wind and solar energy production systems must be coordinated appropriately to meet the design requirements. Thus, the necessary sizing of wind turbines and solar panels will be determined in conjunction with hourly weather conditions to ensure enough green energy supply.",1,109 | Cobb Galleria Centre,Optimizing Electric Mobility Infrastructure for Intermodal and Net-Zero Futures,146
490,6950.0,Charging Station Placement Optimization for Electric Vehicles Integrated with Rail Transit,Academician,"The implementation of consumer electric vehicles to reduce use of fossil fuels may be limited by the availability of charging options. In this paper we examine the optimal placement of charging stations for daily commuters with the integration of rail public transit as a commute option. We present and solve an integer programming model that finds the optimal charger placement locations in Atlanta using existing rail transit options. Our model places charger stations to serve all daily commuters into and out of Atlanta who do not have access to charging at home. It places fast-charging stations for use in each tract and slow-charging stations at each rail station for commuters who use rail transit. We optimize for total commuter capacity and carbon emissions separately and compare resulting station placement. We find that when we bound commuter capacity, it is possible to find a solution nearly as good as the optimal solution for carbon emissions, and vice versa. We also consider the effect of varying rail commuter capacity on resulting total commuter capacity and find a recommended train frequency for each segment of track.",2,109 | Cobb Galleria Centre,Optimizing Electric Mobility Infrastructure for Intermodal and Net-Zero Futures,146
491,6138.0,The Importance of Capturing Power System Operational Details in Resource Adequacy Assessments,Academician,"Traditional methods for assessing the resource adequacy (RA) of a power system are becoming obsolete due to emerging trends such as the increasing deployment of variable renewable energy and storage. Consequently, analysts are recommending that RA be assessed using a Monte Carlo simulation approach that models chronological power system operations over many instances of possible operating conditions. However, this approach is necessarily more complex and computationally demanding, which is an obstacle to real-world implementation. In this study, we investigate which operational details of power systems are important to capture in order to accurately evaluate a system’s RA, versus details that add complexity but do not meaningfully affect RA results. To do so, we develop a probabilistic RA assessment framework by adapting an existing production cost model and apply it to a case study based on the IEEE Reliability Test System. Our results indicate that multi-year data, storage dispatch, and transmission limits are key details to incorporate. Accurate RA results can be obtained using non-economic dispatch strategies as long as they are coordinated with detailed operational strategies. We also demonstrate how popular expectation-based RA metrics can mask important differences in the characteristics of loss of load events.",1,109 | Cobb Galleria Centre,Innovative Modeling and AI Applications for Sustainable Energy Systems,147
492,5770.0,Closing the Feasibility Gap: Modeling to Generate Alternatives for AC Topology Optimization,Academician,"Routine changes to power transmission systems' topology can alleviate congestion, maintain supply security, and increase utilization of renewable energy. Finding the optimal transmission network topology takes the form of a mixed-integer non-linear programming problem (MINLP), which can't presently be solved at scale in the required time. Various linearization methods exist, most famously the DC optimal transmission switching (DC-OTS) problem. Although this approximation sometimes yields good results, the resulting topology can often have no feasible solution to the non-linear problem. This paper introduces the use of modeling to generate alternatives (MGA) as an extension to the DC-OTS problem. We demonstrate that MGA successfully identifies good solutions for all 13 considered scenarios, all of which had infeasible topologies using DC-OTS.",2,109 | Cobb Galleria Centre,Innovative Modeling and AI Applications for Sustainable Energy Systems,147
493,6652.0,Investigation of the Impacts of Training Sample Sizes and Features on Wind Speed Prediction Accuracy of Long Short-Term Memory Method,Academician,"The accuracy of wind speed prediction is critical to wind farm development and operation, including site and turbine selection, energy production forecasting, etc. The increasing utilization of wind energy has required scientists to improve the prediction accuracy of the current approaches to maximize wind energy generation and utilization. Machine learning methods have shown advantages in accurately forecasting future data, such as wind conditions. This study investigates the impacts of different factors related to the Long Short-Term Memory (LSTM) method on the accuracy of predicting short-term wind speeds. An LSTM model was trained using different sample sizes of training data and features, while the testing accuracies of the trained LSTM models were used to evaluate the models and compared to investigate the impacts of training data sizes and features. Due to the complex nature of wind, wind speed patterns vary at different locations. In this project, three locations with different terrain and weather conditions were selected. The trained LSTM model using data from one location was tested at other locations to investigate the accuracy of the LSTM modes at different locations. This research focuses on utilizing datasets that vary in size and location and different features of the model to observe and improve model performance for training purposes.",1,109 | Cobb Galleria Centre,"AI and Simulation for Energy Forecasting, Infrastructure Planning, and System Insights",148
494,8803.0,DeepMIDE: A Multivariate Spatio-Temporal Method for Ultra-Scale Offshore Wind Energy Forecasting,Academician,"To unlock access to stronger winds, the offshore wind industry is advancing with significantly larger and taller wind turbines. This massive upscaling motivates a departure from univariate wind forecasting methods that traditionally focused on a single representative height. To fill this gap, we propose DeepMIDE—a statistical deep learning method which jointly models the offshore wind speeds across space, time, and height. DeepMIDE is formulated as a multi-output integro-difference equation model with a multivariate, nonstationary, and state-dependent kernel characterized by a set of advection vectors that encode the physics of wind field formation and propagation. Embedded within DeepMIDE, an advanced deep learning architecture learns these advection vectors from high-dimensional streams of exogenous weather information, which, along with other parameters, are plugged back into the statistical model for probabilistic multi-height space-time forecasting. Tested on real-world data from future offshore wind energy sites in the Northeastern United States, the wind speed and power forecasts from DeepMIDE are shown to outperform those from prevalent time series, spatio-temporal, and deep learning methods.",2,109 | Cobb Galleria Centre,"AI and Simulation for Energy Forecasting, Infrastructure Planning, and System Insights",148
495,8655.0,Dual Interior-Point Optimization Learning,Academician,"In many practical applications of constrained optimization, scale and solving time limits make traditional optimization solvers prohibitively slow. Thus, the research question of how to design optimization proxies -- machine learning models that produce high-quality solutions -- has recently received significant attention. Complementary to this research thread, which has primarily focused on learning primal solutions, this paper studies how to provide learning-based optimality guarantees by learning dual feasible solutions. The paper makes two distinct contributions. First, to train dual optimization proxies, the paper proposes a smoothed self-supervised loss function that augments the traditional self-supervised loss with a dual regularization term. Second, the paper derives closed-form dual completion strategies for several classes of dual regularizations, eliminating the need for computationally-heavy implicit layers. Numerical results are presented on large optimal power flow problems and demonstrate the effectiveness of the proposed approach. The proposed dual completion outperforms methods for learning optimization proxies which do not exploit the structure of the dual problem. Compared to commercial optimization solvers, the learned dual proxies achieve optimality gaps below 1% and several orders of magnitude speedups.",1,109 | Cobb Galleria Centre,AI-Driven Optimization and Forecasting for Decarbonized Energy Systems,149
496,8869.0,Decentralized Importance Sampling in Variational Autoencoders to generate Industrial Carbon Emissions Scenarios,Academician,"Decarbonization of energy-intensive industries like chemical process heating is expected to rely heavily on electrification through existing, renewable-driven power system infrastructure. Operations planning for decarbonized industrial and power system processes requires data-driven solutions that account for uncertainties across both sets of stakeholders. Stochastic optimization (SO) solutions for decision-making under such uncertainties require data-driven scenarios that capture spatiotemporal interdependencies the fidelity of which can severely impact SO solution quality. Additionally, the distribution of generated scenarios must also be estimated to align closely with target distributions contained in stakeholder datasets. Finally, these scenarios must also be generated from siloed data across diverse industrial stakeholders regarding data residency and privacy requirements. In this talk, we explore the potential of variational auto-encoders (VAEs) and their decentralized variants in importance sampling - a method for computing mathematical expectations with respect to a target distribution - on scenario distribution modeling to address these challenges effectively. Using real-world carbon emissions data, we demonstrate how these models capture spatial and temporal interdependencies across multiple chemical plants to generate high-fidelity scenarios. Our results also highlight how these generated scenarios, along with their associated importance weights obtained from decentralized training and inference, are utilized to drive reliable and efficient SO solutions when compared with other state-of-the-art methods.",2,109 | Cobb Galleria Centre,AI-Driven Optimization and Forecasting for Decarbonized Energy Systems,149
497,9066.0,Enhanced Renewable Energy Forecasting and Operations through Probabilistic Forecast Aggregation,Practitioner,"Accurate and reliable forecasting of renewable energy generation is crucial for the efficient integration of renewable sources into the power grid. In particular, probabilistic forecasts are becoming essential for managing the intrinsic variability and uncertainty of renewable energy production, especially wind and solar generation. This paper considers the setting where probabilistic forecasts are provided for individual renewable energy sites using, e.g., quantile regression models, but without any correlation information between sites. This setting is common if, e.g., such forecasts are provided by each individual site, or by multiple vendors. However, to effectively manage a fleet of renewable generators, it is necessary to aggregate these individual forecasts to the fleet level, while ensuring that the aggregated probabilistic forecast is statistically consistent and reliable. To address this challenge, this paper presents the integrated use of Copula and Monte-Carlo methods to aggregate individual probabilistic forecasts into a statistically calibrated, probabilistic forecast at the fleet level. The proposed framework is validated using synthetic data from several large-scale systems in the United States. This work has important implications for grid operators and energy planners, providing them with better tools to manage the variability and uncertainty inherent in renewable energy production.",3,109 | Cobb Galleria Centre,AI-Driven Optimization and Forecasting for Decarbonized Energy Systems,149
498,6007.0,Exploring the Impact of ESG Performance on Corporate Financial Costs: Dynamics and Implications for Capital Financing,Academician,"This study explores the dynamics between corporate Environmental, Social, and Governance (ESG) performance and capital costs, with a particular focus on financial costs. As corporations increasingly align with ESG objectives, understanding how these commitments impact financing becomes crucial for decision-making and investor confidence. This research aims to develop a methodology that evaluates the correlation between ESG scores and financial costs, providing insights into whether stronger ESG performance lowers capital costs by reducing perceived risk or improves access to more favorable financing terms. The approach will involve analyzing datasets on ESG ratings and capital structure, potentially utilizing regression analysis or machine learning to identify patterns and quantify the financial impact of varying ESG performances. Expected insights include identifying specific ESG factors most influential in financing decisions and understanding their relevance across industries. The motivation behind this study is to equip corporations and investors with data-driven insights, encouraging ESG-friendly practices by demonstrating their tangible benefits on capital costs, while supporting financial institutions in risk assessment and pricing decisions aligned with sustainable development goals.",1,104 | Cobb Galleria Centre,Manufacturing and Environmental Economics,150
499,6933.0,Predicting The S&P 500 Price Based On Various Climate Disasters Using Supervised Learning,,"This paper presents a study about stock market prediction based on various supervised learning methods using climate disaster data to help predict the prices of the S&P 500. The objective of this paper is to determine the types of climate disasters and their impact on the price of the S&P 500. Climate change is a growing topic in various areas of research and with the growing demand for climate change results and impact, interdisciplinary research is becoming increasingly more important. Climate Finance is a growing field which needs new approaches and research. In particular, this field needs more in-depth studies on the potential financial impacts of climate disasters. The research problem at hand is not only determining whether or not climate disasters have an impact on the price of the S&P 500, but also the amount of impact (ex. Index price) and more specifically what type of disaster has the most impact and why. There will be a selection of various types of models being developed and used for this study, including Linear Regression, Ridge Regression, Lasso Regression, and Neural Networks. The data being used for this research will be a set of stock market data providing various market datapoints (in addition to the chosen target) from the years 2019-2024 and for the second data set, it was pulled from EM-DAT, The International Disaster Database. The source database, is compiled from various sources, including UN agencies, non-governmental organizations, reinsurance companies, research institutes, and press agencies.",2,104 | Cobb Galleria Centre,Manufacturing and Environmental Economics,150
500,6413.0,A Cost-Benefit Investigation on the 3D Sand Printing-Metal Casting Manufacturing: Is it Worth It in High-Volume Production?,Practitioner,"The integration of 3D Sand Printing (3DSP) with conventional metal casting introduces a transformative opportunity for enhanced cost efficiency, quality, and scalability in manufacturing. This study focuses on the economic feasibility of integrating 3DSP with traditional casting. A comparative cost analysis between integrated 3DSP and metal casting versus stand-alone metal casting highlights differences in initial setup costs, long-term investments, and production efficiencies. This analysis examines both the initial investments (e.g., equipment purchase and process customization) and the long-term operational costs, including material use, defect management, and energy consumption. By focusing on different production volumes—ranging from low-mix, high-volume to medium-volume manufacturing—the study assesses production cost reductions that can be achieved through optimized process parameters. Cost predictions will be evaluated to indicate the potential for cost reduction at different production volumes. Budget allocation is further analyzed, focusing on investments in 3D printing equipment, casting process customization, operator training, and quality control tools. Emphasis is placed on ensuring optimal use of resources to maximize returns and enhance the feasibility of scaling the integrated process for high-volume production. Scalability and Return on Investment (ROI) will also be explored. Findings are expected to indicate significant cost efficiency for high-volume production and also highlight the benefits of adaptive scalability using 3DSP molds in traditional casting setups. This study aims to provide a holistic assessment of the economic benefits, supporting broader industry adoption of hybrid manufacturing techniques that contribute to both cost reduction and production efficiency.",3,104 | Cobb Galleria Centre,Manufacturing and Environmental Economics,150
501,6836.0,AN ECONOMIC ANALYSIS OF CHEMICAL VERSUS MECHANICAL APPROACHES TO LITHIUM-ION BATTERY RECYCLING: A CONCEPTUAL FRAMEWORK,Practitioner,"The rapid expansion of electric vehicles, portable electronics, and renewable energy storage systems has driven significant demand for efficient and sustainable recycling of Lithium-ion batteries (LIBs). For the initial process of separating a LIB cell components, existing recycling methods mainly rely on heating and chemical processes (hydro- or pyro-metallurgical method) to retain valuable metals. Existing methods while effective for large-scale industrial recycling, are energy-intensive, environmentally polluting, and recovering limited valuable components, specifically lithium. A new method using mechanical approach to separate the LIBs into individual components for the initial recycling process has been proposed. Without chemical treatments or high energy for heat, the mechanical approach separates LIBs into individual components, namely, anodes, cathodes, electrolytes, separator, and current collector, and allows for downstream recovery of critical materials such as Mn, Ni, Li, Co, Al, Cu, graphite, and PE or PP polymer. As the mechanical approach is fundamentally different from the chemical approach, an evaluation of the economic and environmental viability of the mechanical approach is needed. This study presents an initial framework as a foundation to compare the difference between the chemical and mechanical processes. Future work includes identifying further processes for material separation, standardizing batch sizes, quantifying inputs and outputs, and conducting economic analyses to derive insights.",4,104 | Cobb Galleria Centre,Manufacturing and Environmental Economics,150
502,9004.0,Risk Perception in the Interplay Between the Quantification of Risk and Objectives,Academician,"The purpose of risk management is to support informed decision making, considering that current practices do not align with what we know to be human mental approaches posing a significant problem for risk managers. One part of a solution is to state risks in terms of their effect upon program objectives as suggested by the ISO 31000 risk definition. Another approach is to communicate the combined effect of risks in terms of a decision maker’s perception of risk. The Entropy Decision Risk Model Utility (EDRM-U) developed and validated a method for quantifying risk perception in terms of risk aversion and risk sensitivity, while combining the effects of multiple uncertain choices on a single risk perception plot. By considering each risk as a choice, their combined effect can be understood in terms of decision difficulty for a person’s perception of risk. Additionally, the overall effect of all risks affecting an objective can be reduced to a single inverse decision, which characterizes where a set of choices is easiest to make (decision difficulty is minimized). The goal of this research is to improve project/program performance through better understanding of the effect of uncertainty focusing on the critical impact that risk perception plays on this process. This research is seen as providing value to both researchers and practitioners interested in this area of study and practice.",1,104 | Cobb Galleria Centre,Economic Modeling and Risk in Decision-Making,151
503,8968.0,Estimating Real-World Uncertainty: Case Studies and Stochastic Models for Graduate Engineering Economy,Academician,"In today's rapidly evolving industrial landscape, engineers must navigate complex financial decisions amidst uncertainty. This presentation introduces a novel approach to graduate-level engineering economy education that bridges theoretical foundations with practical applications, preparing students for real-world challenges. Our curriculum integrates traditional engineering economy principles with advanced analytical methods, focusing on simulation and stochastic modeling. Central to our approach is the use of Monte Carlo simulation to generate predicted output distributions, incorporating key factors such as capital allocation and costs, currency exchange, and inflation. The course structure revolves around a series of case studies of increasing complexity, designed to progressively build students' analytical skills and confidence. These case studies, drawn from diverse industrial sectors, provide a tangible context for applying theoretical concepts and tools. Students are taught to perform this analysis using fully-parameterized models in Excel, but are also exposed to more specialized tools like @Risk. Student feedback indicates that this approach enhances their understanding of risk and uncertainty in economic decision-making. Moreover, it cultivates critical thinking and modeling skills essential for addressing the multifaceted challenges in today's global industrial systems. Our approach directly addresses the current trend in engineering practice towards more sophisticated risk analysis and the increasing use of simulation tools for strategic planning. This presentation will outline the course structure, showcase key case studies, and discuss the integration of stochastic modeling into the curriculum. We will also share insights on student outcomes and lessons learned, providing valuable perspectives for educators and practitioners in the field of engineering economy.",2,104 | Cobb Galleria Centre,Economic Modeling and Risk in Decision-Making,151
504,5280.0,Using Risk Analysis to Optimize Engineering Economy,Practitioner,"The Engineering Economy of a project plays an important role in determining the overall success of the company or group involved. Project risks may affect the budget, schedule, performance, quality, or safety aspects of the project or system. These risks could mean the difference between a company’s economic stability and bankruptcy. This bibliometric analysis surveys publications from the last two decades that used risk analysis to reduce severity or occurrence of profit loss in a project. Economic risks can occur anywhere in a project lifecycle from the initial concept to product retirement. Risks that exist later in the project lifecycle can cause more severe economic consequences. Research into the topic of risk analysis has changed over the years as companies compare the tradeoffs of risk analysis in decision making. Using risk analysis results in additional costs from conducting thorough risk assessments, but it has the potential to provide financial benefit through risk treatment methods. The insights from this bibliometric analysis identify gaps that require further investigation, while also contributing to a better understanding of recent research regarding risk analysis to prevent unforeseen profit loss.",3,104 | Cobb Galleria Centre,Economic Modeling and Risk in Decision-Making,151
505,5505.0,Systems Engineering Methods and the use in Economics,Academician,"While it is known that systems engineering, and models are useful in all aspects of a lifecycle. The direct impact systems engineering has on the economics of a system are not as thoroughly understood. It has been shown that models using a systems engineering strategy is beneficial for organization to save money and improve how a system performs over its lifetime. The use of systems engineering strategies has direct impacts on the cost, schedule, and the sustainability of a system. Each aspect of the system life cycle has an economic value associated with it. As systems become more complex and larger systems of systems the economic value increases exponentially. A circular economy has been hypothesized for a long time as a systems engineering method will help expedite this economic method to being utilized. The circular economy has the same outline as a system in sustainment. This ideology would have implications on consumers being able to maintain a lower cost of living as systems would be repairable instead of replaced. Systems Engineering and the economy while not always thought of as the same, go hand in hand with their impact on the success and failure of a system.",4,104 | Cobb Galleria Centre,Economic Modeling and Risk in Decision-Making,151
506,5690.0,A Multi-Regional Economic Impact Analysis of Mississippi River Inland Waterway Transportation in Arkansas,,"The Mississippi River, a vital economic artery, has profoundly impacted inland waterway transportation, with Arkansas as a prime beneficiary. Relying on the river for efficient transport of raw materials and products, key industries including the steel industry illustrate the river’s broader economic influence across the Mississippi Basin. Using a multi-regional input-output (MRIO) model, we quantify the economic ripple effects of these transport efficiencies on Arkansas and regional partner states. This analysis highlights how barge transport access boosts Arkansas’s steel competitiveness and strengthens supply chain industries throughout the basin. These economic impacts benefit related industries, fostering job growth and economic prosperity across multiple states. This model offers a holistic perspective on the economic impact of river-based supply chains, such as steel production, on regional economies, emphasizing the intricate relationships between these regions and the need for coordinated policy efforts. By harnessing the economic potential of the Mississippi River inland waterway, Arkansas can strengthen its industrial base and promote economic growth across the basin.",1,103 | Cobb Galleria Centre,Infrastructure and Transportation Economics,152
507,6603.0,Evaluating the Economic Impact of Inland Waterways: A Comprehensive Review,,"Inland waterways serve as crucial infrastructure for sustainable economic growth, enabling efficient goods transport, alleviating road congestion, and bolstering local industries. This paper presents a systematic review of current methodologies used to evaluate the economic impacts of inland waterways, encompassing both established and innovative approaches. Specifically, we examine input-output (I-O) models, computable general equilibrium (CGE) models, and cost-benefit analysis in assessing direct, indirect, and induced economic impacts associated with waterway infrastructure. Case studies from North America and Europe illustrate the diverse applications of these models in assessing waterway infrastructure investments, maintenance programs, and policy impacts on trade flows and employment. The review also explores emerging trends in economic impact assessments, including the integration of environmental factors, digitalization, and climate resilience into economic evaluations, which influence both the economic valuation and future policy frameworks for waterway development. Challenges in data availability and the need for standardized economic assessment methods are discussed, with a focus on addressing these gaps for better policy outcomes. This paper aims to guide future research and inform stakeholders on the economic value of inland waterways, highlighting opportunities to strengthen infrastructure investments for sustainable regional and global development.",2,103 | Cobb Galleria Centre,Infrastructure and Transportation Economics,152
508,8680.0,Evaluation of E-Bikes: Paving the Way for Sustainable Urban Mobility in Smart,Academician,"Electric-assisted bicycles (E-bikes) are emerging as a sustainable and innovative solution to address the evolving urban mobility needs of future smart cities. This paper investigates the economic dimensions of E-bike adoption, highlighting their potential to enhance accessibility, reduce greenhouse gas emissions, and improve mobility for diverse populations. The study focuses on four key cost components integral to E-bike integration: (i) capital costs (production and distribution), (ii) operational costs (maintenance and energy), (iii) infrastructure costs (development of E-bike-friendly facilities), and (iv) externality costs (environmental and health impacts). By conducting a comprehensive economic evaluation of these factors, the study identifies cost-reduction strategies to make E-bikes more affordable and accessible. The findings aim to support the widespread adoption of E-bikes, increasing their market penetration and share. This research offers actionable recommendations for policymakers, manufacturers, and stakeholders, providing a roadmap for integrating E-bikes into urban transportation systems and advancing sustainable mobility solutions.",3,103 | Cobb Galleria Centre,Infrastructure and Transportation Economics,152
509,8989.0,"Multi-criteria decision analysis for foreign investment in tourism development in the province of Santiago, Dominican Republic: focus on health and business tourism.",Academician,"Tourism is a cornerstone of the Dominican Republic’s economy, welcoming millions of visitors annually. Recent years have seen a notable shift toward health and business tourism, driven in part by foreign direct investment (FDI). However, the absence of well-defined decision-making frameworks for investment in these sectors remains a critical gap, potentially influencing the success or failure of development projects. This study applies the Analytic Hierarchy Process (AHP) method within a multi-criteria decision analysis framework to identify key factors influencing FDI decisions in health and business tourism projects. The findings aim to support investors, policymakers, and stakeholders in transforming Santiago de los Caballeros into a premier destination for specialized tourism, promoting sustainable regional economic growth.",4,103 | Cobb Galleria Centre,Infrastructure and Transportation Economics,152
510,8834.0,"Enhancing curiosity, connectedness and value in an Engineering Economic Analysis Course",Academician,"The goal of KEEN’s Entrepreneurial Mindset (EM) is to incorporate learning that enhances students’ curiosity, connectedness of material, and creates value. This presentation provides a case study example for an engineering economic analysis course with the goal of creating the EM thinking and skills. The case study assignment and quantitative survey data will be presented to illustrate the value of the assignment. The students rated the ability to enhance curiosity as an average of 4.01 out of 5, creating connectedness across the course material as 4.18 out of 5, and creating value as 4.16 out of 5. The case study was highly effective in allowing the students to enhance and practice these skills and mindset.",1,103 | Cobb Galleria Centre,Economics of Education and Institutional Analysis,153
511,4847.0,Thermo-economics and the complexities of higher education economics: A visualization of the stratification of the system,Academician,"This paper thoroughly explores the homological relationship between thermodynamics and economics within the theoretical framework of Bertalanffy's general systems theory. Referred to as thermoeconomics, this work seeks to integrate foundational thermodynamic principles with the complicated economic dynamics prevalent in public, not-for-profit higher education institutions across the United States. These institutions confront the persistent challenge of dwindling governmental budgets juxtaposed with escalating concerns among the public regarding the rising tuition costs compared with perceived educational benefits. The overarching aim of this research is to initiate the development of robust predictive models explaining the complexities inherent in the economics of higher education. Such models are intended to empower and facilitate informed strategic decision-making processes for all stakeholders involved in higher education to optimize benefits while mitigating costs. At the heart of this scholarly pursuit lies the conception and refinement of a graphical framework integrating theories, concepts, applications, and elements from thermodynamics and economics into a unified and comprehensive structure. Ranging from the alignment of fundamental principles such as supply and demand with insights derived from the ideal gas laws to the nuanced exploration of the diverse entropic characteristics inherent in economic utilities, this comprehensive framework serves as an invaluable tool, outlining a clear research course and guiding future scholarly inquiries within this interdisciplinary domain.",2,103 | Cobb Galleria Centre,Economics of Education and Institutional Analysis,153
512,5383.0,Is the modern university losing its economic utility?: The entropic reality facing the future of higher education,Academician,"Amidst the rapid evolution of contemporary landscapes, this paper addresses whether the traditional college degree retains its economic utility considering emergent and transformative technologies. The examination unfolds through an exploration of the foundational tenets of economic utility, summarized within the paradigms of form, time, place, and possession. Form utility, intrinsic to the alignment of consumer needs with desires, assumes a principal role in this discourse. The burgeoning influence of social media and pop culture underscores a shifting paradigm where individuals achieve fulfillment and success without the traditional collegiate route, raising questions regarding the necessity of conventional higher education. Furthermore, the dimensions of time and place utility emerge as pivotal considerations. The proliferation of platforms such as Coursera and the University of Phoenix offering asynchronous learning modalities allows consumers to dictate both parameters of their educational pursuits, eliminating the constraints of conventional semester-based structures. Finally, the dimension of possession utility examines the fundamental essence of knowledge acquisition, examining whether the modality of knowledge acquisition is relevant to the consumer when using the knowledge gained. These deliberations highlight the current significance of the discussion and ensure its relevance in the evolution of higher education systems.",3,103 | Cobb Galleria Centre,Economics of Education and Institutional Analysis,153
513,6029.0,"Labor Utilization, Efficiency, and Productivity in Mushroom Farming",Academician,"One of the challenges that farmers in the mushroom industry face is the shortage of farm labor, a crucial resource required in all stages of production. This paper explores the causes of farm labor-related challenges and labor utilization for harvesting operations, the most labor-intensive section of mushroom cultivation. It applied two proficiency metrics (efficiency and productivity) for analysis. Data from a large-scale commercial farm was used to estimate the labor utilization parameters, including the number of workers, skill level, cost, and revenue generated. The analysis results indicate that the labor efficiency in the harvesting section is lower than what is reported in the literature on farm labor efficiency in the farming industry. Several approaches have been recommended to improve efficiency, including improving harvesting proficiency, revising wage, benefit, and incentive payments.",4,103 | Cobb Galleria Centre,Economics of Education and Institutional Analysis,153
514,4999.0,NON-FUNGIBLE TOKENS (NFTS): A PRELIMINARY ANALYSIS WITH COMPARABLE ASSETS,Practitioner,"Non-fungible tokens (NFTs) began gaining traction before COVID-19. Various factors related to the pandemic, such as lockdowns, social distancing, and increased online shopping, significantly boosted the popularity of NFTs throughout 2022. During this period, they became hot topics on social media, generating considerable hype. However, NFT prices experienced a rapid decline in 2023, but they show signs of recovery in 2024. A comparative analysis of NFT prices against other financial assets, such as cryptocurrencies, real estate, gold, bonds, and stocks, is needed. This analysis could demonstrate that NFTs are helpful for portfolio diversification. This research aims to establish a preliminary operational definition of NFTs, assess investment risks, and evaluate their performance compared to various assets through a literature review and analysis of existing studies. The findings indicate that NFTs and cryptocurrencies have emerged as alternative investments to traditional assets like stocks and bonds. NFTs offer unique identifiers and undisputed claims about ownership, but they also carry higher risks and greater volatility—although with the potential for higher returns than other asset classes. Preliminary analysis shows that NFTs outperformed other assets during the COVID-19 pandemic. Further research could provide deeper insights into the factors behind NFT prices' decline and subsequent resurgence. Additional analysis may clarify the proper drivers of strength within the NFT market. Ultimately, this research could benefit practitioners and economic analysts who want to incorporate NFTs into their portfolios and develop risk mitigation strategies for future financial shocks.",1,103 | Cobb Galleria Centre,Emerging Assets and Digital Economy,154
515,5216.0,A History of Utility Theory and Its Implications for Emerging Assets like NFTs,Academician,"The concepts of economic utility have evolved from a straightforward relationship centered on labor to a more complicated understanding that includes behavioral economics, which examines the interplay between economics and psychology in consumer behavior. By focusing on the utility of products, businesses can create offerings that more effectively meet their customers' needs. Non-fungible tokens (NFTs) have become one of the most popular applications of blockchain technology. However, many unknown factors remain regarding the value they provide. Both NFT platforms and creators often have a knowledge gap concerning the overall utility of NFTs. This study reviews the historical underpinnings of utility theory, emphasizing how it relates to NFTs. This work explores how utility theory can be applied to NFTs and develops an initial mathematical formulation of utility specifically for NFTs. This exploratory research seeks to bridge the knowledge gap surrounding NFTs by using utility theories and highlighting areas that can assist in the long-term sustainability of NFTs. The results indicate utility theory helps clarify aspects of behavioral economics such as scarcity, fear of missing out, anchoring, loss aversion, and regret aversion related to NFTs. Moreover, utility theory can be applied to understand the actual value of NFTs, assess associated risks, enhance user experience, and ensure long-term sustainability. This research will be beneficial for scholars interested in emerging technologies like NFTs and economic analysis of assets as well as providing practitioners and NFT Stakeholders potential insights into the application of utility theory in affecting customer engagement and loyalty.",2,103 | Cobb Galleria Centre,Emerging Assets and Digital Economy,154
516,5373.0,Corporate Venture Capital – A review of the state of the art,Academician,"Established companies are increasingly pressured by disruptive innovations or business models from emerging companies, so-called start-ups. This poses a considerable risk to the market position of established firms. Start-up collaborations are, therefore, attracting more and more attention to bring innovations into the company quickly and cost-effectively by working with start-ups. Presently, there is a professional approach to investing in start-ups driven by the necessity for external innovation, globalization, and digital technologies that have widespread industry impact, such as data analytics and artificial intelligence. The concept is corporate venture capital (CVC), which comprises equity-financed investments in emerging , unlisted companies. CVC emerged in the 1960s in the US and has developed into a rapidly growing and increasingly global phenomenon that is experiencing growing popularity and recognition not only in practice but also in academia. This article conducts an analysis of the literature on corporate venture capital from 2002 to 2023 in the Web of Science and Scopus databases, utilizing the Bibliometrix package for the R statistical programming language. By carefully analyzing publication volume, authorship, keywords, clustering, and timeline maps, this research identifies recent research developments, gaps, and insights of interest and value for the research and the developing practice of CVCs.",3,103 | Cobb Galleria Centre,Emerging Assets and Digital Economy,154
517,8827.0,The Role of Digital Technologies: Insights from the U.S. Digital Economy,Academician,"Industrialization is a crucial process to produce goods and services on a global scale. The emergence of the Fourth Industrial Revolution has introduced an array of digital technologies that have transformed our living by improving productivity and efficiency. Digital technologies are increasingly integrating and interconnecting our world through Information and Communication Technologies (ICT), encompassing many innovations such as the Internet of Services (IOS), Internet of Everything (IOE), Internet of People (IOP), Cyber-Physical Systems (CPS), Cloud Computing, Additive Manufacturing (also known as 3D Printing), Blockchain, and Artificial Intelligence (AI). Such digital technologies (as mentioned above) are driving a wide array of economic activities, including e-commerce, banking, logistics, transportation, manufacturing, and many other technology-driven innovations in the production of goods and services. The resulting digital economy stems from the dynamic interplay of billions of daily interactions and transactions among people, businesses, devices, and processes empowered through data. Understanding the growth of the digital economy offers valuable insights into the role and patterns of digital technologies in shaping a nation’s economic activities. This paper examines time series data from a sample of U.S.-based companies specializing in digital technology-related products and services to analyze the contribution and growth patterns of the digital economy within the broader context of the U.S. economy. These findings would provide actionable insights for researchers and professionals studying the factors influencing the economic health and development of nation-states.",4,103 | Cobb Galleria Centre,Emerging Assets and Digital Economy,154
518,6182.0,Optimizing Duck Meat Processing: A Comparison of Mixed-Integer Programming and Simulation-Based Models,Academician,"The duck meat processing sector has experienced rapid growth within the broader poultry industry, particularly in Asia, driven by rising global demand and an increasing need for efficient processes. In addition, the dynamic nature of the processing environment, with its variability in demand, carcass quality, and production constraints, poses unique challenges for duck meat processing. Optimizing duck meat processing is important for maximizing profitability, reducing waste, and meeting market demands. However, few studies have explored optimization in duck meat processing. Given the unique characteristics of duck meat, including its market segments and processing requirements, applying optimization models to this industry is essential. This paper investigates two optimization approaches tailored for duck meat processing: Mixed-Integer Programming (MIP) and Simulation-Based Optimization (SBO). Both models are developed and tested on a structured, real-world dataset to evaluate their performance on key metrics: profit maximization, waste reduction, demand fulfillment, and scalability. Through a comparative analysis, this study examines the strengths and limitations of each approach and offers insights into their applicability and effectiveness in the duck meat processing environment. The findings help understand the pros and cons of MIP and SBO for solving challenges faced with the duck meat processing industry. Future research on advanced optimization methods and decision-support systems to enhance operational efficiency and adaptability in duck meat processing is also discussed.",1,110 | Cobb Galleria Centre,Modeling & Simulation - Agriculture and Environmental,155
519,9054.0,Machine Learning Model Development for Optimizing Surface Temperature of TPMS Geometries in Atmospheric Water Generation Systems,Academician,"Atmospheric Water Generation (AWG) systems offer a sustainable solution to global water scarcity by extracting moisture from the air, leveraging innovative designs and advanced technologies to maximize efficiency. This study aims to optimize AWG performance through a machine learning-based analysis of six Triply Periodic Minimal Surface (TPMS) geometries: Gyroid, Schwarz, Lidinoid, Diamond, Split-P, and Neovious. These geometries, with their high surface-area-to-volume ratios, are designed to enhance heat transfer and improve condensation rates. A predictive machine learning model is developed to analyze the relationship between lattice parameters—such as lattice thickness, cell size, cell porosity, and surface area—and the top surface temperature, a critical factor for efficient water vapor condensation. The dataset, including structural and thermal simulation properties, was generated using nTopology software. Using this dataset, machine learning models such as linear regression, random forest, and gradient boosting are applied. The robustness and accuracy of predictions are ensured through hyperparameter tuning and k-fold cross-validation. The preliminary results demonstrate significant correlations between lattice design variables and top surface temperature, highlighting the potential for optimized configurations to enhance AWG efficiency. Additionally, the study provides a comparative analysis of the six TPMS geometries based on their performance and the accuracy of the predictive models. By minimizing reliance on labor-intensive experimental trials, this approach accelerates the design process and contributes to the development of efficient, scalable AWG systems. Integrating machine learning with innovative TPMS designs presents a promising pathway toward advancing sustainable water harvesting technologies to meet critical global water needs.",2,110 | Cobb Galleria Centre,Modeling & Simulation - Agriculture and Environmental,155
520,6389.0,Optimizing Production Strategies in Catfish Farming: A Decision Support System Using Mathematical Modeling,,"The catfish industry is a cornerstone of rural economies in the U.S., providing significant employment and economic benefits. However, optimizing production strategies in catfish farming to minimize operational costs while sustaining profitability faces substantial challenges. This study introduces a Mixed-Integer Linear Programming (MILP) model designed as a decision support system to aid catfish farmers in selecting the most cost-effective production strategies tailored to diverse pond environments. Addressing the complexities of production, feeding, fish growth variability, and economic constraints, the model systematically assesses the impact of various production strategies, including single- and multiple-batch systems, intensive aeration, and split-pond techniques. Using a comprehensive, real-life case study provided by Delta Research & Extension Center in Stoneville, Mississippi, the methodology validates the model by considering key parameters such as stocking densities, feed costs, and market pricing strategies to optimize overall operational expenses. Notable results reveal that while intensive-aeration and split-pond systems require higher initial investments, they are markedly more productive and profitable under scenarios of high market demand and adequate capital, offering substantial economic advantages.",3,110 | Cobb Galleria Centre,Modeling & Simulation - Agriculture and Environmental,155
522,6383.0,Agent-Based Inventory Management using Deep Reinforcement Learning: An Application in Timber Supply Chain,,"The timber supply chain is a vital component of the forestry industry that serves as a conduit between landowners and mills, providing various wood products for different industries. However, the timber supply chain experiences various challenges, such as fluctuating market demands, seasonal variations, and natural disturbances (e.g., hurricanes), which can severely disrupt the entire operation and inflate costs. To alleviate these stochastic and dynamic challenges, this study introduces a robust agent-based modeling approach utilizing Deep Reinforcement Learning (DRL) technique. Specifically, the problem is formulated within a Markov Decision Process (MDP) framework to enable the mill (considered as the agent) to optimize its inventory policies through trials and errors. This framework dynamically simulates interactions between mills and landowners, considering stochastic demand influenced by economic conditions and environmental factors and timber prices affected by hurricane-induced supply shocks. Later, Proximal Policy Optimization (PPO), a sophisticated policy gradient method, is employed to solve the proposed model by developing adaptive and optimal strategies for inventory management under uncertainty. Initial results reveal significant strategic improvements in decisions, confirmed by the consistent decrease in the total operational cost for the mill across simulated time steps. Various experiments are tested to gauge the model's adaptability to a wide range of operational scenarios, such as timber price fluctuation, hurricane severity, demand uncertainty, landowner availability, and different operational costs (e.g., storage cost) to assess their impact on the timber supply chain.",1,110 | Cobb Galleria Centre,Applications of AI/ML and Simulation,156
523,5809.0,Using AI to Enhance Quality of the 3D Sand Printing-Metal Casting Manufacturing.,Practitioner,"The integration of 3D sand printing (3DSP) with conventional metal casting presents an innovative solution to address the complexities of modern manufacturing in achieving high-quality, high-volume production with reduced lead times. To achieve these benefits of 3DSP requires optimizing the sand printing process parameters to obtain ideal casting molds. This study explores a hybrid modeling framework that merges physics-based simulations with data-driven predictive analytics and artificial intelligence (AI) techniques to optimize the 3DSP-metal casting process, focusing on quality improvement, defect minimization, and determining optimal parameters. MAGMA simulation software is used to take input 3DSP model casting parameters (e.g., binder composition, sand properties, pouring temperature, and cooling rates) and provides insights into solidification, thermal profiles, and the risk of defects, such as porosity and shrinkage cavities, occurring in the casting. Building on the simulation data, machine learning (ML) models (e.g., random forest, support vector machine (SVM), and neural networks) will be introduced to provide a predictive defect analysis. These ML models were trained on 3DSP process parameters and corresponding simulation data to predict potential casting defects, enabling proactive mitigation strategies. Next, genetic algorithms (GA) will be applied to explore the relationships learned by the machine learning models to find the optimal process parameters. The integrating of physics simulations, ML and GA creates an adaptive optimization loop that continuously refines the process, leading to improved efficiency and cost-effectiveness, thus supporting sustainability goals and advancing smart manufacturing.",2,110 | Cobb Galleria Centre,Applications of AI/ML and Simulation,156
524,9336.0,Learning Optimal Policies for Collaborative Robots Performing Overlapping Assemby Tasks with Human Workers,Academician,"Research on human-robot teaming in manufacturing has predominantly focused on optimizing collaboration in scenarios where humans and robots perform distinct, non-overlapping tasks. This study seeks to advance beyond this paradigm by enabling effective collaboration on overlapping tasks while accounting for the differing skills and abilities of human and robotic team members. Specifically, we aim to develop an AI-enabled robotic partner capable of dynamically assessing the skill level of its human counterpart and adapting its actions to minimize the total time for the joint team to complete assembly tasks. By leveraging stochastic models of human performance derived from controlled laboratory experiments, we demonstrate in a computer-simulated environment that a robotic partner trained using reinforcement learning can significantly enhance overall team performance.",3,110 | Cobb Galleria Centre,Applications of AI/ML and Simulation,156
525,5605.0,Decentralized Training Decentralized Execution using a Shared Network (DTDE-SN) – A new reinforcement learning approach for epidemic modeling,,"Multi-agent reinforcement learning (MARL) methods are suitable tools for modeling highly dynamic decision analyses such as intervention analyses in epidemic outbreaks across multiple jurisdictions. MARL models allow us to consider jurisdictional mixing interactions in the decision-making policies, as they evaluate decisions specific to each jurisdiction while considering the interactions between all jurisdictions. In multi-agent reinforcement learning, there are three commonly used training and execution paradigms. Centralized training with centralized execution (CTCE) converts a multi-agent problem into a single-agent problem by considering the joint state/action space of all agents as that of a single virtual agent. Centralized training with decentralized execution (CTDE) trains agents using centralized information but executes their actions independently based on their local observations. Decentralized training with decentralized execution (DTDE) trains agents independently to optimize team rewards, and each agent regards other agents as a part of the environment. We propose a new training and execution approach, where a single deep learning model is trained for all agents (which considers the interactions between agents) but each agent makes its own independent decisions. The standard DTDE approach trains a separate network for each agent. This approach is computationally complex, and this complexity grows with the number of agents. The new method, which we call decentralized training with decentralized execution using a shared network (DTDE-SN) models interactions between agents while reducing computational complexity. We have used a simple epidemic simulation model to demonstrate that the DTDE-SN method can identify optimal intervention decisions.",4,110 | Cobb Galleria Centre,Applications of AI/ML and Simulation,156
526,5613.0,ANALYSIS OF SITUATIONAL STATISTICS IN TREE EXPANSION BASEBALL SIMULATIONS,,"This research leverages situational statistics in baseball, particularly batting performance with runners in scoring position (RISP), to enhance game strategy and create a more accurate value of expected runs. The situational statistics highlight variations in player performance under specific game scenarios, which can influence game outcomes and strategic adjustments to improve lineup performance. The Discrete Event System Specification (DEVS) model has been proposed to simulate baseball games by assessing potential outcomes based on lineup configurations with probabilities and adjusting for RISP situations with 0% error. The DEVS model incorporates the atomic models to transmit player statistics for each at-bat outcome to build an expansion tree, refining the simulation according to the current base configurations. The experimental results show that incorporating RISP-adjusted statistics, such as Kyle Higashioka’s improved single-hit rate from 13.08% to 16.44% with RISP, can increase the expected runs per game, revealing substantial strategic potential. The DEVS model outputs an expected run value more accurately to a real-life situation than the baseline model which does not include RISP situational statistics. This proposed model can be expanded to explore situational statistics across various game conditions further, enhancing the accuracy and application of game simulations, such as implementing dynamic probabilities that update each batter’s array of statistics as each node of the tree is generated.",1,110 | Cobb Galleria Centre,Modeling & Simulation - Data Analytics,157
527,6259.0,NON-LINEAR HYBRID RANDOM PROJECTION TECHNIQUES FOR HIGH-DIMENSIONAL DATA,Academician,"Dimensionality reduction of large datasets is essential in fields such as cloud computing, genetic distribution, image processing, and smart grids, which require high-dimensional data analysis. Despite their efficiency, Linear Hybrid Random Projection methods are limited in capturing the inherent structure of complex nonlinear data. This limitation necessitates the research to introduce a novel Nonlinear Hybrid Random Projection (NHRP) method, combining normal random projection (NRP) and plus-minus random projection (PMRP) with nonlinear transformations. This approach aims to enhance the representation of nonlinear relationships, thereby improving the accuracy and effectiveness of dimensionality reduction in various applications such as cloud computing, genetic distribution, image processing, and smart grids. NHRP dynamically balances NRP and PMRP contributions using a blending parameter, effectively reducing dimensions while preserving complex relationships. The effectiveness of NHRP is evaluated through simulations and real-world data analyses, including datasets from human activity recognition (HAR) , and hyperspectral images. Comparative studies with NRP and PMRP show that NHRP consistently outperforms in terms of distance distortion, preserving data structure across various datasets and settings. NHRP represents a significant advancement in dimensionality reduction, minimizing information loss and enhancing efficiency and precision in machine learning, intelligent systems, bioinformatics, remote sensing, and artificial intelligence.",2,110 | Cobb Galleria Centre,Modeling & Simulation - Data Analytics,157
528,6805.0,Maximizing Returns in Stock Trading: An Integrated Discrete Event System Specification and Q-Learning Framework,,"This study aims to obtain a reliable stock trading strategy that maximizes return rates by integrating Discrete Event System Specification (DEVS) and Reinforcement Learning (RL). Stock trading strategies are utilized by investors as a way to make decisions with a higher confidence of return. This is challenging for investors since personal bias and small sample sizes are limiting factors in finance. To overcome these limitations, Q-Learning, an off-policy RL algorithm, is employed to discover the optimal trading strategy of an index fund. This eliminates bias as there is no human intervention and also allows training of many decades expanding the models knowledge of the index. This agent was then paired with DEVS which utilizes a tree expansion process for modeling systems with discrete random variables, this enables the model to determine the set of decisions with the highest expected profits. Through the integration of these techniques, the most optimal trading strategy can be found. Historic trading data of the Dow Jones Index (DJI) is used to test the performance of the integrated model by deciding to buy or sell one share of DJI each day. The results demonstrated that 95% of the maximum possible profits over that period are expected. In the future, the model will be expanded to optimize trading strategies and estimate expected returns for multiple stock indices. This research can also be advanced by enabling the model to trade in shorter time periods such as each hour or minute rather than its current one trade per day.",3,110 | Cobb Galleria Centre,Modeling & Simulation - Data Analytics,157
529,6174.0,Generative AI and Adaptive Agent-based Simulation for Predicting Essential Supply Demand in Data-Scarce Disaster Regions,Academician,"Contemporary society confronts an increasing frequency and unexpected occurrence of natural disasters by climate change. Building predictive models based on statistical learning approaches (i.e., machine learning) for unique incidents occurring in unexpected locations is challenging due to limited historical data. The impacts of disasters on these less predictable regions are likely more critical than those of regularly affected or predictable locations. This research proposes a new paradigm that utilizes generative AI and agent-based simulations to predict the demand for essential supplies in data-scarce environments under unexpected disasters. The approach focuses on adaptive scenarios and data updates via generative AI models to reflect real disaster scenarios, which traditional simulation modeling lacks due to its reliance on predefined rulesets. Our framework combines synthetic data generation through generative models with LLM-driven agents representing residents, logistics operators, suppliers, and government officials to perform dynamic and adaptive disaster simulations. This framework employs recursive feedback mechanisms by generative models initiated from city map data to reflect each agent’s situation, supply demands, and accessibility updates. We utilize Unity software to create a realistic, interactive 3D environment that allows LLM-driven agent’s communication and response to the disaster scenario, and AnyLogic software serves as the core backend for scenario management, feedback processing and generative modeling. To validate our framework and constructed models, we conducted a sensitivity analysis, component-specific validation, and agent Turing test for LLM-driven agent verification. This novel approach provides meaningful insight into disaster prediction and supply-demand forecasts in data-scarce regions by providing adaptive and responsive simulation models.",4,110 | Cobb Galleria Centre,Modeling & Simulation - Data Analytics,157
530,8766.0,Agent-based modeling of Search and Neutralize Operations in Undersea Warfare,Academician,"Effective ""Search and Neutralize"" operations are paramount for maintaining maritime security and protecting strategic assets. This study employs an Agent-Based Modeling (ABM) approach, aligned with a tactical Mixed-Integer Programming (MIP) model, to simulate and optimize the dynamics of searchers and high-risk targets within a maritime environment. The ABM incorporates complex operational assumptions and constraints, facilitating a comprehensive analysis of search strategies and resource allocation. A key objective of this research is to address the impact of weather uncertainties on search operations, which the MIP model can not capture. Weather conditions, including wind speed, visibility, precipitation, and storm occurrences, are dynamically modeled to influence the operational capabilities of search assets. The simulation incorporates Areas of Uncertainty (AOUs) that expand over time after the detection of a threat by surveillance assets. These AOUs are also influenced by changing weather conditions, necessitating adaptive search strategies to maintain operational effectiveness. To enhance the efficiency of search operations, this study introduces an efficient search heuristic. This heuristic optimizes the pathfinding process of search assets, enabling them to navigate AOUs more effectively by prioritizing areas with higher probabilities of target presence, improved observation capabilities, and greater strategic value. The findings aim to inform the development of optimized search strategies and resource deployment plans, ensuring robust maritime security operations even in the face of environmental uncertainties.",1,110 | Cobb Galleria Centre,Modeling & Simulation - Defense,158
531,6353.0,Deep Bayesian Inverse Reinforcement Learning Technique to Assess Building Safety under an Active Shooter Violence Situation,,"Active shooter incidents are considered one of the deadliest yet increasing threats to public safety. There is an urgency to understand the dynamics and civilian response under an active shooter violence scenario to help manage and mitigate future incidences, saving uncountable lives. Since the majority of the past incidents occurred in public buildings (e.g., shopping malls, schools, and churches), it is necessary to methodologically assess the safety of multi-storied public buildings under an active shooter violence situation. As such, this study proposes an advanced Deep Bayesian Inverse Reinforcement Learning (DBIRL) technique to advance our understanding of civilian response behavior in complex multi-storied building environments under an active shooter violence situation. We test the performance of our proposed DBIRL technique under different building configurations (e.g., location and number of entrances/exits, stairs, floors, and possible hiding places) and shooter attributes (e.g., number of shooters, firearm types). Each experiment is evaluated under different performance metrics, such as the percentage of civilians who can successfully hide or exit the building or still reside under varying building configurations. We derive several observations from the experiments, such as considering different exits/entrances on the first floor and emergency exits on the second floor help reduce the number of trapped civilians, and in a case that the incident involves multiple shooters, more hiding places (such as locked classrooms) play a significant role in surviving civilians under an active shooter violence situation.",2,110 | Cobb Galleria Centre,Modeling & Simulation - Defense,158
532,5598.0,Real-Time Digital Twin Development and Augmented Environment Generation for Multi-Agent Robotic Systems,Academician,"This study focuses on the design and development of a real-time digital twin of a multi-agent robotic system utilizing the NVIDIA Omniverse platform. Multi-agent robotic systems and their Digital Twin systems often struggle to make effective, collaborative decision-making in a dynamic environment. These limitations are mainly rooted in limited sensory input, data inaccessibility from occluded regions, and resource constraints like available numbers and intelligence of robots or robotic agents. Our proposed approach addresses these challenges by establishing collaborative data-sharing networks among multiple robotic agents, enabling enhanced situational awareness and interaction. To this end, with its enhanced data-sharing networks, our system first constructs a high-fidelity virtual environment using multimodal input data from various sensors, such as LiDAR and cameras. A unified virtual map is created by fusing individual sensory data, accurately representing the environment. Second, our approach is powered by generative Artificial Intelligence (GenAI) models. Third, it further enhances the fidelity of the virtual environment by generating the most reliable data for hidden or unobserved areas using its pattern recognition capabilities on available data for visible areas. Fourth, our approach optimizes resource utilization to find a robust balance between system performance and resource availability. Fifth, our digital twin system will leverage the privacy-preserving framework of distributed data networks to improve security measures and maintain data integrity to be extended to real-world industry problems. As a result, the advantages of the proposed approach are expected to support efficient decision-making and navigation for multiple robotic agents in a complex and dynamically changing environment.",1,110 | Cobb Galleria Centre,Modeling & Simulation - Digital Twins,159
533,6128.0,Digital Twin for Automotive Decision-making During Unplanned Downtimes,Practitioner,"Automotive manufacturers have difficulty making key decisions when abnormal events with uncertain recover y times disrupt production. Key decisions may include whether to shut down a shop early in the event of a significant gap in production, take advantage of downtime to send personnel to training, add overtime to recover from a problem, or delay the start of the next shift. Some of the difficult ies result from a lack of visibility into the system due to limited sensor coverage, incomplete data and a lack of tools to analyze the problem quickly and sufficiently. To address these issues and help make better decisions faster, a two-part digital twin was developed. The first part of the digital twin utilizes a Unity model to generate a virtual real-time representation of the facility based on the limited sensor data and other production information. This model helps to simulate the position of vehicles between the sensors, where visibility is low, identifies vehicles that are or are likely to become late, and displays vehicles currently in offline areas. This model also gives the decision-makers access to vehicle specifications, tracking data, and key system performance measures. The second part is a gap analysis tool that utilizes a FlexSim model to predict the impact of an abnormal event based on the system’s current state . This tool allows decision-makers to analyze multiple scenarios for the projected recovery times of the event.",2,110 | Cobb Galleria Centre,Modeling & Simulation - Digital Twins,159
534,5520.0,Digital Twin for Enhanced Decision-Making by Optimization of Fiber Spinning Process,,"Traditionally, the fiber spinning process has been operated in the manner of a trial-and-error. Since the final properties of a product can’t be verified until all processes are complete, it often requires inefficient, and repetitive efforts, and causes significant costs to achieve the target characteristics of a product. Furthermore, most decision-making is based on tacit knowledge and individual experiences, therefore quality instability and high defect rate may be possible. Also, the complexity of the fiber spinning process, which consists of multiple dispersed stages, requires comprehensive monitoring and management. To address these challenges, this paper proposes to apply digital twin to the fiber spinning process, by integration of process equipment. The digital twin suggested in this paper provides real-time processing of shop floor data for better decision-making. It also derives optimal parameters of processes for achieving the desired characteristics of products by reinforcement learning. In addition, a machine learning-based predictive model supports pro-active simulations for predicting product quality before operations. As a conclusion, the digital twin supports operators more efficient work environment, reduces decisions based on personal experiences, and shorten time, cost and efforts for many trial-and-errors.",3,110 | Cobb Galleria Centre,Modeling & Simulation - Digital Twins,159
535,5822.0,A Decision Support Framework for Building Energy Efficiency and Productivity Based on a Sensor Network,Academician,"The buildings sector accounts for 40% of global energy consumption and over 30% of carbon emissions, with HVAC (heating, ventilation, and air-conditioning) systems responsible for more than 51% of building energy use. To address the challenge of reducing HVAC energy consumption while maintaining thermal comfort and indoor air quality (IAQ), this study proposes an enhanced HVAC system model for air-water systems, integrating advanced optimization techniques. The model incorporates real-time data from occupancy and IAQ sensors, including occupancy levels, temperature, CO2, and indoor air pollutant concentrations. Using this data, the system dynamically adjusts control variables such as airflow and fan speeds to optimize energy efficiency and maintain IAQ. To manage uncertainties in occupancy and environmental conditions, a robust optimization framework is developed based on a deterministic HVAC model. Adjustable robust optimization enhances system adaptability, ensuring consistent performance under varying real-world conditions. The proposed framework is validated through EnergyPlus, a high-fidelity building energy simulation platform simulating real-world HVAC operations. This research demonstrates a scalable solution for energy-efficient HVAC management. It achieves significant energy savings while maintaining essential comfort and air quality standards, contributing to the broader goal of sustainable building operations.",1,110 | Cobb Galleria Centre,Modeling & Simulation - Energy,160
536,5984.0,Predicting Manufacturing Power with Equipment Conditions: A Simulation Modeling Approach,Academician,"Power prediction in manufacturing processes is essential for reducing energy costs and improving production efficiency. Various process parameters can influence accurate power prediction in manufacturing. Most previous studies, however, have limited their focus only on a few process parameters such as processing time, feed rate, cutting depth, and spindle speed. Addressing this research gap, this study aims to build a power prediction model that incorporates not only process parameters but also equipment condition factors, such as component wear and equipment lifespan. Specifically, a literature review is conducted to identify the relationship between equipment condition and power consumption, and Copula models are used to model the dependency between these variables. In addition, using this dependency between various variables, we simulate activities and times series power of manufacturing processes. Finally, a Long-short Term Memory (LSTM) model is developed based on the generated simulation data to predict power consumption, allowing for a comparative analysis of input parameters on prediction accuracy. This study serves as foundational research expected to contribute to energy management in manufacturing by enhancing prediction accuracy using diverse parameters.",2,110 | Cobb Galleria Centre,Modeling & Simulation - Energy,160
537,5679.0,Discrete Event Simulation for Nuclear Waste Disposition,Practitioner,"Nuclear waste disposal is a growing issue, with significant backlogs of nuclear waste at many national laboratories and research centers dealing in medical research, clean energy, and other nuclear activities. This backlog prevents future use of nuclear materials, risks environmental impacts, and complicates federal and state regulatory compliance, making disposal costly. Rising interest in nuclear energy will further inflate the issues of backlogged nuclear waste. Therefore, effectively managing and disposing of nuclear waste is critical to invest in future nuclear solutions. Simulation approaches can provide insight into effective resource allocation and waste processing operations to stabilize productivity and cut the backlog of nuclear waste. Therefore, we created a discrete event simulation in ExtendSim to model a nuclear waste disposition process, which is a process within nuclear waste disposal. We inform the simulation by evaluating a case study of the Savannah River National Laboratory (SRNL), which is a United States nuclear waste disposal facility, using data and insights from SNRL staff. The analysis showed that workers' utilization of nuclear waste processing was 10.14%. This low utilization suggests potential improvements through adjusting staffing, schedules, and non-nuclear waste processing duties. In the future, time studies of each process may identify worker time spent on individual tasks to help increase time spent on nuclear waste processing. Our findings offer valuable insights for decision-makers and other national laboratories or research entities engaged in nuclear operations by supporting more efficient nuclear waste management.",3,110 | Cobb Galleria Centre,Modeling & Simulation - Energy,160
538,7050.0,Simulation-Based Dynamic Pricing Optimization for Integrating Electricity and Hydrogen in Local Energy Markets,Academician,"The growing need to reduce carbon emissions and promote sustainable energy use is driving the transition toward environmentally friendly energy systems. The integration of distributed renewable energy sources and advancements in smart grid technologies are paving the way for innovative solutions to enhance energy efficiency and reduce dependence on fossil fuels. This study introduces an agent-based simulation model to optimize a charging station system that seamlessly integrates electricity and hydrogen infrastructure. The charging station is equipped with solar and wind generation, battery storage, and an advanced electricity-hydrogen transfer system, allowing it to generate electricity, store hydrogen, and support both electric vehicle (EV) and hydrogen fuel-cell vehicle (HFCV) charging, as well as supply electricity to building prosumers. To optimize the operations of this integrated system, we develop a leader-follower optimization model, which is then evaluated through simulations. The upper level maximizes the charging station’s profit, while the lower level addresses users’ decisions on energy consumption and charging behaviors in response to dynamic pricing. By simulating different scenarios with varying building loads, EV behaviors, and uncertainties, we analyze their impact on energy pricing and transaction dynamics. The results demonstrate that the integrated electricity-hydrogen system, when optimized through simulation, can achieve significant cost savings and enhance energy-sharing efficiencies, contributing to resilient and sustainable local energy networks.",4,110 | Cobb Galleria Centre,Modeling & Simulation - Energy,160
539,6686.0,Disruption Management Policy Assessment Through Discrete-Event Agent-Based Simulation,Practitioner,"Aiming towards the goals of Industry 4.0, companies are increasingly using simulation as a tool to improve performance of manufacturing and logistics systems. The ultimate goal of this line of research is to implement a Digital Twin of a full system, being able to automate a rapid response to disruptions when they occur making the best decisions. Moving towards this goal, this research leverages a high-fidelity large-scale discrete-event agent-based (DEABS) simulator platform of an assembly plant to assess the performance of different disruption management policies offline. This study analyzes the overall performance of individual response strategies, and studies static state-based policies, evaluating the conditions under which one policy dominates another depending on the state on the system when a disruption occurs. The automation of decision making through a Digital Twin will be economically viable when the best decisions are state dependent, meaning the selection of the best policy to implement will depend on the state of the system. Once the existence of state-based policies is validated and all policies are embedded in the model, a live connection to the physical system data enables the automation of disruption management policy selection under uncertainty.",1,110 | Cobb Galleria Centre,Modeling & Simulation - Engineering Management,161
540,9184.0,Systems Model of Integrated Organizational Resilience,Academician,"Modern organizations function in complex and dynamic environments, where the ability to be resilient in the face of disruptions is essential for long-term health and sustainability. Organizational Resilience (OR) refers to an organization's ability to prepare for, adapt to, and learn from disruptive events. While OR has been studied across various contexts, few studies explore models that incorporate insights from related aspects of resilience within organizations such as employee, leadership, and team resilience. There is also a lack of commonly-accepted frameworks for evaluating resilience capabilities, and more research is needed to understand how these capabilities evolve over time. This study extends previous research that applied system dynamics modeling to develop an integrated model of OR that describes the factors that affect the development of OR capabilities at both individual and aggregate levels. This paper presents a multi-level system dynamics model based on empirical evidence available in the literature. The model is initialized using evidence extracted from the literature and partially validated using published case studies. The resulting model provides an approach for quantitatively investigating strategies for building resilience capabilities in organizations.",2,110 | Cobb Galleria Centre,Modeling & Simulation - Engineering Management,161
541,9095.0,A Mixed-Integer Optimization Tool for System-Level Proactive Obsolescence Risk Management,Academician,"Diminishing manufacturing sources and material shortages (DMSMS) pose a significant risk to the reliability and maintainability of many modern complex systems due to part obsolescence. Part obsolescence increases the risk of system failure or downtime, which is often not only undesirable, but non-negotiable. As such, it is imperative to anticipate events of part obsolescence and take proper actions to mitigate their effects. However, developing a detailed system-wide plan of mitigating events of obsolescence could necessitate the consideration of thousands, or even millions, of decisions before an optimal plan is derived. Thus, there exists a need for an automated tool that can produce a cost optimal obsolescence management plan. In this work, we propose a mixed-integer optimization tool that proactively manages and mitigates events of obsolescence at the system level. By considering the architecture of the bill of materials, part inventories, expected system demand, and forecasted obsolescence dates, our tool makes decisions of when and how to enact up to 8 unique short-term and long-term resolutions for every part within the system at every time until the anticipated system end of life. The parameters determine the structure of a mixed-integer optimization problem, where the objective is to minimize costs for the system over the entire time horizon. We present the findings of a case study that compares the obsolescence management plan of a reactive strategy to a plan produced by our mixed-integer programming optimization tool and derive insights based on the difference between the solutions produced by the two methods.",3,110 | Cobb Galleria Centre,Modeling & Simulation - Engineering Management,161
542,6412.0,Simulation-enabled Evaluation of Operational Strategies at Shield Illinois Depot for Efficient Disease Testing,Practitioner,"To meet the state’s need for accessible, efficient COVID-19 testing during the pandemic, an initiative from the Illinois Department of Transportation was launched which is called SHIELD Illinois. Utilizing methodology pioneered by the University of Illinois Urbana-Champaign (UIUC), SHIELD Illinois launched a statewide, saliva-based COVID-19 testing network that reaches 1,700 schools, colleges and universities in Illinois. The depot (Darien, IL) was essential to this operation; it served as the central distribution hub daily routing thousands of samples to testing laboratories across the state. Depot operations were heavily impacted at the peak of their pandemic, owing to an extremely erratic arrival pattern of samples and a manual dispatching system which used spreadsheets for tracking and decision-making. It often leads to inefficiencies, and longer cycle times. The aim of this research is to make depot operations more efficient along the SHIELD Illinois network through resource allocation and dispatching route optimization using a simulation-based model. This study explores and simulates routing strategies, assessing their impact on travel times, resource requirements and cycle times over an entire system from collection to results. By subjecting the design to numerous simulations tests, this research aims to obtain optimal dispatch routes or resource allocation that will make operation more efficient and consistent thereby minimizing delays. The proposed simulation model will provide SHIELD Illinois with a systematic data-driven platform for managing demand changes and aligning resource allocation strategies. The findings of this research will provide valuable insights into future logistics for large-scale public health responses.",1,110 | Cobb Galleria Centre,Modeling & Simulation - Epidemiology,162
543,6775.0,Diagnostics for Detecting Prepatent Guinea Worm Infections among Dogs in the Last Mile of Disease Eradication: The Impacts of Disease Prevalence and Tethering Compliance,Academician,"Guinea worm disease (GWD) has been targeted for eradication and much progress has been made towards this goal, with a 99.99% reduction in human cases since 1986. However, animal infections, particularly among dogs, create challenges to reaching disease eradication due to continued risks for the human population. Diagnostics for detection of prepatent infections among dogs are under development with the goal of identifying infected dogs prior to worm emergence and tethering/containing those dogs to break the cycle and prevent infection of water sources. We use an agent-based simulation which models guinea worm infections among dogs and interactions between dogs and water sources to determine the impact of interventions to prevent GWD over time. Based on the potential characteristics of diagnostics for prepatent infections, we analyze a variety of testing accuracy levels (i.e., test sensitivity and specificity) and testing protocols. Due to the low prevalence of GWD among dogs (approximately 1% of the population), we find that repeat testing provides added confidence in the results of diagnostic testing under scenarios with high numbers of false positive results (e.g., when there is low diagnostic specificity). In settings with low adherence to proactive tethering recommendations, diagnostic testing can have a positive impact due to the increase in tethering due to dogs testing positive. Increasing adherence to proactive tethering reduces the impact of diagnostic testing.",2,110 | Cobb Galleria Centre,Modeling & Simulation - Epidemiology,162
544,6085.0,Mechanistic Modeling of Social Conditions in Disease-Prediction Simulations via Copulas  and Probabilistic Graphical Models: HIV Case Study,Academician,"Epidemic models typically simulate the spread of diseases as functions of behaviors, e.g., sexual and care behaviors for sexually transmitted diseases. However, multi-level factors, including poverty, housing or food insecurity, mental health, and substance use disorder etc., (collectively refer to these as social determinants of health (SDH)), are drivers of those behaviors. Therefore, there is increasing awareness of the need to model SDH into epidemic simulations to evaluate structural interventions alongside behavioral and care interventions. However, the multivariate joint associations between SDH and behaviors needed for the modeling are not available. Data for SDH are mostly available as county-level marginal distributions, and associations between SDH and behaviors are mostly bivariate. We combined copula probability theory and probabilistic graphical models to estimate the multivariate joint distributions. We estimate bivariate associations between SDH using a novel copula approach by transitioning from continuous to discrete copula. As data for associations between SDH and behaviors are mostly available through observational studies as relative risks or odds-ratios, we used the bivariate SDH associations derived using copula and between SDH and behaviors from literature studies, as links in undirected graphical models, to estimate the joint density. As a case study, we used the joint distributions to model HIV-risk related behaviors as a function of SDH in a national-level HIV/AIDS (PATH 4.0) model and studied the impact of hypothetical 100% efficacious SDH interventions on HIV prevention. We found that this intervention could lead to a cumulative 10-year reduction of 29% in HIV incidence.",3,110 | Cobb Galleria Centre,Modeling & Simulation - Epidemiology,162
545,6202.0,Optimizing Hospital Capacity During Pandemics: A Dual-Component Framework for Strategic Patient Relocation,Academician,"The COVID-19 pandemic has placed immense strain on hospital systems worldwide, leading to critical capacity challenges. This research proposes a two-part framework to optimize hospital capacity through patient relocation strategies. The first component involves developing a time series prediction model to forecast patient arrival rates. Using historical data on COVID-19 cases and hospitalizations, the model will generate accurate forecasts of future patient volumes. This will enable hospitals to proactively plan resource allocation and patient flow. The second component is a simulation model that evaluates the impact of different patient relocation strategies. The simulation will account for factors such as bed availability, staff capabilities, transportation logistics, and patient acuity to optimize the placement of patients across networked hospitals. Multiple scenarios will be tested, including inter-hospital transfers, use of temporary care facilities, and adaptations to discharge protocols. By combining predictive analytics and simulation modeling, this research aims to provide hospital administrators with a comprehensive decision-support tool. The proposed framework will empower them to anticipate demand, simulate relocation strategies, and implement optimal policies to distribute patients and resources. Ultimately, this work seeks to enhance the resilience of healthcare systems in the face of COVID-19 and future pandemics.",4,110 | Cobb Galleria Centre,Modeling & Simulation - Epidemiology,162
546,6908.0,Integrating Reverse Engineering and Life Expectancy Analysis in the Lifecycle Management of a Mechanical Component,Academician,"The life expectancy of critical mechanical components, such as brake rotor, is essential for ensuring both performance and safety. In this study, reverse engineering is employed to recreate the CAD model of a brake rotor part lacking digital model representation. Point cloud data is generated using a FaroArm laser scanner to capture the rotor’s geometric and material properties in detail, enabling the creation of an accurate CAD model. Finite element analysis (FEA) is conducted to simulate the rotor’s response to various operational conditions, including changes in load, speed, and temperature, allowing for the assessment of stress, strain, and potential failure points. To further evaluate the rotor’s durability, a series of simulations is performed to collect stress and strain data over time, focusing on factors such as fatigue and wear. Degradation models and fatigue analysis are applied to estimate the rotor’s remaining useful life across different operational scenarios, with critical conditions identified that may accelerate component failure. Failure mode analysis pinpoints specific thresholds under varying load cycles, enhancing understanding of the rotor’s operational limits. This approach emphasizes life expectancy forecasting over predictive maintenance scheduling, providing valuable insights for lifecycle management. The findings contribute a framework for determining optimal replacement schedules, ultimately supporting safer and more cost-effective brake rotor usage. This study advances reverse engineering and lifecycle management practices by presenting a predictive approach that aligns with industry needs for reliability and efficiency.",1,116 | Cobb Galleria Centre,Modeling & Simulation - Manufacturing,163
547,8746.0,Utilizing Simulation for Multiple Phases of Automation Selection and Deployment,Practitioner,"Many SKUs cannot be sent to retail stores in full case quantities, as they are slow movers and would require individual stores to carry excessive inventory. Breakpack is the process of breaking cases down to individual eaches and combining them into mixed SKU cartons. Automating breakpack offers significant labor and quality savings, but also a high degree of complexity. SKUs should be grouped to minimize labor during the store put-away process, while also attempting to minimize labor and transportation cost for the DC and overall supply chain. An AnyLogic simulation model was developed to assist the retailer in multiple phases of their breakpack automation journey. These phases, spanning 5 years of the design/deployment lifecycle, included: Concept selection and initial refinement A data-driven model was developed to allow the retailer’s engineers to make a comparison of different vendors’ proposed automation solutions. The model was also used to also learn what worked well, so the selected concept could be initially sized and improved before a live proof-of-concept implementation. Detailed algorithm refinement The simulation model was used to understand the tradeoffs between distribution center efficiency, transportation considerations, and store-friendliness. Next generation refinement After initial deployments, the automation vendor has several suggestions to further improve the system. The simulation model was utilized to better understand the potential benefits of these proposals.",2,116 | Cobb Galleria Centre,Modeling & Simulation - Manufacturing,163
548,9119.0,The Prospect of Geospatial Analysis in the Prediction of Surface Quality in Machining,Academician,"This research underscores the prospect of geospatial analysis in machining operations to enhance precise prediction and robustness, offering a comprehensive framework for advanced manufacturing processes. Geospatial analysis not only provides accurate predictions but also estimates the uncertainty associated with these predictions, offering valuable insights for process optimization. The surface quality in the machining processes is expressed by the estimation of the average surface roughness. While machining parameters are extensively analyzed for their influence on surface quality, the roughness profile parameters are inadequately explored. This work integrates these underexplored parameters into geospatial predictive models and evaluates their impact on the prediction of average surface roughness in turning operations. The spatial model is validated using a standard method, such as 5-Fold Cross-Validation, to ensure reliability and accuracy. The findings of the proposed method highlight the advantages of the spatial surrogate model in surface quality prediction, particularly in its ability to incorporate spatial dependencies based on roughness profiles and provide uncertainty quantification.",3,116 | Cobb Galleria Centre,Modeling & Simulation - Manufacturing,163
549,6272.0,Enabling Efficient Manufacturing Facility Design Assessment Feedback Loop:  A Simulation Modeling Framework,Practitioner,"In the era of manufacturing 4.0, the complexities in product and process design, resource organization, configuration, and layout make simulation model development of manufacturing facilities an important but potentially complex and time-consuming task. Consider the design of a production facility in which the detailed product design, detailed process design or selection of resources is to be determined. This research defines a fundamental concept of operation which constrains the organization of resources. Formal models of product definition, process specification and facility layout support the description of specific design decisions and enable the automation of design simulation and design evaluation. The framework enables fast adaptation of simulation models to changes in product and process design, resource organization, configuration, and layout, so that there would be minimal to no maintenance effort on the simulation model required to adapt to the design changes from these four areas. An application of the concepts to the design of industrial kitting with takt time driven push operational control system demonstrates the feasibility and effectiveness of the approach.",4,116 | Cobb Galleria Centre,Modeling & Simulation - Manufacturing,163
550,5687.0,Simulation-Based Evaluation of Battery Charging and Swapping Station Deployment Strategies in a Hyperconnected Logistic Hub,Academician,"The transition to net-zero emissions in freight transportation is essential for reducing the environmental impact of logistic operations. This paper addresses the design and evaluation of various deployment plans for battery swapping and charging stations at existing logistics facilities. These deployments aim to ensure that electric trucks can operate efficiently within the freight system. We explore several configurations for battery charging infrastructure, including fixed charging stations at docks and parking areas, as well as movable charging stations. In addition, we develop various decision-making policies, such as (1) when to swap or charge batteries, (2) truck-to-dock assignments, and (3) truck-to-charging station assignments. A simulation-based approach is used to assess the performance of these configurations and policies. The evaluation focuses on system efficiency, operational costs, environmental impact, and the ability to meet energy demands. The findings provide insights into optimizing charging infrastructure deployment to support the transition towards sustainable fleet operations. Future research could include simulation optimization to further refine deployment strategies.",1,110 | Cobb Galleria Centre,Modeling & Simulation - Transportation,164
551,6299.0,Modeling and Predicting Traffic Congestion During Wildfires with Hidden Markov Models,Academician,"With the increasing frequency of natural disasters like wildfires, the need for efficient disaster management policies has become critical. This paper employs a Hidden Markov Model (HMM) to analyze highway vehicle routing behavior during the 2019 Kincade Fire in Sonoma County, California. The proposed model infers traffic congestion levels from GPS coordinates, timestamps, and highway identifiers, using these metrics as observations. The hidden states represent three congestion levels: high, medium, and low, distinguished by average vehicle speed thresholds, where lower speeds indicate higher congestion. The model utilizes speed thresholds to differentiate these congestion states, where lower average speeds suggest higher congestion. By framing traffic congestion as hidden states within an HMM, the model uncovers underlying traffic patterns that are not directly observable. It predicts likely congestion levels based on travel speeds, even as conditions change across highway segments. Such predictions enhance real-time traffic management by identifying and anticipating congestion points, enabling dynamic adjustments to traffic control and route guidance systems. The scalable nature of the proposed approach supports broader highway network analysis without requiring continuous direct observation. The results demonstrate the potential of HMMs to improve traffic analytics, offering a robust framework for real-time transportation planning and optimization during wildfire evacuations.",2,110 | Cobb Galleria Centre,Modeling & Simulation - Transportation,164
552,9090.0,"A Flexible, Realistic, Simulation Framework for EV-based On-demand Transportation System",,"This research presents a Python-based simulation framework designed to model electric vehicle (EV) on-demand transportation systems, with a focus on optimizing urban fleet operations. Built on a process-driven architecture, the system efficiently simulates EV fleet dynamics, including passenger matching, vehicle dispatching, and charging strategies, while enabling customization to address critical challenges such as charger placement, fleet management, and algorithm performance. The study evaluates various dispatching algorithms, including closest vehicle dispatch, closest available vehicle dispatch, and power-of-d vehicles dispatch, and explores charging approaches like continuous and nighttime charging. Results reveal that adaptive power-of-d dispatch, which dynamically adjusts to real-time conditions, significantly improves operational efficiency by balancing pickup times, service levels, and state-of-charge stability. Nighttime charging further enhances fleet performance, reducing pickup delays and increasing efficiency during peak demand. The framework integrates real-world datasets, such as NYC and Chicago taxi data, to closely mirror real-world dynamics. With features such as asynchronous event handling, non-Markovian state management, and high-dimensional state space tracking, the system achieves remarkable computational efficiency and scalability. It is capable of handling peak demand scenarios involving thousands of trips and completing multi-day scenarios in minutes. The modular design enables users to experiment with parameters, test algorithms, and integrate custom datasets, making the tool highly adaptable for diverse urban contexts. By providing a realistic and extensible platform, this adaptable, scalable, and open-source framework advances the optimization of EV fleet operations and offers a valuable resource for decision-makers and city planners navigating the transition to sustainable urban mobility solutions.",3,110 | Cobb Galleria Centre,Modeling & Simulation - Transportation,164
553,5757.0,A Multi-Objective Study on OHT Scheduling and Waiting Strategies for Semiconductor Fabs through Evolutionary and Machine Learning Algorithms,,"Semiconductor manufacturing is one of the most intricate and complex processes involving hundreds of operations to produce a wafer. Modern semiconductor fabrication plants, or fabs, utilize a large fleet of overhead hoist transports (OHTs) to ensure the timely transportation of wafer lots to their destination. As the OHTs receive transportation requests for thousands of lots per day, it is necessary to develop sophisticated and efficient dispatching algorithms to minimize the quality degradation of lots due to the detrimental environmental impact while maximizing productivity. Therefore, in this study, we develop and implement a simulation-optimization framework to explore the performance of mixed and composite dispatching rules against traditional rules. Multi-objective genetic algorithms are developed to identify the near-optimal sequence of dispatching decisions for mixed-rule scenarios and the relative importance of rules in composite-rule scenarios for a balanced compromise between wafer quality (tardiness) and productivity (throughput). To improve the greedy task arrival (or completion)-initiated scheduling decisions, we explore different waiting strategies for accumulating task requests to identify more efficient task-OHT allocations. Experimental results reveal that optimally weighted composite rules outperform mixed rules, which in turn outperform traditional dispatching rules. Furthermore, a waiting strategy with buffer periods for task consolidation simultaneously improves performance metrics while reducing computational time compared to a greedy event-based scheduling approach. Finally, this study utilizes Machine Learning approaches to investigate the influence of various task attributes, environment variables, and the relative importance assigned to scheduling objectives on the choice of dispatching decision and composite-rule strategy yielded by the simulation-optimization framework.",1,110 | Cobb Galleria Centre,Modeling & Simulation - Optimization,165
554,6254.0,Multi-Armed Bandits for Safety-Aware Network Scheduling Problems under Uncertainty,Academician,"Network scheduling must balance performance with safety considerations, particularly in critical infrastructure systems. While traditional approaches either rely on detailed system models or simplified binary safety indicators, both have limitations in uncertain environments. We propose a novel model-free approach for decentralized network scheduling that incorporates continuous safety indicator measures for each schedule, enabling more nuanced safety assessment. We specifically utilize multi-armed bandits with each possible schedule representing an arm. Our method demonstrates superior performance in identifying and eliminating unsafe schedules while maintaining probabilistic guarantees in finite time. Theoretical analysis reveals important trade-offs between time taken to eliminate unsafe schedules, number of rounds needed and the mean safety levels. We validate our approach through numerical experiments and a real-world case study of a Water Distribution Network in Tampa, FL, where we optimize restoration scheduling under contamination risks. This work provides infrastructure managers with a practical framework for evaluating resilient strategies under simulated disruptions.",2,110 | Cobb Galleria Centre,Modeling & Simulation - Optimization,165
555,8890.0,Unifying Trust-region Algorithms with Adaptive Sampling for Nonconvex Simulation Optimization,Academician,"Continuous simulation optimization is challenging due to its derivative-free and often nonconvex noisy setting. Trust-region methods have proven remarkable robustness for this class of problems. Each iteration of a trust-region method involves constructing a local model via interpolation or regression within a neighborhood of the current best solution that helps verify sufficient reduction in the function estimate when determining the next iterate. When the local model approximates the function well, larger neighborhoods are advantageous for faster progress. Conversely, unsuccessful approximations can be corrected by contracting the neighborhood. Traditional trust-region methods can be slowed down by incremental contractions that lead to numerous unnecessary iterations and significant simulation cost towards convergence to a stationary point. We propose a unified regime for adaptive sampling trust-region optimization (ASTRO) that can enjoy faster convergence in both iteration count and sampling effort by employing quadratic regularization and dynamically adjusting the trust-region size based on gradient estimates. This unification with other regularization frameworks enables almost sure $\mathcal{O}(\epsilon^{-1.5})$ iteration complexity and $\tilde{\mathcal{O}}(\epsilon^{-4.5})$ sample complexity compared with the state-of-the-art $\mathcal{O}(\epsilon^{-2})$ and $\tilde{\mathcal{O}}(\epsilon^{-6})$, respectively.",3,110 | Cobb Galleria Centre,Modeling & Simulation - Optimization,165
556,8763.0,Enhancing Urban Resilience with Deep Reinforcement Learning: A Proactive Model for Infrastructure Hardening under Uncertain Climate Risks,Academician,"Climate change presents increasingly volatile challenges, particularly in the form of more frequent and severe hurricanes, posing unprecedented threats to urban infrastructure and people’s safety. Given these challenges, decision-makers require an advanced model that can predict long-term environmental risks and how these risks change the performance profiles of individual facilities and their surrounding systems. Essentially, decision-makers require tools to optimize infrastructure maintenance and replacement schedules and the associated costs under uncertain climate changes and extreme weather events. To address these challenges, this study proposes to develop a deep reinforcement learning technique to dynamically evaluate infrastructure health and make investment decisions to harden infrastructure(s) against possible hurricane-induced damages over a long-term planning horizon. Our proposed technique accounts for several critical factors while dynamically assessing the condition of the infrastructure, such as building material and type, structural integrity, location-specific hazards, interdependent infrastructures, and available financial resources. The model is tested and validated using real-life building datasets from the coastal counties in Mississippi to provide data-driven insights for decision-makers. Specifically, the model provides infrastructure-specific and optimized cost-effective decision support for repair, replacement, and maintenance of infrastructure.",1,110 | Cobb Galleria Centre,Modeling & Simulation - Scheduling,166
557,6290.0,On route peer-to-peer dynamic ridesharing system: an agent-based modeling approach,Academician,"Over the past decades, the urban mobility sector has explored ridesharing systems (RSs) as sustainable solutions to reduce private car ownership and improve mobility for transit-dependent users. However, simultaneously optimizing both matching of drivers and passengers and routing given the dynamic and real-time complexity of urban environments is still a challenge to overcome. This research presents a novel framework that combines Agent-based Modeling (ABM) with Reinforcement Learning (RL) to address this joint optimization problem. The ABM component simulates interactions among individual agents—drivers and passengers—capturing the spatial and temporal complexities inherent in urban mobility patterns. Coupled with an RL algorithm, the framework learns adaptive policies that optimize driver-passenger assignments and routing decisions in a realistic, simulated environment. By jointly addressing matching and routing, the model can reduce idle time, improve driver-passenger proximity, and dynamically adjust routes based on traffic conditions and demand fluctuations. Furthermore, the model provides insights to inform government policies and business models to support sustainable urban transit solutions.",2,110 | Cobb Galleria Centre,Modeling & Simulation - Scheduling,166
558,5522.0,Exploring Optimal Supply Chain Strategies Using Computer Simulation with Microlevel Delivery Tasks Consideration.,Academician,"As globalization reshapes our industries worldwide, supply chain management has evolved into a highly interconnected world that oversees traditional management, making countries more interdependent. Driven by rapid technological advancements, this shift allows companies to share information in a matter of seconds across continents, aligning operations and building adaptability to quick changes. Therefore, industries must adjust to the new technology and use it to their advantage. However, even when this impact can be observed globally, the general impact can be analyzed and eradicated by performing micro-level analysis within the supply chain using technologies to their advantage. Hence, this study aims to investigate the micro-level analysis, simulating how different delivery strategies could be adopted for houses, departments, and apartments within the El Paso, TX, location. Key components such as delivery delays and overall processes were thoroughly integrated into the simulation framework. To improve the accuracy of delay times, the MOST (Maynard Operation Sequence Technique) analysis technique and a Monte Carlo simulation analysis were employed to ensure more accurate data in the simulated scenarios. In addition, the results obtained from this analysis were translated into another simulation using GIS mapping in AnyLogic simulation software. This study demonstrates the importance of performing detailed analysis, showing scenarios where companies could have hired more employees and had a low utilization rate and low incomes. The result also demonstrates the scenarios where companies may have fewer employees failing to perform the required deliveries, resulting in unsatisfactory employees due to high workload and customers.",3,110 | Cobb Galleria Centre,Modeling & Simulation - Scheduling,166
559,6192.0,Productivity-Driven Physician Scheduling in Emergency Telemedicine,Academician,"Telemedicine extends healthcare services across broad geographic areas, enabling patients to consult with remote physicians. In emergency teleneurology, stroke patients brought to healthcare facilities connect with off-site physicians employed by a teleneurology service provider via a navigation system that assigns time-sensitive cases to on-duty, credentialed physicians within a strict time window to avoid penalties, referred to as “blast.” The navigator’s decision-making is complicated by the physicians’ credentialing portfolios and by physician heterogeneity in productivity, measured by patient consultations per hour (PPH). Physician productivity varies due to multiple factors, including seniority, age, and stress levels. Remote physician productivity is further influenced by on-site factors, such as the availability of scribes and registration protocols at the hospitals handling emergency cases. We propose a mixed-integer linear programming model to address the Physician Roster Problem (PRP), incorporating physicians’ credentialing portfolios, productivity indices based on individual performance, and performance factors of the healthcare facilities from which emergency cases originate. Additionally, our model accounts for the dynamic, non-stationary nature of productivity, which varies by shift type (day or night). The objective is to minimize the system’s overall cost by optimizing productivity-aligned staffing to meet fluctuating patient demand over the planning horizon. To test our model and solution approaches, we conduct a case study based on a real-life application by a telemedicine service provider operating in the United States and serving a variety of healthcare facilities across multiple states.",4,110 | Cobb Galleria Centre,Modeling & Simulation - Scheduling,166
560,6750.0,Life Cycle Assessment of Networked Geothermal Systems: A Path to Sustainable Urban Energy,Academician,"This study presents results of life cycle assessment (LCA) of networked geothermal systems (Net-Geos) in urban environments, focusing on environmental impacts. Net-Geos connects multiple building types—including single-family, multi-family, and commercial—via a shared underground loop of ambient-temperature water, delivering heating and cooling through ground-source heat pumps (GSHPs). By enabling heat exchange between buildings, Net-Geos enhance energy efficiency, reduce fossil fuel reliance, and lower greenhouse gas (GHG) emissions. Based on the Massachusetts pilot project, this study initially focuses on five single-family homes in Lowell, MA, as part of a broader Net-Geo system serving 13 buildings with 44 boreholes. To build the life cycle inventory, data were collected from design documents provided by National Grid, supplemented by LCA databases and literature for materials, manufacturing, and installation processes. A static LCA approach was applied to compare Net-Geos with conventional heating, ventilation, and air conditioning (HVAC) systems powered by natural gas and electricity over a 25-year lifecycle. Preliminary results indicate that Net-Geos achieve approximately 26% lower GHG emissions, largely due to a 72% reduction in operational emissions from GSHPs compared to traditional systems. These findings underscore Net-Geos’ environmental benefits and potential for sustainable urban energy solutions. Future work will expand this model to evaluate multi-family and commercial buildings within the Net-Geo design and incorporate life cycle cost analysis. By supporting SDGs 7, 9, 11, and 13, this research provides a framework to assess Net-Geos’ sustainability across diverse urban settings, advancing insights for broader adoption and urban resilience.",1,116 | Cobb Galleria Centre,Approaches to Sustainable Energy and Resource Management (SDG 7),167
561,6449.0,"Leaching of critical metals from spent lithium-ion batteries using organic acids: Process optimization, techno-economic analysis and life cycle assessment",Academician,"The demand for lithium-ion batteries (LIBs) has increased significantly due to the global adoption of electric vehicles (EV) to achieve climate goals. LIBs are preferred due to their extended longevity, lower self-discharge, and superior output voltage relative to other batteries. The growth in the production of LIBs will lead to a higher number of spent LIBs to be managed in future. Spent LIBs are a significant secondary source of critical metals such as lithium (Li), cobalt (Co), nickel (Ni), and manganese (Mn), which can be recovered through recycling to alleviate the demand for virgin raw materials. We tested five different organic acids and optimized the leaching process using the best performing organic acid, through design of experiments, life cycle assessment, and techno-economic analyses to improve the sustainability and economic value of the process. We employed analysis of variance (ANOVA) and response surface optimizer in the central composite design (CCD) to optimize the leaching conditions. At optimal condition, leaching efficiency was 100% for all target metals (Co, Ni, Li, and Mn). A leaching plant with an annual throughput of 10,000 metric tons of black mass, plant life of 30 years, and a discount rate of 8% was found to be economical with a net present value (NPV) of $428 million. Organic acid concentration and FeSO4 molar ratio were found to be the most significant factors to achieve the highest NPV. In addition, the environmental impacts of the organic acid-based process were lower compared to state-of-the-art leaching processes reported in the literature.",2,116 | Cobb Galleria Centre,Approaches to Sustainable Energy and Resource Management (SDG 7),167
562,7093.0,Reflections in Education for Responsible Use of Nuclear Energy and Alternative Energies,Academician,"Energy demand in the world is in constant rise the last 20 years, there has been a 73% net increase in electricity consumption, worldwide energy consumption has increased by 40%. In order to produce sufficient amounts of electricity, a big amount of fossil fuels is used, accounting for 80% of the growth. However, that implies the release of air pollutants that cause environmental and health problems. This has resulted in several environmental issues, the most significant of which being the discharge of 36.6 gigatons of CO2 in 2022. Passive chronic exposure to 600 ppm of CO2 can cause a decrease in cognitive functioning and illnesses related to kidneys and bones. Our Educational reflection investigates the negative effects of petrol on the environment and looks at potential green energy options like solar, wind, hydropower and nuclear energy alternatives which can lower the world’s carbon levels by 70% within reach. Nuclear Power, which is oftentimes rejected because of unjustified fear, is an Alternative to fossil fuels. We review technological developments and the transition barriers to fossil-free energy, considering the efforts needed to prevent a energy supply crisis. Diverse benefits are presented. Among them lie economic viability due to a much smaller size and 90% efficiency, environmental benefits like 10g of CO2 emissions per MWh and social challenges that need to be tackled, like exaggerated fear of past nuclear incidents. Nuclear Power is not a permanent solution, but considering the present situation and technologies, it can serve as a reliable temporary measure.",3,116 | Cobb Galleria Centre,Approaches to Sustainable Energy and Resource Management (SDG 7),167
563,6118.0,Integrating circular economy with digital technologies  – a continuous improvement framework approach,Academician,"The urgency for sustainable global development has intensified in recent years, with increased focus on the need for accelerated actions towards identifying and implementing sustainable solutions. Circular economy (CE) strategies are critical in achieving these goals, and digital technology development is a key enabler in overcoming recognized challenges for CE implementation, particularly in areas of data management and process digitalization. This research identifies opportunities for advancing intelligent circular economy solutions. Based on the Plan-Do-Check-Act methodology, a continuous improvement framework is proposed. This approach allows for assessing the current state from a systems lifecycle perspective, defining the desired circular economy strategy, evaluating digital maturity levels, identifying improvement opportunities, implementing necessary actions, and learning from each cycle to initiate further iterations. The framework is tested and validated through a number of industrial case studies. The findings contribute to the knowledge base for researchers and practitioners working on circular economy and sustainable supply chain solutions.",1,119 | Cobb Galleria Centre,Innovative Strategies for Responsible Production (SDG 12),168
564,9079.0,A System Dynamics Model-Based Approach for Minimizing Pollution and Water Consumption While Achieving Sustainability in Craft Beer Supply Chains,Practitioner,"Craft breweries face significant challenges in balancing economic performance, operational efficiency, and environmental sustainability. This study introduces a Lean-integrated System Dynamics (SD) approach to minimize pollution, optimize water consumption, and recover wasted beer while maintaining sustainable performance metrics. By combining Lean principles with SD modeling, the research provides a comprehensive framework for managing the interconnected dynamics of supply chain operations, resource efficiency, and sustainability goals. The model focuses on three critical objectives: (1) Pollution Reduction: Examining strategies for waste recovery, emissions minimization, and byproduct reuse to improve environmental outcomes; (2) Water Consumption Optimization: Exploring closed-loop water systems and recycling technologies to reduce the water intensity of brewing processes; and, (3) Sustainable Performance: Balancing economic profitability and operational efficiency with reasonable environmental sustainability through adaptive supply chain strategies. The SD model captures feedback loops and trade-offs among these objectives, providing insights into how Lean principles can drive resource efficiency and environmental stewardship. The findings demonstrate a replicable methodology for achieving sustainable supply chain practices in small- to medium-sized breweries and offer actionable recommendations for integrating Lean and SD approaches to enhance both economic and environmental outcomes.",2,119 | Cobb Galleria Centre,Innovative Strategies for Responsible Production (SDG 12),168
565,6294.0,"Data Uncertainty Impact on Environmental Life Cycle Assessment (LCA) Results, a Pulp and Paper Case Study",Practitioner,"LCA has become a valuable tool in holistically evaluating environmental footprint from cradle-to-grave. It both quantifies resources consumed and assesses the impact of releases such as greenhouse gas emissions. However, to complete such a comprehensive study an analyst must employ a wide variety of data sources which can vary significantly in aggregation and accuracy, leading to uncertainty in study results. Though the challenge of evaluating data uncertainty in LCA studies has long been recognized, few studies have actually attempted to quantify it and to incorporate it in interpreting study results. This work focuses on environmental data uncertainty in the pulp and paper industry. It sources information from a well-established pulp and paper database of anonymized plant-specific data. Data is derived for specific, energy intensive points in the process, evaluated for variability, and developed into probability distributions. Three LCAs of a complete papermaking process are then developed using (1) a publicly available, highly aggregated dataset such as ecoinvent, (2) the pulp and paper industry specific database without consideration of these probability distributions, and (3) the pulp and paper specific database with consideration of these probability distributions. Results are compared and evaluated for statistical significance. The work further compares these results to a publicly available LCA study in which uncertainty was not considered to understand how LCA results can be misinterpreted and comparative assertions erroneously made. This work addresses goal 9 for sustainable industrialization, and also goal 12, in supporting the achievement of sustainable production and consumption patterns.",3,119 | Cobb Galleria Centre,Innovative Strategies for Responsible Production (SDG 12),168
566,6288.0,IoT system design framework for sustainable agriculture- A case study,Academician,"The adoption of IoT based precision agriculture technologies such as yield maps & variable rate fertilizer (VRT) has seen an increasing trend with 20% increase in adoption over the last five years among Midwest row crop producers. However, there are cost, operational, technical and data management barriers which discourage the adoption of IoT-based precision agriculture technologies. In this study,these barriers are analyzed and optimized using an IoT system design framework proposed.Interpretive structural modeling and participatory action research approaches are used to develop and test an IoT system design framework. Power consumption cost, data scalability, data latency, data interoperability and communication range of IoT devices are the decision variables that are optimized for a precision agriculture based IoT system. The IoT system design improvements resulted in 80% reduction of power consumption and 94% reduction in operational costs with an open-source IoT based real-time system. The IoT system design framework proposed can be used for digital twins, IoT design for precision agriculture, smart manufacturing and building smart cities to foster a resilient and sustainable economy.",4,119 | Cobb Galleria Centre,Innovative Strategies for Responsible Production (SDG 12),168
567,5830.0,An Integrated Data-Driven Approach to Enhance Equitable Spatial Access to Pharmacy Services,Academician,"Providing equitable access to quality essential healthcare services is crucial for enhancing overall well-being and health outcomes, which aligns closely with the United Nations Sustainable Development Goals (SDGs) as a key priority. However, in the United States, approximately 15.8 million people live in pharmacy deserts. The lack of pharmacy services forces many individuals in these areas to travel excessively long distances for essential medications and preventive services, highlighting the pressing need for better pharmacy accessibility in underserved areas. Thus, we aim to develop a novel data-driven approach that integrates large-scale human mobility patterns with location modeling, aimed at improving equitable spatial access to pharmacy services. Specifically, we propose a choice-based mobility model to predict people’s access patterns, which is further embedded in a facility location model. The key contribution of this work is the integration of predictive analytics and prescriptive analytics to enhance pharmacy accessibility. Finally, to demonstrate the applicability of the proposed framework, we provide a case study in Los Angeles (LA) County, where we extract human mobility patterns from large-scale mobile phone data. We show that disparities exist among various population groups in accessing pharmacy services. Overall, this project aims to provide deeper and more accurate insights into how people access pharmacies and to support sustainable development on promoting equitable spatial access to pharmacy services.",1,119 | Cobb Galleria Centre,Data-Driven Innovations for a Sustainable and Equitable Future (SDG 9),169
568,8779.0,Data-Driven Solutions for MSW Material Composition Prediction: Bridging Industrial Engineering and Sustainability,Academician,"According to the World Bank's ""What a Waste 2.0"" report, municipal solid waste (MSW) is projected to reach 3.4 billion metric tons by 2050, representing a 70% increase from current levels. Factors like population growth, rapid urbanization, and economic development are considered to be the main contributors to this projected rise. To promote and establish effective waste reduction, recycling, and reuse strategies, it is critical for governments and municipalities to identify the types and quantities of materials in waste streams. We develop a machine learning model to predict MSW material composition at the county level using socio-economic factors such as population density, urbanization rate, and employment sectors. For this purpose, we also collect and harmonize data from municipal waste audits across the U.S., creating the first publicly available dataset of its kind. We utilize SHAP (SHapley Additive exPlanations) value plots to explain the output of ML model and importance of each feature. We evaluate the model performance using the coefficient of determination. A random forest model with 24 selected features achieves a high level of accuracy over the test set. The results of this research could contribute to the data-driven planning of MSF infrastructure such as recycling facilities and waste-to-energy plants. Furthermore, data on waste composition could help guide educational campaigns to encourage waste segregation and promote sustainable behaviors.",2,119 | Cobb Galleria Centre,Data-Driven Innovations for a Sustainable and Equitable Future (SDG 9),169
569,9052.0,Big Data as Sustainable Educational Key to a Waste-Free Future,Academician,"How can we reconcile economic growth with environmental preservation? The answer largely lies in the education for application of innovative technologies such as Big Data. Big Data, understood as the vast volume of data that contains new forms of information with greater velocity, variety, volume, and veracity, offers immense potential to transform the way industries operate. By analyzing this massive data, companies can identify patterns, trends, and opportunities for improvement that were previously invisible. This ability to obtain valuable insights in real-time allows organizations to make more informed decisions and optimize their processes, from the supply chain to energy management. In this review, we will explore how Big Data is becoming a strategic ally in achieving Education in the Sustainable Development Goals. We will analyze the main applications of this technology in industry, the benefits it provides, and the challenges involved in its education and implementation. Additionally, we will present examples that illustrate how companies from various sectors are using Big Data to reduce their carbon footprint, improve energy efficiency, and promote more sustainable practices.",3,119 | Cobb Galleria Centre,Data-Driven Innovations for a Sustainable and Equitable Future (SDG 9),169
570,6147.0,Designing Conservation Incentives for Freshwater Sustainability,Practitioner,"The growing concern over water scarcity and the sustainable management of freshwater is common worldwide. Voluntary incentives, such as payments offered to water users, are recognized as a strategy for reducing conflicts and promoting sustainable water consumption. A primary challenge lies in determining the allocation of these incentives among water users, considering socio-environmental factors. This study leverages differential game theoretic models to design incentive schemes for the network of water users, geographically distributed across a river network. Two networks of water users are considered: those who utilize groundwater and those who rely on surface water for agricultural purposes. A non-governmental organization (NGO) participates as another key player, providing conservation incentives to encourage water users to reduce their consumption. The proposed model considers both vertical and horizontal interactions within the network of players. By considering the unique characteristics of each water user, the NGO aims to introduce an incentive scheme designed for each water user to promote sustainable water practices. Addressing the challenge of model scalability becomes crucial as the number of players or water users within each network increases, in order to identify optimal decisions. Thus, we propose solution methods for both convex and non-convex decision problems using Karush-Kuhn-Tucker (K.K.T.) conditions, Value Iteration (VI), and Basis Function Approximation (BFA) methods. Finally, we conduct parametric analyses to examine how various parameters influence the selection of solution methods and impact decision-making processes related to freshwater sustainability and long-term resource management.",1,118 | Cobb Galleria Centre,Innovative Solutions for Water Sustainability and Management (SDGs 6 & 14),170
571,6131.0,"Water management and industrial adaptation in Aguascalientes, Mexico: An in-depth survey in times of scarcity",,"In recent years, water scarcity has intensified in Aguascalientes, in center Mexico, posing serious challenges for industrial growth and sustainability. This arid region, reliant on overexploited aquifers, faces an escalating demand for water amid rapid industrialization, particularly in sectors like automotive, manufacturing, and agribusiness. These industries drive economic growth, yet simultaneously heighten pressure on critical water resources. This paper explores how Aguascalientes’ leading companies, representing the top GDP contributors, measure, mitigate, and adapt their supply chains to these conditions of water stress. Through a structured survey, we examined the water management practices of these key players to assess their strategies for resource conservation and risk mitigation. Initial findings indicate a progressive shift towards implementing sustainable water management solutions, yet varying approaches to risk measurement and adaptation reveal areas needing improvement. The data provides a comparative framework for understanding corporate responses to environmental challenges and highlights potential areas for policy and technological innovation. This research underscores the urgency of sustainable water management practices and lays the groundwork for future studies comparing water management strategies across regions with differing hydrological conditions.",2,118 | Cobb Galleria Centre,Innovative Solutions for Water Sustainability and Management (SDGs 6 & 14),170
572,8549.0,"Building sustainable infrastructure and scaling access to Water, Sanitation and Hygiene (WASH) in rural Alabama",Practitioner,"This paper examines the strategies used by the organization PEER Consultants P.C in sustainable engineering development. Focusing on scaling the organizational infrastructure of Black Belt Wastewater Unincorporated Program (BBUWP) to support installation of onsite wastewater systems. In these unincorporated regions of Alabama, the lack of adequate sanitation infrastructure has led to persistent health disparities affecting historically marginalized communities. PEER Consultants P.C. utilized sustainable engineering practices to scale BBUWP’s installation of onsite wastewater treatment systems, which directly benefit underprivileged residents. Through a case study approach, data were gathered from community engagement activities, participatory workshops, staff training and interviews with stakeholders involved in WASH-related outreach and training. Findings from this study highlight the importance of aligning technical solutions with socio-cultural, economic, and behavioral factors to ensure the success and sustainability of WASH interventions. The sustainable and scalable approach used in this project enabled the development of culturally sensitive, community-led educational programs, which fostered awareness, acceptance, and commitment to improving local WASH conditions. Additionally, engaging community members as active participants in the project promotes a sense of ownership and empowerment, which are critical to sustaining long-term improvements in sanitation infrastructure. This paper builds on previous literature on WASH solutions in rural settings by demonstrating how interdisciplinary collaborations between engineers, social scientists, health experts, and community stakeholders can effectively address complex WASH challenges. Ultimately, it underscores sustainable engineering as a promising pathway for expanding equitable access to sanitation services and enhancing public health outcomes in Alabama's rural, underserved communities.",3,118 | Cobb Galleria Centre,Innovative Solutions for Water Sustainability and Management (SDGs 6 & 14),170
573,8771.0,Microgreen Logistics: Combatting Degradation with Efficient Supply Chain Routing,Academician,"Microgreens are vegetables that are not grown to fully maturity. They have significantly higher nutritional value which is beneficial to human health. They also have high production rates which can help with food insecurity. For this study we specifically focus on using the calabrese broccoli microgreen because of their cancer fighting properties(sulforaphane). Due to the perishable nature of this food product, degradation is an issue when transporting microgreens to the end consumer which leads to loss of profit. This research explores the influence of the pre- and post-harvest stages on the degradation and food quality of microgreen supply chains. A discrete event simulation (DES) is developed to model the production, transportation, and sulforaphane degradation to understand the best time to harvest microgreens and the desired length of time that would allow for the maximum level of sulforaphane to be present in the microgreens at the retailer for purchase. This study emphasizes the importance of decision-support tools that can aid in creating better production systems for microgreens and more sustainable food systems. The findings indicate that more efficient transportation and production, result in better quality food products to the end consumer.",1,119 | Cobb Galleria Centre,Innovative Solutions for Sustainable Logistics (SDG 9),171
574,5093.0,Designing a green Supply chain network: A mathematical modeling  approach,Academician,"A green supply chain (SC) design model with the objectives of improving environmental, economic, and social sustainability is proposed. The model integrates lean systems and practices with SC operations that entail collaborative relationships with retailers and suppliers and facilitate empowerment and providing training to the SC employees. The model plans product marketing through their partner retailers. In the forward loop, the model plans new component procurement for new product from their partner suppliers. The SC worked with their Partner suppliers past several years to improve their process performance and reliability of their plants to obtain ensured quality inputs with reduced per input energy consumption. The model plans product distribution through a set of distribution centers optimally located between SC manufacturing and the retailers in the markets. The SC company included solar panels on their factory roof for covering a significant percentage of their total power requirements by green energy thereby reducing carbon footprint. The model includes retailers and suppliers in the SC’s information network to improve visibility with retailers and suppliers and thus to improve resilience for supply and marketing management. The SC manages customer returns collection at the end of product’s useful life by paying incentives to customer and sends them to third-party recovery services small firms through a services fee-based contract with their retailers. The model includes Training by the SC manufacturing to the recovery services firms and inspectors used by retailers to inspect customer returns to determine incentives. A numerical example illustrates applicability of the model",2,119 | Cobb Galleria Centre,Innovative Solutions for Sustainable Logistics (SDG 9),171
575,5950.0,Paving the way towards a new education in logistics: Green and automated,Academician,"Green Logistics has gained considerable attention in educational academic and industry literature in recent years as government regulations seek to comply with international sustainable goals and companies intend to gain competitive advantage by appealing to conscious consumers. Understanding how to implement green logistics practices is proving to be instrumental in profiting from its advantages such as fuel cost reduction, transport routes optimization, efficient use of resources, improving energy efficiency and increasing competitiveness and better market positioning. Moreover, the leaps and bounds of technology have led the way towards the fourth industrial revolution with Industry 4.0. Thus, this study aims to answer the following question: can the use of an educational approach to Industry 4.0 support the transition towards greener, more efficient logistics? It examines how green logistics education supports the implementation of a circular economy. While the first one refers to connecting resources to commodities and subsequently to consumers’ needs through products, the second one refers to a system where all material flows within an economy are completely closed, in such a way that both concepts are intrinsically linked to each other. Industry 4.0 technology is of paramount importance to optimize and virtualize the supply chain. The analysis findings reveals how industry 4.0 technologies and implementation of Internet of Things (IoT) will facilitate process’ traceability, production optimization and logistics. On top of that, the adoption of electric vehicles has proved to reduce fuel cost, inactivity time and emissions. This research offers insight into the drivers of adopting green logistics education.",3,119 | Cobb Galleria Centre,Innovative Solutions for Sustainable Logistics (SDG 9),171
576,8955.0,"Changing Administrations, Shifting Corporate Sustainability Policies: Opportunities for Industrial and Systems Engineers",Practitioner,"Dramatic swings in climate change and sustainability policies over the past 16 years have created significant uncertainty for businesses, communities, shareholders, and consumers. This paper explores whether such volatility could present business opportunities for Industrial and Systems Engineers (ISEs) in supporting and leading corporate sustainability efforts. U.S. policy has shifted from Obama’s ambitious environmental regulations and global leadership, to Trump’s skepticism-driven, deregulatory, and energy independence economic growth-focused approach, and then to Biden’s comprehensive climate legislation and investments. As stakeholders seek stability, the Trump campaign’s promise to again withdraw from the Paris Climate Agreement and reverse Democratic administration policies adds to uncertainty. Meanwhile, the E.U. has maintained a steady commitment to addressing climate change while fostering economic growth and social equity. Corporations operating in both the U.S. and E.U. tend to adhere to the most stringent regulations. Despite political swings, decreasing renewable energy costs and state-level initiatives support ongoing momentum, and cost savings and risk reduction from corporate sustainability initiatives have attracted investors and consumers. Moving forward, corporations may increase engagement with state-level initiatives, enhance sustainability reporting transparency, and advocate for favorable climate policies. Chief Sustainability Officers will need to navigate a complex regulatory landscape, balancing short-term and long-term goals, and driving innovation and resilience. This presents opportunities for ISEs to lead process optimization, data-driven decision-making, supply chain management, scenario planning, risk reduction, innovation, and integration of sustainability into business operations. Through literature reviews, surveys, and interviews, this paper examines how corporations may adjust to a second Trump Administration and the resulting opportunities for ISEs.",1,119 | Cobb Galleria Centre,"Global Sustainability Leadership: The Interplay of Policy, Education, and Technology",172
577,7077.0,A Sustainability Education Dilemma: Is a better environment worth a worse economy?,Academician,"Sustainability has become a global priority, with growing public and corporate interest. Therefore teaching Sustainability in an Education Environment is a priority. Here we explore the complex challenges which contribute to climate change, driven largely by industrial processes, population growth, and unsustainable manufacturing practices. It examines the environmental impacts of consumer demand, particularly in the transportation sector, and the pressure on companies to adopt eco-friendly methods, despite economic and logistical challenges. The paper presents a Challenge Based Learning (CBL) experience at Tecnologico de Monterrey Mexico City Campus from the Shell Eco-Marathon, focusing on the innovative use of bamboo as a substitute for aluminum in vehicle design by the student team ""Escudería EcoVolt."" Although bamboo offers significant environmental benefits, such as reduced carbon emissions and renewable properties, it presents technical difficulties in production, including structural integrity concerns and manufacturing constraints. The importance of teaching life cycle assessment (LCA) in evaluating sustainability efforts is highlighted, along with the difficulties of implementing environmentally friendly alternatives at scale. Ultimately, sustainability requires comprehensive solutions, but these often come with high costs and operational hurdles. While sustainable innovations are necessary to mitigate the effects of climate change, they must be balanced with economic considerations and consumer willingness to adopt these changes. The path toward a more sustainable future will be difficult, but it is essential to find ways to reduce environmental impacts without significantly compromising quality of life.",2,119 | Cobb Galleria Centre,"Global Sustainability Leadership: The Interplay of Policy, Education, and Technology",172
578,6961.0,Transformative Power of Inland Ports to Revolutionize Global Sustainability and Contribute to the UN SDGs,Practitioner,"Inland ports are increasingly recognized as pivotal nodes in global logistics, addressing seaport congestion and enabling efficient supply chain operations through intermodal transport, cargo tracking, and cross-docking. Beyond logistical benefits, inland ports contribute to sustainable development by enabling economic growth, reducing emissions, alleviating road congestion, and promoting social equity. They play a critical role in regional development by stimulating business activity, creating jobs, and linking remote regions to global trade networks. However, the extent to which these hubs align with long-term sustainability goals, including the United Nations’ Sustainable Development Goals (SDGs), remains underexplored. This paper evaluates the contributions of inland ports to logistics efficiency, regional economic development, and environmental sustainability. Through a systematic review of existing practices and case studies, this research highlights key gaps and proposes strategies to optimize inland port operations for sustainability to be aligned with the SDGs. The findings provide actionable insights for policymakers, investors, and industry stakeholders to ensure inland ports evolve as effective solutions for balancing economic efficiency with environmental and social responsibility in global logistics systems.",3,119 | Cobb Galleria Centre,"Global Sustainability Leadership: The Interplay of Policy, Education, and Technology",172
579,6868.0,Factors Limiting and Impacts of Technology Adoption on the Sustainable Development of Africa,Academician,"Currently, Africa is the fastest developing continent, modernizing at rates never seen before. For any region to develop, technology adoption is essential, making the region’s current industries more efficient, and allowing industries that otherwise would not have been viable, viable. Industrializing in the era of computers and climate change, Africa has both opportunities that were not available to previous industrializing regions and constraints that were not applied to other regions. Africa can make use of computers, AI and mobile networks, but, at the same time, under threat of climate change (which disproportionately affects less developed regions), needs to worry about pollution and use of green energy. A sustainable development of Africa, leveraging modern technology, is crucial. This paper will analyze several factors affecting technological adoption in Africa, including the role and impact of politics, economics and logistics/physical limitations on the adoption of technology. Factors within politics will include personal/corporate income tax policies, tariffs, intellectual property rights and global and regional cooperation. For economics, the cost and benefits of adopting new technologies, limitations to entrepreneurship and investment, both foreign and domestic, will be examined. For logistics and physical limitations, infrastructure, electricity consumption (and its sustainability) as well as distribution and supply chain limitations of these technologies will be examined. This paper relates most strongly to UN SDG goals 7, 8 and 9 (adopting green energy, promoting economic growth through the adoption of new technologies and promoting sustainable industrialization through adoption of technology).",4,119 | Cobb Galleria Centre,"Global Sustainability Leadership: The Interplay of Policy, Education, and Technology",172
580,5125.0,Crop Yield Prediction for Semi-Arid Regions,Academician,"Predicting crop yields is complex due to various factors like crop genotype, environmental conditions, and their correlations. This research uses a deep learning approach combining convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to forecast crop yields using weather and soil data. The CNN-RNN model was compared with traditional methods like random forest (RF), support vector machine (SVM), and LASSO using historical wheat yield data from Arizona and Kansas, and cotton yield data from Arizona and Georgia during the period of 1990-2023. Results show that the CNN-RNN model achieves significantly lower root mean square error (RMSE) than other methods, indicating its superiority in yield prediction for semi-arid and humid regions. In this paper, we also demonstrate consistency in yield prediction across diverse weather, specifically examining a semi-arid region in the southwest U.S. (Arizona) and other regions (Kansas and Georgia). This comparison provides insights of the influence of weather and soil on yield variability, highlighting the differential effects in semi-arid versus humid regions. By comparing these diverse environmental influences, the study reveals how regional climate and soil conditions shape yield predictions, providing valuable information for optimizing crop management practices in varying regions.",1,118 | Cobb Galleria Centre,Critical Issues in Agriculture and Food Sustainability (SDG 2),173
581,8825.0,"Food Wastage in Food Service Industry, Drivers & Consequences",Practitioner,"Research studies related to agriculture and food production showed that global food production uses approximately 25% of inhabitable land and 70% of the potable water supply and is also responsible for 30% of Global Greenhouse Gas (GHG) emissions and 80% of global deforestation. However, one-third of the total food produced for human consumption gets lost or wasted in the food supply chain (FSC). In brief, food loss and waste (FLW), is a widespread problem that occurs at all stages of the entire food supply chain from farm to fork. FLW in the food service industry (FSI) that represents the terminal stage of the FSC has emerged as a critical research area where food waste generation needs actual measurement and quantification for making plans and procedures to reduce it. Over 50% of total FLW occurs in the FSI that represents food retail and consumption part of FSC. Therefore, there is tremendous potential to reduce and prevent food waste at the FSI part of the FSC. In this paper, review of literature related to the food waste generation in the FSI part of FSC helped us to identify the nature, and quantity of food waste, and important drivers causing it, to find practical solutions to reduce food waste. The findings of the paper would be useful to the managers and professionals, workers and students in the food service industry, academicians, and various food regulatory agencies of governments",2,118 | Cobb Galleria Centre,Critical Issues in Agriculture and Food Sustainability (SDG 2),173
582,6647.0,Overview of Main Food Shelf-Life Extension Technologies and the Methodology for Comprehensive Evaluation,,"By prolonging food shelf-life, we can reduce the food loss and waste (FLW) resulting from expiry, as well as the greenhouse gas (GHG) emissions and energy consumption. Although shelf-life extension technologies may reduce FLW, they often consume additional energy and produce extra GHG emissions. In this research, a methodology is developed to comprehensively evaluating self-life extension technologies, especially for sustainable materials for packaging. Because of missing data, our methodology focuses on estimating the shelf-life change allowance to ensure a new technology is more sustainable over its lifetime.",3,118 | Cobb Galleria Centre,Critical Issues in Agriculture and Food Sustainability (SDG 2),173
583,6614.0,Assessing the Quantity of On-Field Crop Residue Burning -A Case study of Punjab,Practitioner,"The illegal practice of on-field Crop Residue (CR) burning in Indo-Gangetic Plains of Haryana, Uttar Pradesh, and Punjab (India) creates enormous environment pollution that leads to detrimental effects on the health of humans, animals and plants in the region. On-field Crop Residue burning helps clear the fields in a short time window between harvesting and sowing of the next crop in rice-wheat crop rotation sequence. Therefore, to clear their lands in a short time duration, a large number of farmers take a risk of an on-field crop residue burning activity, knowing the fact that regulatory agencies do not have adequate staff to monitor and register cases against each and every farmer committing this illegal activity. The objective of this paper is to assess the quantity of the surplus crop residue that gets burnt in this illegal activity in the state of Punjab (India). The quantification of the on-field crop residue burning is the basic input requirement in planning the programs and policies to encourage farmers to adopt surplus crop residue composting as an alternative to burning. To assess the quantity of surplus crop residue burnt the secondary data related to the cropped area, crop yield, the quantity of crop residue used as fodder, fuel, and in various other uses is collected from all the district headquarters of the state of Punjab (India). The findings of the paper would be useful to professionals, researchers, academicians, and policy makers involved in sustainable crop residue management and environmental pollution reduction practices.",4,118 | Cobb Galleria Centre,Critical Issues in Agriculture and Food Sustainability (SDG 2),173
584,8836.0,Are We Ready? Attitudes and Perceptions of Shared Autonomous Shuttles in College Communities,Practitioner,"College campuses often face significant transportation challenges, including traffic congestion, limited parking, and the need for efficient, sustainable transit solutions. Shared autonomous shuttles (SASs) offer a promising alternative to alleviate these issues, but widespread adoption depends on public trust and perceptions of their safety, efficiency, and reliability. This research, which was supported by the Fulbright Scholars Program and the US Department of Transportation, investigates factors influencing trust in and willingness to adopt SASs, focusing on how incremental exposure affects perceptions. Participants from two college campus communities (one in Australia and one in the United States) engaged in a study designed to measure pre-existing attitudes and changes after exposure to SASs through first-person perspective videos simulating the riding experience. Surveys administered before and after the intervention assessed trust, perception, and intent to use. Results highlight shifts in participant attitudes and provide insights into how exposure may influence acceptance of SASs. Additionally, analyses of demographic data are used to identify trends in attitudes and use intentions across different user groups. These findings can inform targeted marketing strategies and SAS design enhancements to address the needs and preferences of specific populations. This study contributes to the understanding of human factors critical to fostering trust and encouraging the adoption of autonomous transit technologies.",1,114 | Cobb Galleria Centre,Human Factors & Ergonomics - Technology Integration and Workplace Innovation,174
585,8848.0,Industry and Academic Partnership: Validating the Novel ENG,Practitioner,"Vigilance and the ability to react quickly in stressful scenarios are critical skills for professionals of many high-stakes industries. Existing research highlights how vigilance declines over time, particularly during periods necessitating sustained attention. Many variations of what is the Psychomotor Vigilance Task have been studied in the years since, with differing auditory or visual stimuli. This novel skin surface electrode at the wrist detect small electrical signals from the brain as they travel through muscles and nerves. The novel electroneurography (ENG) device enables a non-invasive means to udnertsand the brain’s motor cortex transmits signals throughout the central nervous system (Pison Technology Salus, Version, Massachusetts, USA). Using custom AI algorithms and a mental chronometer the ENG device measures reaction time to gauge mental acuity and executive function. Aim: Validate the novel ENG device (PIson Technologies) as mobile, wearable technology that can be used by end users in austere environments to monitor and alert changes in status related to PVT. It was hypothesized that the novel ENG device would provide general status changes over shorter time periods primarily related to PVT increases (increased response time) due to either low-stress levels and high-stress levels. Nurses and other clinical professionals are expected to perform cognitively and physically demanding tasks while under the stress of long working hours, high patient acuity, and high patient-to-provider ratios. In this study, we tested clinical vigilance with a PVT before and after a simulated patient monitoring intervention. The PVT deployed through a novel ENG device, which used visual stimuli.",2,114 | Cobb Galleria Centre,Human Factors & Ergonomics - Technology Integration and Workplace Innovation,174
587,6550.0,Life Cycle Assessment of Ethanol Production Using Various Energy Canes as Biomass Crops,Academician,"Energy cane is a wild relative of sugarcane, cultivated primarily to produce bioenergy, specifically ethanol from the extracted fibers of the plants. The cultivation of energy cane is characterized by its efficiency in requiring minimal input conditions. The yield of fibers and chemical compositions varies among genotypes, which affects the output of biofuels. A life cycle assessment (LCA) was performed using the OpenLCA software to evaluate the environmental impacts associated with ethanol production using three different energy cane genotypes and a standard sugar cane cultivar. The study was designed to identify the best genotype of energy canes associated with the lowest environmental footprint, prioritizing key ecological indicators such as global warming potential, acidification and ozone layer depletion. Two approaches were employed in this study. The first used an identical amount of biomass to compare the total ethanol production and environmental impacts of the three genotypes of energy canes and sugar cane. The second approach analyzed the environmental impacts and the amount of biomass needed when producing the same amount of ethanol. The findings from this investigation indicated that all three energy cane genotypes outperformed the control sample of the standard sugar cane, showcasing a noteworthy reduction in environmental impact, while maintaining comparable levels of biofuel output. The results also indicated that one of the three genotypes showed the lowest environmental impacts compared under both scenarios. The findings highlight the potential of energy canes as environmentally friendly options in sustainable bioenergy production.",1,118 | Cobb Galleria Centre,Agricultural Sustainable Energy Innovations (SDG 7 & 2),175
588,4870.0,Technical Feasibility Analysis of an Agrivoltaics System for Cotton Farms,Practitioner,"The climate change has increased the value of sustainable methods, such as agrivoltaics systems which serve as the integration of agricultural practices and solar energy systems. Agrivoltaics systems allow farmers to contribute to the United States' transition to renewable energy while providing security to their agricultural businesses. This project introduces a portable agrivoltaics system, which was specifically tailored for cotton crops, and evaluates the structural integrity of its structure paired with the structure’s shadow analysis. The portable structure could maximize the benefits of agrivoltaics and minimize the impact on agricultural activities, allowing farmers to access energy resilience while minimizing disruptions to the farmer’s responsibilities. Additionally, a shadow analysis of the structure is performed to minimize the negative impact on the selected crop. It is focused to make the design portable, retractable, and height adjusting, therefore, optimizing the compatibility with cotton crops. These features allow the design's height to be adjusted accordingly with the crop height and make it suitable for allowing machinery access to the fields. A structural analysis of the design is conducted to assess its stability and resistance to extreme wind conditions. The shadow analysis code implemented for the design is developed using NOAA solar calculator equations, which allows for critical parameters, such as azimuth and elevation angles, to be determined to predict the shadow length and direction. The results of the study aim to support the development of agrivoltaics systems while minimizing disruptions to agricultural activities.",2,118 | Cobb Galleria Centre,Agricultural Sustainable Energy Innovations (SDG 7 & 2),175
589,6877.0,Sustainable Solar Irrigation Technology for Rwandan Women Farmers,Practitioner,"We aim to present our interdisciplinary efforts in fighting climate change in East Africa using sustainable technology. Rwanda’s low agricultural productivity adversely affects food security. However, the nation boasts a female-majority Parliament and 82% labor-force participation by women in agriculture. Our investigation aims to center Rwandan women as leaders in policy and implementation of solar irrigation technologies to improve crop yields and, over time, child nutrition outcomes. This study combines scholarship across economics, industrial engineering, and other fields to promote a design for irrigation and cultivation technologies for women agriculturalists in rural Rwanda. Our study seeks to directly address 5 UN Sustainable Development Goals, including taking urgent action to combat climate change and its impact. The urgency of this issue cannot be overstated, and we look forward to developing a poster or a 20-minute presentation of our work to highlight this urgency.",3,118 | Cobb Galleria Centre,Agricultural Sustainable Energy Innovations (SDG 7 & 2),175
590,5765.0,Enhancing Transportation System Performance by Promoting Sustainable Travel through Public-Private Mobility Partnerships,Practitioner,"Public-private partnerships, involving transit agencies and emerging mobility service providers (such as micromobility and ridesharing), offer a promising strategy to improve transportation system performance and promote sustainable travel behavior. These partnerships have potential to address key challenges, including the first-mile/last-mile problem, reducing dependency on personal vehicles, and increasing transit ridership. This study develops an optimization-based framework that integrates private mobility services with public transit to encourage transit adoption among personal vehicle users through personalized incentive packages. The incentives include monetary and non-monetary rewards designed to minimize user disutility (including long wait times and inconvenient transfers) that often deters transit use. Cluster analysis is applied to group users based on their sociodemographic characteristics, and choice models are employed to estimate the likelihood of mode switching under various incentive scenarios. These methods ensure that incentive packages are aligned with user preferences and system-wide performance objectives. Numerical experiments on multiple transportation networks demonstrate that this framework enhances transportation system performance by increasing transit ridership, reducing overall system travel time, and supporting environmental sustainability through lower emissions and the promotion of sustainable travel options. This research offers actionable insights for policymakers and urban planners, highlighting the value of public-private partnerships in creating sustainable transportation systems that enhance urban mobility and social welfare. Moreover, leveraging app-based platforms from private mobility providers and transit agencies may enable low-cost implementation of the framework without requiring new infrastructure.",1,119 | Cobb Galleria Centre,Innovative Approaches to Urban Planning and Sustainable Mobility (SDG 11),176
591,6262.0,Evaluating CO₂ Emissions in Urban Traffic from Light-Duty Vehicles: A Comparative Study of Dhaka City and International Driving Cycles,Practitioner,"Transportation-related emissions are a significant environmental challenge in densely populated urban areas, and Dhaka city in Bangladesh is no exception. This study quantifies CO₂ emissions from light-duty vehicles using the Dhaka Driving Cycle (DDC), a cycle developed specifically for Dhaka’s major routes, and compares these results to established international driving cycles. To calculate the CO₂ emissions, we developed a Python program that simulates the driving cycles and their corresponding emissions. The simulated tailpipe CO₂ emissions for the DDC were found to be 499.6 grams per kilometer, substantially higher than those measured in other driving cycles. For comparison, CO₂ emissions per kilometer using the U.S. EPA Urban Dynamometer Driving Schedule (UDDS) were 247.4 grams (50.5% lower than the DDC), the New European Driving Cycle (NEDC) recorded 231.3 grams (53.7% lower than the DDC), and the Worldwide Harmonised Light Vehicles Test Procedure (WLTP) class 3 driving cycle showed 222.5 grams (55.5% lower than the DDC). These findings suggest that the unique traffic conditions in Dhaka—characterized by frequent stops, slow traffic flow, and congestion—contribute to substantially higher emissions compared to standardized urban driving conditions. This disparity highlights the need for locally relevant emission control policies and underscores the importance of developing location-specific driving cycles for more accurate emissions modeling. Insights from this study are crucial for policymakers aiming to address the environmental impact of transportation in Dhaka and other cities with similar traffic patterns.",2,119 | Cobb Galleria Centre,Innovative Approaches to Urban Planning and Sustainable Mobility (SDG 11),176
592,8807.0,Optimizing Local Parks for Equity with Decentralized Resident Decisions,Academician,"Local parks are foundational green infrastructure for cities. Existing research on improving park access focuses on availability of high-quality parks; it does not yet consider resident visitation and use of the parks. We present a model that optimizes location and size of new parks which considering resident use of the parks. It includes equity considerations, uncertainty, and endogenous factors such as distance and park crowding. We present the model, solution approach, and discuss practical implications.",3,119 | Cobb Galleria Centre,Innovative Approaches to Urban Planning and Sustainable Mobility (SDG 11),176
593,6768.0,Machine Learning-Based Building Value Loss Prediction for Flooding Hazards,Academician,"Floods pose a significant threat to communities, especially those with high social vulnerability. This research aims to enhance flood loss prediction by calculating a Social Vulnerability Index at a building level and integrating it with machine learning techniques to account for socio-economic factors in flood-prone regions. Focusing on Lumberton, North Carolina, which was severely affected by hurricanes Matthew (2016) and Florence (2018), the study leverages data on income, housing stability, race/ethnicity, gender, and education alongside building information to predict economic losses. A Random Forest or Gradient Boosting model will be trained on historical flood and economic loss data, using SoVI components as predictive features. To ensure interpretability, explainable AI (XAI) techniques such as SHAP will be applied, offering insights into the influence of social vulnerability factors on model predictions. Additionally, a sensitivity analysis will assess how changes in vulnerability components impact predicted losses. This approach not only improves prediction accuracy but also provides actionable information for policymakers to better target disaster mitigation and recovery efforts. By incorporating social vulnerability into flood loss models, this work aims to deliver more equitable and effective solutions for disaster resilience.",1,119 | Cobb Galleria Centre,The Future of Urban Resilience and Smart City Innovation (SDG 11),177
594,8626.0,Application of Principal Component Analysis as an Unsupervised Machine Learning to Model Socio-Economic Status of a Community under Natural Disaster.,Academician,"Community resilience is defined as an ability of an ordinary community to plan, prepare, absorb and recover from a natural or manmade disasters. Construct a comprehensive framework that includes all variables which influenced by a disaster and depict variation in them under this such event is vital. To achieve these kind of frameworks researchers and scholars introduce various frameworks. These frameworks suffered from many weaknesses and limitation that was the key motivator of our study to develop a custom framework based on the needs and limitations of Jefferson County, TX that we named it CRISys- Community Resilience Indicator System-. To develop it we looked community resilience in data-driven approach and utilized data analysis techniques. In this research we applied Principal Component Analysis as an unsupervised machine learning to reduced dimensionality and weigh to 22 variables that we explored for socio-economic status as a main dimension in CRISys framework. Data collection was done based on the reliable sources such as the United States Census Bureau that reflects the variable's values for seven consecutive years from 2015 to 2021. The results show CRISys framework can illustrate socio-economic status of Jefferson County, TX under a disaster event and complies to mathematical representation of community resilience.",2,119 | Cobb Galleria Centre,The Future of Urban Resilience and Smart City Innovation (SDG 11),177
595,7053.0,Sustaining Smart Cities: Benchmarking Analysis,,"Development and sustainability of smart cities are driven by the integration of various elements including community, technology, favorable policy, and others guided by smart workforce. Chattanooga is a vibrant city, seat of Hamilton County, located in the southeast of the state of Tennessee. In view of smart workforce preparedness as part of Chattanooga’s, thrive to become the hub of a sustainable smart city, it is imperative to evaluate the academic achievement and graduation rates of Hamilton County students. This research, as a continuation of two previous studies, mainly focuses on benchmarking analysis. Preliminary data was gathered from two magnet schools, namely, STEM School and Tyner Academy. The research aims to guide policymakers, educators, and stakeholders to enhance graduation rates and educational quality that streamlines students’ preparedness for a tech-centric economy. Moreover, it is anticipated that the results of this research can be generalized as needed to serve other school districts, as a piece of best practice in smart city sustainability and progression. Looking at some of the factors influencing academic performance such as socioeconomic backgrounds, school-based intervention methods, and economic & racial diversity in schools, it was observed that the STEM School exemplifies socio-economic and racial diversity standpoints. Also, it has high academic performance and graduation rates. On the contrary, Tyner Academy faces challenges in terms of low academic performance, a high enrollment of economically disadvantaged students, lower economic and racial diversity, and lower graduation rates. To capture in-depth analysis, with students’ views in mind, a survey is currently ongoing.",3,119 | Cobb Galleria Centre,The Future of Urban Resilience and Smart City Innovation (SDG 11),177
596,5033.0,A Double Auction Mechanism for Token-based Carbon Credit Trading in Blockchain-based Carbon Market,Academician,"The voluntary carbon market (VCM) is crucial to enabling businesses and individuals to achieve net-zero climate goals and support global climate efforts. Driven by net-zero commitments and goals, the VCM was worth $ 2 billion in 2022 and is predicted to expand to $ 250 billion in 2050. However, current VCMs face many challenges such as inefficient trading, credibility issues, and double counting. This paper presents a novel double auction mechanism for token-based carbon credit trading using blockchain technology, addressing the need for efficient, credible, and decentralized governance in voluntary carbon market. First, smart contract is used to execute the trades when bids from carbon credit buyers match asks from sellers at a market clearing price. Second, tokenization allows carbon credits to be represented as digital assets, facilitating seamless trade while adhering to standardized carbon certification processes. Finally, we simulate optimal token pricing strategies under various market conditions, providing insights into the economic dynamics of the carbon credit market. Our findings demonstrate that this approach can effectively support the global shift toward more sustainable and credible carbon markets.",1,119 | Cobb Galleria Centre,Approaches to Carbon Neutrality and Climate Change Mitigation (SDG 13),178
597,7008.0,Towards An Individual Decision-Making Framework to Achieve Mitigation of Climate Change Effects with a Local Perspective,Practitioner,"Natural disasters in Puerto Rico, already a constant threat to the island, are becoming more frequent and severe due to climate change. This research focuses on the role of individual decision making in the selection of plants to decrease the effects of wind, wildfires, floods, plagues and high temperatures. Initial results from four locations across the island recognize the important role that certain plant characteristics and their environment play in helping them survive and mitigate the effects of climate change. These characteristics include leaves, space, shade, crown shape, and type of stem body, among others. The results emphasize the power that individual decision making, and citizen organizations have in the design of their environment to face climate change.",2,119 | Cobb Galleria Centre,Approaches to Carbon Neutrality and Climate Change Mitigation (SDG 13),178
598,6006.0,Can cost overruns in projects be reduced? A re-examination of cost-duration dependencies.,Practitioner,"The share of projects in global economy rises consistently. Yet, all too often, projects are substantially overdue, substantially overrun in costs, fail to deliver the promised value, or combinations of the three. Worldwide, capital projects have exceeded their approved forecasted budgets for centuries. Despite Immense efforts which have been invested throughout the years in trying to improve this reality, it is still rather disappointing. As some experts summarized: hundreds of years of pain with minimal gain. In this presentation, time/duration-cost relationships are reexamined. The outcome of this examination is the understanding that different dependencies of costs on durations might exist, and at all levels: from individual activities and up to the project level. In particular, cases have been found where very few activities determined the outcome of much larger projects. These distinctions are explicitly considered in cost estimations during project planning and are included and integrated in project control. The application of the integrated control scheme and its advantages are demonstrated using data from real projects.",1,Ansley | Renaissance Waverly Hotel,Systems Engineering Applications,179
599,6738.0,A way forward in election security: Technology and security considerations,Academician,"This exploratory study investigates integrating emerging technologies in United States elections to enhance security and transparency. As digitalization progresses and technology evolves, electronic voting equipment such as Ballot Marking Devices (BMD) and Precinct Count Optical Scanners (PCOS) have improved speed of vote counts and auditability but are not immune to potential cyber, physical, and insider threats. This research highlights the limitations of current systems and processes, which can undermine public trust and electoral integrity. After examining potential solutions proposed by the literature, potential ways forward emerge, including distributed ledger technologies (DLT), which offer immutable, decentralized ledgers that enhances transparency, mitigate insider and cyber threats, and address potential issues related to vote verification. This study offers a perspective that identifies the need for scalable, interoperable solutions to improve processes along with advances in technology to support security and integrity of critical infrastructure elections equipment. Future research should focus on practical implementation strategies that align with contemporary technologies to enhance system security.",2,Ansley | Renaissance Waverly Hotel,Systems Engineering Applications,179
600,5465.0,Stress Testing for Systems Engineering,Practitioner,"Often in the early stages of the systems engineering lifecycle, engineers will develop conceptual ideas to address stakeholder problems and issues. They will follow well-established techniques to describe, decompose, and model the functional and physical concept, resulting in a solution set of alternatives that may address these issues. In the latter part of the lifecycle, testing will often expose the limits of what is and isn’t possible, and the concept will need be matured to a testable design state that may take significant time to develop. We propose a means to “stress test” the conceptual model to identify the vulnerable area(s) of the system earlier to understand the limitations of the capability and quickly determine where the feasible area vs. the infeasible area of capability exists. By doing so may reduce waste and improve efficiency in the concept development, in order to focus on specific capabilities that may prove more fruitful towards accomplishing the overall objective. In order to demonstrate this, we propose a digital model and several analytical techniques using an illustrative example performed in the concept development phase in order to evaluate the feasibility of this approach.",3,Ansley | Renaissance Waverly Hotel,Systems Engineering Applications,179
601,8930.0,Reigniting the Transdisciplinary Nature of Industrial and Systems Engineering,Practitioner,"Industrial and systems engineering developed as a transdiscipline by integrating engineering principles and methods with mathematics, physics, and sociology to design, improve, and install integrated systems where people are present. Nevertheless, it has strayed from its transdisciplinary nature due to the necessary specialization of its knowledge areas and their respective application domains. Frederick Taylor stated that scientific management’s (ISE) purpose was to maximize employee and employer benefits while reducing impacts on society and the environment through appropriate resource utilization. This is an increasingly complex problem that spans disciplines and domains of application, calling for a reunification of the ISE knowledge areas and their expansion to other disciplinary domains (such as AI). In this work, we adapt a framework, developed by INCOSE’s Bridge Team, that demonstrates how industrial and systems engineering is linked to numerous disciplines and domains of application. We establish that by connecting our motivation to our capabilities, a value loop emerges, enabling the pursuit of increasingly elegant solutions to increasingly complex problems. The bridge that connects motivation and capabilities is our transdisciplinarity; thus, the value of Industrial and Systems Engineering is increased.",4,Ansley | Renaissance Waverly Hotel,Systems Engineering Applications,179
602,6923.0,Enhancing Model-Based Definitions for Manufacturing and Metrology in Advanced Manufacturing,Academician,"This research aims to establish a systems engineering approach that integrates manufacturability and inspectability considerations directly into the design phase of advanced manufacturing (AM) processes. The rapid evolution of AM presents significant challenges for complementary systems to keep pace with essential design specifications required for effective implementation. Stakeholders are increasingly concerned with the quality, tolerances, efficiency, cost, and feasibility of manufactured parts. However, alignment with manufacturing workflows often falls short due to communication gaps among designers, integrators, modelers, manufacturers, and administrators. Limited knowledge sharing across enterprise, manufacturing, and metrology levels creates a substantial gap in understanding what can be effectively manufactured and measured during production. To address these issues, this research proposes a shift towards a ""manufacturing on the left"" approach, embedding the model-based definitions of cost, quality standards, manufacturability, and inspectability from the earliest design stages in the manufacturing systems engineering life cycle. By integrating design for manufacturing and metrology with improved enterprise design specifications, the study develops clear operational concepts and mission scenarios, and creates digital artifacts for systems, subsystems, and components. The system is decomposed into core-periphery modules to facilitate future prototype development with AM processes. This early incorporation of manufacturability knowledge enhances stakeholder understanding of final product viability, improves development efforts by upholding rigorous standards, and strengthens the resilience of the manufacturing ecosystem through integrated product design and verification. This foundational shift enables the creation of manufacturing- and inspection- ready systems capable of fulfilling mission-specific requirements and meeting future demands in the evolving manufacturing landscape.",1,Ansley | Renaissance Waverly Hotel,Systems Engineering in Manufacturing,180
603,6994.0,A Distributed Manufacturing System,Academician,"From the mass production era, manufacturing systems began to be highly centralized for production efficiency, cost savings, and easy management. This approach is effective and significantly reduces the product cost, making products more accessible to the public. However, it has numerous drawbacks, such as a lack of design flexibility, limited customization, risk of overproduction, and high initial investment. Therefore, a more robust and decentralized manufacturing system is needed to improve efficiency, facilitate small game players’ participation and mobilize idle and less competitive resources. To achieve this, a distributed manufacturing system that can coordinate them and enable communication between them and the customer. Moreover, to ensure quality, numerous standards need to be established, such as a model-based manufacturing protocol and standardized quality inspection methods. In this work, a plan is being proposed on how to achieve such an ambition. This plan first designs the functionality of the system then identifies all the system's key components and specifies them with stages from minimal to vision. The system requirements and preliminary approaches of the initial version are documented. Following that, to better introduce the functionality of the system, a customer story is being made from both customer and supplier perspectives.",2,Ansley | Renaissance Waverly Hotel,Systems Engineering in Manufacturing,180
604,6716.0,Supervisory Control and Data Acquisition Systems for Utilities and the Dawn of Industry 5.0,Practitioner,"Supervisory Control and Data Acquisition (SCADA) systems are essential for the operation of distributed industrial processes i.e. water systems or electrical grids. While the primary function of SCADA is to monitor and control physical processes, SCADA also performs secondary functions that are critical to continuous operation and improvement of the overarching physical processes. SCADA systems contextualize real-time data to inform decision-making, alert stakeholders to potential issues, and mitigate downtime via situational awareness. Beyond contextualization of process data, SCADA systems are a critical component of analysis and utilization of real-time process data for optimization. When designed appropriately, SCADA system architecture facilitates comparison of real-time data to historical data and/or process models, enabling capability for dynamic process control changes to optimize system performance in real-time. SCADA Systems are composed of many interconnected components commonly organized by geographical regions, process control areas, and/or digital zones. As industrial processes become more complex and technology advances, new challenges and opportunities emerge, necessitating that SCADA systems are designed not only to optimize performance but also to enhance resilience, and safeguard against physical and cyber threats. This paper examines the evolution of SCADA systems, explores current state and future trajectories, and applies a requirements-based systems engineering framework. It also conducts stakeholder analysis, proposes a comprehensive System Development Lifecycle, and presents a Systems Integration Plan derived from the V-model. Additionally, the paper outlines a traceable Verification, Validation, Testing, and Training plan and utilizes a Failure Mode and Effects Analysis to evaluate and improve system resilience.",3,Ansley | Renaissance Waverly Hotel,Systems Engineering in Manufacturing,180
605,5053.0,How may I be of assistance?   Systems Engineering's Multifaceted Role on Aerospace Programs,Practitioner,"Systems Engineers provide a critical function to aerospace programs to meet compliance expectations in Design Requirements but also Interfaces, Manufacturing, Quality, Condition of Supply, and Contracts. This presentation will step areas of impact with discussion of lessons learned and iterative improvements which are demonstrating value to the company and the building of the function as program keystone for success.",4,Ansley | Renaissance Waverly Hotel,Systems Engineering in Manufacturing,180
606,4811.0,Improving Railroad Crossing Safety with Positive Train Control and Autonomous / Connected Vehicles,Academician,"In 2023, the Federal Railroad Administration and the Department of Transportation recorded 766 injuries and 247 fatalities due to train-vehicle collisions, indicating a persistent challenge in addressing railroad crossing safety. Additionally, next-generation autonomous and connected vehicle (AV/CV) technologies, along with new rail safety systems like Positive Train Control (PTC) are becoming increasingly widespread, yet there is little understanding of how they might work together to enhance railroad crossing safety. We explore ways to combine these technologies to improve rail safety, specifically in relation to driver error, warning efficacy, preemption techniques and limitations, and information transparency. Furthermore, we employ data mining techniques to determine the most significant risk factors at railroad crossings, and therefore the best targets for improvement. Additionally, we use human factors methods, such as human information processing models and interface design standards to analyze the effectiveness of existing and proposed railroad crossing warning systems. Our analysis will guide recommendations that leverage PTC, legacy infrastructure (track circuits), and state of the art technologies (AVs/CVs) to reduce the number and severity of incidents involving trains, motor vehicles, and pedestrians. Recommendations will be supported by data and heuristics while accounting for diverse stakeholder needs and future technological challenges such as the looming radio frequency deficit. We propose warning redesigns, new information transmission processes, and an infrastructure system that integrates rail safety systems and AVs/CVs, while focusing on implementation-candidate technologies and throughput optimization.",1,Ansley | Renaissance Waverly Hotel,Systems Engineering in Safety and Risk Management,181
607,9118.0,Blockchain-based AI-assisted Cyber-Physical Systems for Robust and Reliable Machining Processes,Academician,"This study demonstrates the prospects of Blockchain-based Cyber-Physical Systems to establish a scalable framework for designing a secured, automated, and traceable modeling in machining processes. Machining operation like turning, being inherently complex, relies on different types of explanatory parameters such as feed rate, depth of cut, cutting speed, tools, and environmental factors. All of these variables significantly influence key response variables like surface quality, tool wear, cutting forces, and energy consumption. The proposed Blockchain-based framework enables the reliable exchange of comparative data streams within a unified data analytics platform. By replacing traditional databases with Blockchain-enabled systems, information becomes immutable and readily accessible, which reduces the credibility gap in shared manufacturing environments. The Blockchain-based system can automate decision-making processes by verifying predefined systemic and operational criteria, while ensuring data accuracy and reliability. The inclusion of Artificial Intelligence (AI) and Machine Learning (ML) within the ecosystem empowers system architects and engineers to evaluate design alternatives effectively and make predictions from reliable data. This approach can minimize human error, smooth streamline operations, and enhance process efficiency.",2,Ansley | Renaissance Waverly Hotel,Systems Engineering in Safety and Risk Management,181
608,5648.0,Overview of Model Based Systems Engineering,Academician,"Model-Based Systems Engineering (MBSE) is an emerging methodology that enhances traditional modeling and systems engineering practices. It supports the development of the Concept of Operations (ConOps), as well as requirements, design, testing, risk management, configuration management, verification, and validation of the system design. MBSE serves as a crucial enabler for program managers and systems engineers, facilitating programmatic and technical decisions that can impact the integration and interoperability of various systems. Compared to the conventional document-based approach, MBSE offers significant advantages. In a document-based methodology, various authors produce multiple documents to capture the system's needs and design. In contrast, MBSE establishes an authoritative source of truth (ASOT) for the system, creating descriptive and specific views through model elements. Utilizing MBSE can bring a range of benefits to a program, including improved communication among scientists, systems engineers, program/project managers, and stakeholders. This is achieved using diagrams and tables that support development and help identify potential issues and risks that may arise during integration and testing activities. MBSE comprises three key components: Method, Language, and Tools. The method helps capture essential system artifacts, such as requirements. The language fosters effective communication among stakeholders by employing a common modeling language, such as the Systems Modeling Language (SysML). Finally, tools are developed to support the method and language, with Cameo Systems Modeler being a commonly utilized tool.",1,Highlands Ballroom | Renaissance Waverly Hotel,Model-Based Systems Engineering,182
609,8985.0,Facilitating Model-Based Systems Engineering with Data Element Mapping and Analysis (DEMA),Practitioner,"This paper draws connections between Data Element Mapping and Analysis (DEMA) and established modeling frameworks, processes, and languages. DEMA is a technology-agnostic approach that aims to achieve the full potential of the Digital Thread and Model-Based Enterprise (MBE) by overcoming the barriers of fragmented data structures and the lack of standardization within organizations. The recognized modeling frameworks referenced in this study include the Department of Defense Architecture Framework (DoDAF), a structured approach for defense systems architecture, and the Unified Architecture Framework (UAF), which is designed to align complex systems engineering with enterprise architecture. The modeling process referenced in this study is the Object-Oriented Systems Engineering Method (OOSEM), which leverages object-oriented principles to streamline systems design. The modeling language referenced is Systems Modeling Language (SysML), a general-purpose modeling language used for systems engineering applications. By linking the DEMA process to these respected modeling techniques, we demonstrate how it can complement and even enhance these methodologies, extending their capabilities in terms of data granularity, digital system architecture, and lifecycle management. The analysis outlines how DEMA can be incorporated into an organization’s preexisting modeling methods, enabling the full realization of digitalization benefits while reducing organizational risk, increasing efficiency, and improving decision-making capabilities.",2,Highlands Ballroom | Renaissance Waverly Hotel,Model-Based Systems Engineering,182
610,6344.0,A Model-Based Systems Engineering Approach to Designing a Water Booster Pump Station Incorporating Cyber-Informed Engineering Principles,Academician,"Water booster pumps play a critical role in infrastructure by enabling the distribution of water from treatment plants to residential and commercial areas. With many of these systems now operated remotely and managed via cloud-based controls, securing them against cyber threats is crucial to prevent disruptions, malicious manipulation, and targeted attacks. This abstract illustrates how the Object-Oriented Systems Engineering Method (OOSEM), combined with Model-Based Systems Engineering (MBSE) using System Modeling Language (SysML), integrates Cyber-Informed Engineering (CIE) principles into water booster pump station design. By adopting proactive cybersecurity mindsets from the earliest stages, CIE helps mitigate vulnerabilities through embedded controls and defense mechanisms. MBSE’s comprehensive modeling framework supports robust simulation and analysis, enhancing system resilience against cyber threats while enabling secure remote monitoring. This approach not only improves the reliability, safety, and sustainability of water supply systems but also sets a new systems engineering approach for critical infrastructure engineering. Addressing unique challenges in smaller municipal utilities, this methodology fosters secure, efficient, and adaptable water systems that meet evolving community needs and defend against growing cyber risks, thereby building a more resilient water infrastructure.",3,Highlands Ballroom | Renaissance Waverly Hotel,Model-Based Systems Engineering,182
611,8933.0,The Nature of Industrial and Systems Engineering Principles and how to Evolve them to Enhance our Transdisciplinary Capability,Academician,"Industrial and systems engineering integrates engineering principles and methods with mathematics, physics, and sociology to design, improve, and install integrated systems where people are present. This has resulted in numerous guiding propositions such as heuristics, principles, conventions, and perspectives that guide our actions when intervening in complex systems. However, the maturity and applicability of these guiding propositions are uneven and even unclear. Amid the 5 th Industrial Revolution, where we are moving past a focus on full automation into the integration and collaboration between humans and intelligent systems, gaining insight into the maturity of our guiding propositions is imperative. In this paper, we introduce a framework, developed by INCOSE’s Bridge Team, that illustrates the origin and evolution of all guiding propositions. The evolution process is valuable to ISEs as it will guide us in determining when and how to evolve our guiding propositions, thus enhancing the transdisciplinary nature of ISE.",4,Highlands Ballroom | Renaissance Waverly Hotel,Model-Based Systems Engineering,182
612,6590.0,AI-Driven Insights: Predicting Systems Engineering Skills with Machine Learning Algorithms,Academician,"With the rapid growth in technology and proliferation of information, the behaviour and structure of modern complex systems presents escalating challenges. The problems stemming from increasing complexity continue to confound our capabilities to maintain and secure reliability in systems. Thus, there is a need to develop a proficient group of systems engineers who can effectively address the complex system problems. The objective of this study is to apply Machine Learning (ML) algorithms to predict the System Engineering (SE) skills of engineers by examining their competency based on six fundamental attributes of SE such as requirement engineering, hierarchical view, system life cycle, system design and integration, and interdisciplinary approach . To that end, different ML algorithm such as Support Vector Machine, Logistic Regression, K-Nearest Neighbours, Random Forest and Adaboost will be applied to develop a predictive ML model for SE skills of engineers based on aforementioned predictors (attributes). Further, machine learning models will be hyper parameter tuned to enhance the performance of prediction and then most efficient ML model will be identified through the comparative analysis that can better predict the SE skills of engineers. Employers can use the proposed data-driven models to better understand the skillsets needed for SE roles, streamline recruitment processes, and tailor training programs to fill skills gaps. Additionally, educational institutions can use outcome of the predictive insights to develop curricula that are more closely aligned with market demands.",1,Highlands Ballroom | Renaissance Waverly Hotel,Emerging Computing Technologies in Systems Engineering,183
613,9030.0,Cyber-Informed Engineering (CIE) Integration into MBSE,Practitioner,"Engineering design in the field of industrial engineering, such as designing automated factories or warehouses, is critical for the effective operation of facilities. Any design flaws introduced early can result in significant capital expenses to correct. However, early-stage engineering design is inherently complex. The systems are not yet built, requiring designers to integrate various aspects, including digital engineering and cybersecurity, to support virtual representations throughout the design process. In this study, we propose an approach to integrate Cyber-Informed Engineering (CIE) principles into model-based systems engineering (MBSE). This approach facilitates the development of a digital thread for engineering systems, ensuring secure digital artifacts in the design of industrial engineering systems.",2,Highlands Ballroom | Renaissance Waverly Hotel,Emerging Computing Technologies in Systems Engineering,183
614,7030.0,Leveraging Quantum Computing for Modeling Using MBSE: A Literature Review,Practitioner,"Quantum computing is commonly described as a type of computation that uses the principles of quantum mechanics to process information. Specifically, quantum computers can handle complexity efficiently using superposition and enhanced problem-solving capabilities as a result of its utilization of the quantum bit instead of the bit used in classical computing. In the age of digital manipulation, the capabilities of quantum computers have allowed for a more holistic understanding of quantum computer’s ability to bring about solutions to cybersecurity, banking and finance, and advanced manufacturing systems. Some of the most prevalent challenges to develop quantum algorithms include benchmarking and error mitigation techniques necessary for software development and hardware implementation and interfacing. To address these shortcoming, Model-Based Systems Engineering (MBSE) is being proposed to develop a framework through a formalized application of modeling to support system requirements and traceability and create models to support system behavior and how quantum machines will interact with the software and hardware to increase stakeholder understanding. Other MBSE techniques like recursive validation and verification throughout the project lifecycle ensure that quantum-based projects reach the highest level of development. This research will use a systematic literature review methodology for evidence-informed research through a bibliometric analysis. Therefore, the aim of this literature review is to identify past or current trends in existing research that applies MBSE methods to quantum computing. This paper will identify where MBSE has been used to add value in quantum computing and set priorities for future work",3,Highlands Ballroom | Renaissance Waverly Hotel,Emerging Computing Technologies in Systems Engineering,183
615,5861.0,"A System of Systems: AI and Cybersecurity - People, Hardware, and Software to Deliver on AI.",Practitioner,"The omnipresence of software and semiconductors has profoundly reshaped our world, advancing faster than our ability to manage risks to public health, safety, and welfare. Emerging technologies like AI, automation, and robotics form an intricate web of hardware, software, and human factors, reshaping systems that permeate daily life. Digital transformation now spans Software Factories, Data Warehouses, Software Bills of Materials, and complex Software Supply Chains, driven by exponential reliance on Open-Source Software. This evolution raises ethical questions and emphasizes the need for industrial and systems engineers to uphold public safety, especially as software becomes integral to manufacturing, defense, infrastructure, and control systems. The profession must prevent digital mishaps, often caused by overlooked fundamentals in systems design, quality, and ergonomics. As open-source components make up 90% of applications, the risk of malware, bugs, and counterfeit hardware grows. New legislation pushes for liability on vulnerabilities within integrated, interdisciplinary systems and organizations. This session examines supply chain practices in the digital era, showcasing both successes and pitfalls. Industrial and systems engineering principles are essential for addressing AI talent shortages in national security and for establishing standards of care. The integration of social, mathematical, and physical sciences is increasingly vital. Targeted at public policymakers, engineers, software developers, and security leaders, this presentation underscores the importance of a collaborative approach to navigating the digital landscape. Leveraging an industrial and systems engineering workforce and framework, it advocates for comprehensive, ethically grounded responses to these transformative changes.",4,Highlands Ballroom | Renaissance Waverly Hotel,Emerging Computing Technologies in Systems Engineering,183
616,6025.0,A Decision Analysis Data Model for Digital Engineering Decision Management,Practitioner,"Industrial and systems engineers must be able to inform program/system management decisions in a digital engineering environment with big data and descriptive, predictive, and prescriptive models. Systems Design and Engineering (SE) is one IISE Body of Knowledge areas. Decision Management is a SE life cycle process for making program/systems decisions. The purpose of the decision management process is: “…to provide a structured, analytical framework for objectively identifying, characterizing and evaluating a set of alternatives for a decision at any point in the life cycle and select the most beneficial course of action.”( ISO/IEC/IEEE 15288 ), This presentation reports on the INCOSE Decision Analysis Working Group’s project to develop a Decision Management Data Model (DADM) based on the Decision Management life cycle process described in the INCOSE SE Handbook and the Systems Engineering Body of Knowledge. The goal is to provide the digital DADM to organizational Decision Management Life Cycle process owners to use directly in their digital engineering decision management processes for a system development in any life cycle stage.",1,Highlands Ballroom | Renaissance Waverly Hotel,Systems Engineering and Decision Support,184
617,6513.0,MBSE-Driven Optimization of Resource Allocation for Hospital Bed Cleaning Services,Practitioner,"Efficient allocation of resources for hospital bed cleaning is critical to maintaining patient flow, reducing patient wait times, and ensuring a high standard of care in healthcare facilities. This paper presents a Model-Based Systems Engineering (MBSE) approach to optimize server allocation for hospital bed cleaning services. Using MBSE principles, we develop a system model that captures the interactions between patient discharge and bed cleaning. As the current state of queueing theory is not apt to solve staffing optimization problem with non-stationary arrival processes, we propose a discrete-event simulation model to evaluate different staffing scenarios and minimize the number of cleaning personnel required to meet service level targets. The optimization process considers factors such as non-stationary patient discharge rates and data-driven variability in cleaning times. Our results demonstrate significant improvements in bed turnover times and resource utilization, providing an analysis framework that hospital administrators can use for data-driven decision-making. This study highlights the potential of MBSE as a powerful tool for addressing complex operational challenges in healthcare, paving the way for more efficient and adaptive resource management strategies.",2,Highlands Ballroom | Renaissance Waverly Hotel,Systems Engineering and Decision Support,184
618,6398.0,Integrating Diverse Stakeholder Input into Resilience Investment Decision-Making Processes,Academician,"Weather and man-made disruptions are increasing the need for resilience decision-making. Stakeholders are critical components of a system (e.g., a system or a system of systems). In addition, each stakeholder has their own opinions. Taking into account those diverse opinions across stakeholders can increase the quality of the decisions. However, assessing and balancing stakeholder inputs can be difficult since different stakeholder groups each have their own priorities. In this work-in-progress, we are using an online survey to demonstrate the impact of aggregating stakeholder input on decision quality. Participants are asked about their priorities regarding each resilience criterion and answer questions related to their personal interests that are associated with their stakeholder influence. The participants’ inputs are then weighted based on stakeholder influence attributes. The attributes are initially weighted based on historical evidence. We use sensitivity analysis to determine how much the weight affects the priorities and compare the equity implications of aggregation methods. The weighted inputs are aggregated together to analyze stakeholder group differences and similarities as well as create a list of priorities for decision-makers. The priority list can then be used to evaluate which investment projects should be selected based on available funding. Effectively weighing and prioritizing different stakeholder groups may help decision makers prioritize resilience investment projects and provide support for team discussions.",3,Highlands Ballroom | Renaissance Waverly Hotel,Systems Engineering and Decision Support,184
619,8634.0,"A Framework for Systems Engineering Digital Transformation: Integrating NIST CSF, CISCO Strategies, and the Vee Model",Practitioner,"This study evaluates the strategic integration of digital transformation methodologies within aerospace components manufacturing industry, focusing on enhancing the quote generation process for Request for Proposal (RFP) and Request for Quote (RFQ) through advanced digital technologies. By synthesizing the National Institute of Standards and Technology (NIST) Cybersecurity Framework, the VEE Life Cycle Model, and Cisco’s Digital Transformation Model, we aim to develop a secure, efficient, and adaptive digital organization. The paper details the systematic approach of cybersecurity measures, data integration tools, and process automation to optimize workflow and improve and generate reliable quotes in an efficient manner. The study highlights the challenges of digital transformation, including the integration management of cybersecurity risks. Suggested solutions will result in significant improvements in process efficiency and data management, addressing cybersecurity using industry best-practices. The paper proposes future strategies focusing on the use of Artificial Intelligence (AI) and Machine Learning (ML) for predictive analytics and system optimization, advocating for an agile, data-driven approach. The findings suggest that with careful planning, comprehensive risk management, and continuous improvement, digital transformation can significantly enhance the accuracy and efficiency of quoting process, positioning industry for long-term success in a dynamic business environment.",4,Highlands Ballroom | Renaissance Waverly Hotel,Systems Engineering and Decision Support,184
620,6936.0,A Comparative Analysis of Resilience Across Different Classes of System of Systems: Insights from Transportation and Healthcare Systems,Academician,"Rooted in ecology, resilience emerged as a vital concept for safety-critical systems facing possible disruptions such as conflicts, natural disasters, disease outbreaks, among many others. Consequently, many research communities strive to conceptualize, measure, and manage resilience. However, despite this shared interest, there is a disconnect between these communities, along with their perspectives of resilience and methods to supplement it. To that end, this study analyzes the literature of two distinct classes of systems of systems (SoS); namely transportation and healthcare systems; with the objective of documenting the differences between their views on resilience and to identify opportunities for knowledge transfer. We report two main findings. First, there is great diversity in the perception, measurement, and management of resilience across these communities, primarily driven by their contextual settings and objectives. For instance, the healthcare community emphasizes organizational resilience, often relying on qualitative methods whereas, the transportation community aims to engineer resilience into the systems under concern by utilizing quantitative methods more frequently. Such differences provide opportunities for knowledge transfer; however, feasibility and efficacy of transferred approaches require further research given contextual differences. Second, while these communities share a common set of practices, they differ in nuance and application. For example, both integrate human-in-the-loop principles and flexibly delegate authority to decision-makers, though two systems emphasize these practices in different ways. Collectively, findings of this research should help bridging the gap between these communities and enable engineering a more resilient world.",1,Highlands Ballroom | Renaissance Waverly Hotel,Systems of Systems (SoS),185
621,9120.0,A System of Systems (SoS) Meta-Architecture Approach to Design Digital Platform-based Domestic Worker Distribution System,Practitioner,"In this research, a System of Systems (SoS) meta-architecture is conceptualized to design a digital platform-based system framework for the distribution of domestic workers to boost the crowd-sourced economy. While Object Process Methodology (OPM) is used to articulate the relationships between the objects and functions, Design Structure Matrix (DSM) has been applied to address the interactions between individual components to categorize them into subsystems in order to create the system architecture. This SoS architecture incorporates several Key Performance Attributes (KPA) and Key Performance Parameters (KPP) to systematically evaluate the meta-architecture. The Analytical Hierarchical Process (AHP), Pugh’s Evaluation Matrix, and Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) are employed to evaluate different levels of system architecture. To estimate the system capability, the most significant KPPs for each KPA are used to design the Adaptive Neuro-Fuzzy Inference System (ANFIS). This system architecting and evaluation approach distinguishes critical factors and attributes of the SoS architecture for the domestic worker supplier system, mitigating potential risks and identifying unwanted design features.",2,Highlands Ballroom | Renaissance Waverly Hotel,Systems of Systems (SoS),185
622,8722.0,The Drivers of Infrastructure Decentralization: Modeling Infrastructure Under Stochastic Disruption,Academician,"We consider several infrastructures: water delivery, wastewater collection, electricity delivery, hydrogen delivery. Each of these services can be provided by a centralized or decentralized system. Some may have similar architectures; some (e.g. electricity) have distinct architectures. We use engineering economics to baseline system size. We use the relevant parameters: cost of centralized and decentralized systems, cost of connection, and density of demand from customers to derive baseline size of centralized systems vis-à-vis decentralized system. To provide insight into the factors influencing decentralization or centralization, we extended these infrastructure models to an Ising model approach, used in statistical physics for studying phase transitions. Ising models include the failure risk of a connected system. As the failure risk grows, these models exhibit a critical point at which the network transitions from being fully connected to largely disconnected. Under a continuous rebuilding model, we show that with increasing probability of disruption, the system exhibits self-organizing criticality, in which the size distribution of the connected portions exhibits a power law corresponding to the critical point of the system.",3,Highlands Ballroom | Renaissance Waverly Hotel,Systems of Systems (SoS),185
623,8625.0,Using Capella in an ISE Systems Design Course,Academician,"Relative to other engineering disciplines, ISE education has been hampered by the absence of computational design tools. Systems modeling languages, like UML and SysML are very capable for documenting system designs, but many ISE students (and faculty) are not familiar and have little motivation to become competent users. Moreover, these are modeling tools and not design tools. In contrast, Capella is an open source, Eclipse-based software tool that implements the RFLP (requirements analysis, functional analysis, logical architecture, and physical architecture) design methodology. This talk will describe experiences using Capella along with several CICMHE design cases to teach design thinking and design principles to undergraduate and masters students. A major issue in this kind of course is the student’s lack of domain knowledge, and the use of Capella provides a natural mechanism for introducing required domain knowledge “on demand”.",1,Highlands Ballroom | Renaissance Waverly Hotel,Systems Engineering In the ISE Curriculum,186
624,6015.0,From Classroom to Global Impact: Systems Thinking in Addressing UN-Identified Challenges,Academician,"The United Nations (UN) is one of the biggest universal global organizations. Over the last seven decades, the UN has been focused on addressing global challenges such as aging populations, climate change, and ending poverty, among others. It is crucial that comprehensive effort is dedicated to meeting global challenges by increasing education and awareness among young people to address these complex problems. This study examines how higher education juniors and seniors in a systems course applied systems thinking to address global challenges identified by the United Nations. Over a 16-week semester, students explored complex systems methods to develop holistic solutions using the simulation-based tool NetLogo. The analysis of their work revealed four distinct clusters that capture their systems thinking approaches: Resource Management and Sustainability, Public Health and Social Equity, Governance and Policy Compliance, and Technological Solutions and Innovation. Each cluster demonstrated students’ ability to recognize interdependencies within global issues, applying concepts such as feedback loops, preventive care, and adaptive policy. In the sustainability cluster, students highlighted long-term ecological impacts, while in the public health cluster, they emphasized equitable access to resources. The governance cluster underscored the importance of trust in policy compliance, and the technology cluster focused on adaptability and real-time responsiveness. Collectively, these insights suggest that students approached global issues with an integrative, sustainable mindset, recognizing that effective solutions must address interconnected social, environmental, and technological systems. This research contributes to our understanding of how systems thinking can be fostered in engineering education to tackle complex, real-world problems.",2,Highlands Ballroom | Renaissance Waverly Hotel,Systems Engineering In the ISE Curriculum,186
625,6849.0,A Proposed Categorical Systems Theory: Mathematical Foundations for Engineering Complex Systems,Academician,"Engineering activity, especially when dealing with complex systems, is characterized by a large set of approaches such as frameworks, theories, methods, methodologies, and tools focusing on specific aspects of systemic attributes such as performance, materials transfers, information flows, etc. Yet, these approaches exhibit inconsistent maturity and effectiveness, resulting in different usage of terms and their association with fundamental systems engineering concepts. Each approach brings a relative engineering point of view typically related to a subset of contexts, purposes, dynamics, and perspectives. This brings significant challenges in our practice when dealing with complex systems. A systems approach has served us well in its different incarnations: complexity science, general systems theory, and cybernetics. However, even these systems approaches have failed to meet two demands: one is found in practice with problems of pluralism and dynamism, and the other stems from theory, where we need to be precise about what we talk about. The solution lies in taking a mathematical theory of context and importing it into a general theory of systems: a categorical systems theory. This work is an initial investigation into a small set of reference mathematical structures found in category theory capable of enhancing our ability to capture pluralism and dynamism and convey with precision our approaches to dealing with complex systems. A simple example is used to depict static and dynamic configurations, perspectives, and precision.",3,Highlands Ballroom | Renaissance Waverly Hotel,Systems Engineering In the ISE Curriculum,186
626,6377.0,Integrating Demand Prediction and Location Modeling for Shelter Service Planning Using Human Mobility Data,Academician,"Natural disasters have become more frequent and severe in recent years, significantly affecting the sustainability and resilience of essential services in communities. This trend emphasizes the urgent need for shelter service planning to protect vulnerable populations and facilitate recovery efforts in the face of natural disasters. However, effective shelter planning poses several challenges, including insufficient shelter management that completely relies on experience without utilizing high-quality data. In literature, most studies focus on either predicting shelter demand or optimizing shelter location allocation. This does not account for the dynamic interaction between demand prediction and location modeling. Therefore, our paper aims to address this research gap by developing an integrated shelter planning framework that incorporates demand forecasts into shelter location decisions. By leveraging human mobility data collected from large-scale mobile phone users during the historic Winter Storm Uri in Texas, we aim to develop a predictive model to estimate shelter demand. The discrete choice theory will be utilized to understand mobility behaviors in population subgroups. Finally, we will design an optimization model that incorporates predicted demand and shelter choice behaviors to guide decisions on shelter location. This study enhances the importance of strategic shelter planning in mitigating the impact of disasters and safeguarding community well-being.",1,108 | Cobb Galleria Centre,Session 5: Healthcare and Service Optimization,187
627,8894.0,The Potential for Smart Sensor Systems to Augment Novice Nurse Decision-Making,,"Given the high turnover among nurses, a significant portion of the workforce is comprised of recently graduated nurses. Novice nurses lack the clinical experience to perform holistic patient assessments. Furthermore, novice nurses strictly adhere to patient care protocols and are not able to leverage experience to engage in critical thinking about patient status changes or intuitively make clinical decisions. Due to inexperience among the majority of the nursing workforce, healthcare systems have begun implementing virtual nurses to surveil and assist novice nurses with patient care. However, these technologies have several limitations. Artificial intelligence (AI) solutions have been developed to support healthcare provider decisions in the clinical environment. However, these systems remove novice nurses from the decision-making loop and suggest specific courses of action or make diagnostic inferences without input from nurses or the provision of transparent reasons for suggestions. Alternatively, some AI-based decision support systems have been embedded in electronic medical records to provide diagnostic support. These systems are dependent on accurate patient data to inform their models. Given novice nurses’ limited experience, comprehensive patient assessments may not be reliably performed, which would directly limit the performance of these models. Our team has developed an explainable, AI-driven system to capture nurses’ attention allocation (i.e., based on mobile eye-tracking) and perform iterative behavioral analysis (i.e., compared to expert nurse behaviors) to automatically generate real-time suggestions for nursing assessments. This presentation will focus on summarizing preliminary work from our team in this area and next steps for decision support systems in healthcare.",2,108 | Cobb Galleria Centre,Session 5: Healthcare and Service Optimization,187
628,4864.0,Optimizing Aircraft Production Performance: A practical approach to standard work observations for complex manufacturing systems,Practitioner,"Site leadership were interested in bringing more business to the site, and were looking for ways to reduce flow, cost, and overall process improvement to be able to handle the new work statement with the same level of resourcing. An Innovation cell was established to address these issues and help the site accomplish its goals. Standard work observation is one initiative that was put in place to help identify improvement opportunities that could be replicated across the company, as well as implementing solutions to positively impact unit cost, WIP, safety, and cycle time reduction. The standard work observation was conducted through a cross-functional team walking through the work instructions of a selected set of high impact tasks, then observing the tasks being performed on the shop floor, collecting observation data, analyzing the data, and concluding with a brain storming session to help identify solutions with their implementation plan. The criteria for selection was the task was either on the critical path or had a high impact on the overall production process with respect to safety, cost, quality, and schedule performance. The studies found numerous opportunities for reducing flow, eliminating defects, improving ergonomics and safety. Furthermore, the findings provided valuable insights, highlighted opportunities for continuous process improvement efforts, and led to better resource utilization. While the study focused on a pilot area in the beginning, it established a foundation for future replication opportunities both within the site and other parts of the company and in industry.",1,108 | Cobb Galleria Centre,Operational Excellence Session 1,188
629,5046.0,Sepramesh Manufacturing Area Labor and Productivity Improvements,Academician,"Beckton Dickinson (“BD”) is a medical device company that strives to advance in the world of health. By embedding continuous improvement into their culture, a medical device company can achieve sustained success, delivering high-quality products and staying competitive. An opportunity identified in the Casting Process at Sepramesh is related to labor and productivity. In the last 6 months, this area experienced idle times between operations, adding 2 minutes per unit casted per shift. This resulted in low productivity, from 90% to 45%. In the past, BD used DMAIC methodology to improve labor productivity, reducing non-value-added activities and process wastes. However, control mechanisms were not adequately sustained, deteriorating productivity in Phasix at Sepramesh manufacturing areas. Existing research highlights operational inefficiencies, such as inefficient workflow design, older equipment, and a non-optimal facility layout, which can severely impact both labor performance and overall productivity. This study aims to evaluate the facility layout since it is crucial for optimizing workflow while investing in modern equipment to reduce idle time between operations from 2 minutes per unit casted to 0 minutes per unit casted in the next 3 months. To address these challenges, this study aims to identify and analyze the root causes of current labor and productivity inefficiencies within the Sepramesh manufacturing area, and propose actionable solutions, as well as training programs to ensure employees are well-equipped with the necessary skills to perform their tasks efficiently. Implementing these solutions will help BD Humacao enhance operational performance, meet customer demands, and secure a competitive advantage.",2,108 | Cobb Galleria Centre,Operational Excellence Session 1,188
630,6090.0,Improving Operational Efficiency for Meals on Wheels Central Texas: A Process Optimization Initiative,Practitioner,"Meals on Wheels Central Texas (MOWCTX) experiences significant challenges in managing call volume during peak hours, resulting in delays, missed deliveries, and decreased service reliability. With three staff members handling approximately 100 calls daily, volunteers and clients face difficulties in reaching staff promptly. This project aims to improve MOWCTX’s call-handling process to enhance communication, increase delivery accuracy, and ensure uninterrupted service for vulnerable clients. The first step involves mapping the current call process to identify bottlenecks and inefficiencies. By analyzing call data and understanding peak demand patterns, we aim to pinpoint critical times and tasks that create delays. Key methods will include workflow analysis, time studies, and staff interviews to gain insights into common communication barriers. By optimizing MOWCTX’s call-handling framework, the project seeks to decrease response times, improve delivery accuracy, and strengthen volunteer coordination. These enhancements will not only improve operational efficiency but also help ensure that each client receives their meal reliably and on time. Ultimately, this initiative will support MOWCTX’s mission by creating a more responsive and resilient communication system, better meeting the needs of those who depend on its services.",3,108 | Cobb Galleria Centre,Operational Excellence Session 1,188
631,8732.0,Integration of Technology and People — a People-Centric Approach to Industry 4.0,Practitioner,"Our objective is to explore the rationale and methodology behind the integration of technology (with human elements, aiming to bridge any existing gaps through the implementation of people-centric systems, followed by the application of these systems. The session should begin explaining the purpose, approach, and technological foundations of advanced manufacturing within the University of Tennessee, Oak Ridge Enhanced Technology and Training Center, and other partner organizations. The session will provide the background and guidelines progressing into Industry 5.0, where the emphasis shifts towards incorporating human-centric systems using the Optimized People Development System and The Sawhney Model, culminating with an example.",1,108 | Cobb Galleria Centre,Special Session Workshop: Integration of Technology and People — a People-Centric Approach to Industry 4.0,189
632,8897.0,"Application of IE Principles, Case Study 1: Developing Work Instructions for Manual Jobs",Practitioner,"This case study presentation will cover the development process and impact of Work Instructions (aka Prescribed Work Methods, or “PWM”) for a large Consumer Products distributor in southern California. The effort took place over four months in 2024 and resulted in 21 PWM for the current-state operations and another 10-15 future-state PWM based on optimization of processes and job station setups. The operation to date has deferred over $10 million in labor cost based on improved efficiencies from standardized work.",1,108 | Cobb Galleria Centre,Session 3: Technology Integration and Workforce Development,190
633,8899.0,"Application of IE Principles, Case Study 2: Long-range Facility Strategy for a Municipal Utility",Practitioner,"The case study will cover the development processes and outputs of a broad-spectrum network analysis of the properties of a large Municipal Utility provider in the southeast United States. The work included routing analysis and service center placement, warehouse and yard utilization, office utilization of call centers and administration workspaces, and potential long-range buy/sell/lease/sub-lease strategies to support planned growth trajectory over the next decade. The project spanned approximately five months in 2024 and was supported by experts from Supply Chain & Logistics, Process & Productivity, and Advanced Data Analytics.",2,108 | Cobb Galleria Centre,Session 3: Technology Integration and Workforce Development,190
634,6324.0,Engineering for Democracy - How do Engineers support voting in US elections?,Academician,"Every year, citizens across the U.S. participate in democracy through the seemingly simple act of voting. In reality, voting is a highly complex ecosystem. Constitutionally, the states are responsible for elections. Rather than a single set of election rules, election rules exist for each of the 3,000+ counties in the U.S. This complexity makes designing universal systems for election operations difficult, requiring significant effort to approve and implement even small changes. Despite this, those tasked with planning, operating, and designing elections are often under-resourced and overworked. Election administrators are required to master seventeen domains–with STEM-related aspects including designing ballots, training poll workers, allocating resources, accommodating citizens aged 18 and older while maintaining accessibility for those with physical and cognitive disabilities, and planning for in-person voting, vote-by-mail, and overseas voting. Despite these requirements, most election administrators are not trained designers, engineers, or experts in human factors, and their extensive workload limits their time to commit to any one task. Election administrators desperately need support, with many requesting help to no avail. Engineers have the capability and expertise to assist election administrators through the execution of research and direct collaboration in tackling challenges in planning and operating elections and related infrastructure. The Engineering for Democracy Institute is attempting to address this lack of participation by developing the STEM for Elections Network, a key pillar in growing the next generation of researchers. During this presentation, we hope to inspire others to see the systems of systems in elections, not just the winner.",3,108 | Cobb Galleria Centre,Session 3: Technology Integration and Workforce Development,190
635,8583.0,Deep Learning-based Driver Classification for Takeover Events in Autonomous Vehicles,Academician,"As automated driving systems continue to evolve, understanding driver takeover behavior remains critical to ensuring safe and seamless transitions. Takeover scenarios occur when the autonomous vehicle (AV) requests the human driver to regain control, typically due to system limitations, sensor failures, ambiguous traffic situations, safety concerns, or regulatory requirements. These scenarios are especially relevant for Level 2 and Level 3 autonomy, where drivers must be prepared to promptly resume control. This research introduces a framework for classifying driver types to improve takeover experience in AVs. The framework leverages data from various sensors, including driver monitoring systems, vehicle dynamics, and environmental context in a Virtual Reality driving simulator, to analyze and categorize drivers based on their response patterns. By employing predictive analytics and advanced deep learning models, such as Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks, the system evaluates factors like reaction time, gaze behavior, and hand movement to classify drivers into profiles, including normal, cautious, assertive, or aggressive. By understanding these profiles, the AV system can tailor takeover strategies and alerts to meet the specific needs and behaviors of individual drivers, enhancing overall responsiveness and adaptability. Experimental validation, using an immersive driving simulator built upon CARLA autonomous driving engine, demonstrates the framework’s effectiveness in accurately classifying driver types (i.e., accuracy of 0.99%, Precision of 0.99%, Recall of 0.99%) providing significant insights into the development of personalized and adaptive AV systems. This work lays the foundation for safer and more intuitive human-vehicle interactions, advancing the integration of autonomous technology.",1,108 | Cobb Galleria Centre,Session 6: Work Systems and More,191
636,8809.0,Can better work system design unlock employee performance in Nigerian banks?,Academician,"This study explores the impact of work system design elements—task complexity, job autonomy, feedback mechanisms, and technology integration—on employee performance in Nigerian banks. As banking operations increasingly rely on digital systems, understanding their effects on cognitive load, motivation, and decision-making is critical. Poorly designed systems can lead to cognitive overload and disengagement, while effective systems enhance productivity by reducing strain and fostering engagement. Data will be collected from 150 employees across roles in major Nigerian Commercial banks. Using structured and standardised surveys, participants will evaluate their experiences with work system design and its influence on cognitive and behavioural outcomes. Validated tools such as the NASA-TLX (for cognitive load) and Utrecht Work Engagement Scale (UWES) for measuring work engagement the interplay between system design and performance will be measured. Descriptive statistics and multiple regression analysis will be used to analyse relationships and identify significant predictors of performance. All statistics will be managed with SPSS v23. The study aims to provide actionable insights into optimising work systems to balance automation with human-centric design. Findings will highlight how Nigerian banks can improve task structures, autonomy, and feedback mechanisms to enhance employee productivity, engagement, and satisfaction. By focusing on both cognitive and behavioural outcomes, the research contributes to practical strategies for creating work environments that support high performance while addressing the challenges of digital transformation.",2,108 | Cobb Galleria Centre,Session 6: Work Systems and More,191
637,8806.0,Behavioural and Cognitive Impacts of Work System Design on Employee well-being,Academician,"The purpose of this research is to examine how work system designs impact employee well-being through behavioural and cognitive channels. This study hypotheses that well-designed work systems positively influence employee well-being by reducing cognitive load, improving motivation, and fostering better decision-making processes. Conversely, poorly designed systems can lead to cognitive overload, fatigue, and demotivation, ultimately impairing mental functioning and physical health of employees. The research will assess how elements of work system design, such as task variety, job autonomy, use of automation, and collaborative technologies, affect employee cognitive functioning, stress levels, and overall work behaviour. The study will involve surveys, and performance assessments of bank employees across several commercial banks in Nigeria. Through correlational matrix and multiple regression analysis, inferences will be drawn to interpret the impact of cognitive and behavioural impacts of cost analysis on the well-being of bank employees. All data will be managed through SPSS v23. By understanding how system design impacts cognitive processes such as attention, memory, and problem-solving, this research aims to provide practical recommendations for designing work systems that minimise cognitive strain while promoting high levels of engagement and productivity. Additionally, it will explore how different work systems affect behaviour in both high-performance and low-performance settings, providing a clear connection between system design and health outcomes.",3,108 | Cobb Galleria Centre,Session 6: Work Systems and More,191
638,8808.0,"Unmasking Hidden Influences: How Biases, Heuristics, and Stress Shape Cost Management in Nigerian Banks",Academician,"This study explores the psychological factors affecting cost management decisions in the Nigerian banking sector, focusing on biases, heuristics, and stress. Key biases, such as overconfidence, anchoring, and loss aversion, can distort financial planning, while heuristics, often used under time pressure, may lead to errors. Stress, a prevalent factor in high-demand environments, further impairs problem-solving and decision-making processes. Data will be collected from 100 employees in Nigerian banks through surveys and scenario-based tasks simulating cost management challenges. Analysed factors include the frequency and impact of biases, the reliance on heuristics in cost-related decisions, and the influence of stress on resource allocation and financial outcomes. The study aims to identify psychological barriers that hinder efficient cost management and provide actionable strategies for mitigating these effects. Insights will guide the development of decision-making frameworks and training programs to reduce cognitive distortions, enhance financial planning, and improve cost efficiency. This research offers practical solutions to optimise cost management processes in Nigeria’s dynamic banking sector.",4,108 | Cobb Galleria Centre,Session 6: Work Systems and More,191
639,6298.0,Analysis of Adaptive Problem Solving in a Multiplayer Virtual Reality Manufacturing System,Academician,"The dynamic nature of modern manufacturing systems necessitates adaptive problem-solving approaches that respond to rapid changes and complex challenges to improve productivity. This research explores multiplayer virtual reality (VR) environments for adaptive problem-solving in manufacturing settings. Existing VR studies often focus on specific organizational contexts, isolating technological or social factors rather than integrating both. This approach limits our understanding of VR’s potential to support adaptive problem-solving in diverse, realistic environments. We address this gap by examining how VR allows users to dynamically respond to varying task complexities and promote collaborative problem-solving across industries. The study investigates how task transitions affect physiological and cognitive engagement of participants during task execution. In a simulated production environment, ten teams of three participants were asked to design and assemble toy cars. Participant tasks were distinguished by having high, medium, or low complexity. Some of these tasks include ordering from a workstation, connecting building components, or teleporting around the production environment. Throughout the simulation experiment, electrodermal activity (EDA) data was collected to assess stress and engagement during task execution. By analyzing physiological responses, this research investigated correlations between task complexity and adaptive problem-solving capacity, as well as patterns in task transitions. The findings contribute to understanding how VR environments can enhance team performance, adaptive thinking, and efficient decision-making in manufacturing. This research highlights VR’s potential as a tool for advancing collaborative problem-solving in complex, real-world production environments.",1,108 | Cobb Galleria Centre,Session 2: Manufacturing and Industrial Engineering,192
640,6986.0,Measurement-driven design thinking:  Industrial Engineers and UX Strategy,Practitioner,"For years, parallels between industrial engineering and user experience (UX) strategy have silently existed, especially when applying human-centered design thinking in product development. UX design teams have struggled to measure their operational impact directly, beyond user satisfaction, usability studies, or conversion rates. For UX designers, some analysis techniques used within industrial engineering are either unknown or perceived as too technical and mathematical to be included in a creative role. However, if we view experience design as simply designing a workflow, a career in user experience (UX) design, traditionally considered unsuitable for industrial engineers, suddenly transforms into a viable discipline for bridging the gap in work design expertise. To better engineer UX strategy models that demonstrate influence on overarching organizational business model metrics, merging some industrial engineering concepts into the design thinking process can aid leadership in recognizing clear ties and impacts. Techniques associated with measuring workflow productivity and activity-based costing, for example, can illustrate the operational expense savings that UX solutions provide and facilitate a more informed and connected UX strategy model that has yet to be previously available to design practitioners. This study demonstrates how altering the design thinking model with industrial engineering elements can transform the UX value proposition and enrich career opportunities and the body of knowledge for industrial engineering in user experience design and research.",2,108 | Cobb Galleria Centre,Session 2: Manufacturing and Industrial Engineering,192
641,8506.0,Visualizing and Improving Assembly Task Using Motion Capture Analysis,Academician,"In recently, labor shortage is serious problem in whole world, because the birthrate is declining, and the population is aging rapidly. Additionally, complex tasks that are difficult to automate or mechanize require workers to become proficient and skilled technicians to trace on their skills. There are studies that use motion capture skill to analyze the motion of workers to analyze their skills and propose improvement movement. Motion capture is a technology that digitally records the movements of physical workers and objects, and can simultaneously acquire time and three-dimensional coordinate data. This technology can be used to analyze work subjects which may lead to the improvement and training of tasks for wokers to Improve productivity for industry. In this study, we analyze workers and propose improvement plans to increase work efficiency by visualizing the motion of assembly work (nut tightening) using motion capture.",3,108 | Cobb Galleria Centre,Session 2: Manufacturing and Industrial Engineering,192
642,6126.0,Lean Manufacturing Implementation for Waste Reduction and Efficiency Improvement in Textile Production: A Case Study of Polanco Fashion International,Academician,"This project proposes a Lean Manufacturing implementation framework for Polanco Fashion International, a textile manufacturer in La Vega, Dominican Republic, facing challenges with production inefficiencies, high defect rates, and substantial waste. Currently, waste levels exceed 20%, with quality checks lacking standardized processes, resulting in inconsistent product quality and delivery delays. The proposed Lean Manufacturing approach integrates key tools such as Value Stream Mapping (VSM), 5S, and visual control, aiming to streamline production workflows and minimize waste. A Value Stream Map revealed a total lead time of 27.5 days and process efficiency issues, suggesting a 26% lead time reduction potential through targeted improvements. The 5S methodology addresses workspace organization and cleanliness in the cutting, sorting, and sewing areas, projecting a 15% productivity increase by reducing downtime due to disorganization. Visual control tools, such as pre-production checklists, are introduced to ensure material availability, reducing idle time and rework due to incomplete components. With an initial investment of $13,168, this Lean strategy is expected to reduce waste by 20%, enhance process flow, and cut rework costs, achieving a 12% overall reduction in operational expenses. Benefits are anticipated within six months of implementation, with a projected net gain of approximately $2,991,121 over a three-year horizon, establishing a feasible and replicable model for quality and efficiency in the Dominican Republic's textile sector.",4,108 | Cobb Galleria Centre,Session 2: Manufacturing and Industrial Engineering,192
643,5315.0,Predicting Cognitive Workload of Teleoperators in Collaborative Remote Work: Leveraging Physiological Metrics,,"Teleoperation of robots offers substantial advantages over in-person collaboration, particularly in terms of safety, accessibility, and flexibility. However, it can impose a high cognitive workload on teleoperators, often due to factors like communication delays, the complexity of managing remote control systems, and the need to process multiple information streams simultaneously. Accurately predicting cognitive workload for teleoperators is crucial for dynamically adjusting task allocations and delivering timely interventions to ensure optimal workloads in collaborative environments. Despite its importance, research on cognitive workload in teleoperators working with onsite workers remains underexplored. We conducted an experiment with 32 participants (16 onsite and 16 teleoperators; gender-balanced) performing wire assembly tasks with and without a teleoperated robot arm under different cognitive workload levels. Participants were paired and randomly assigned to either an onsite or a teleoperator role in a between-subjects design for Human-Human (HH) or teleoperator-Robot-Human (tRH) collaboration. Teleoperators’ cognitive workload were assessed using subjective NASA-TLX scores and physiological metrics, including heart rate, skin temperature, and skin conductance levels. These metrics were used to predict subjective NASA-TLX scores through machine learning regression models. Of the three regression models evaluated (Multiple Linear Regression, Random Forest, Support Vector Machine), the Random Forest model demonstrated the best performance, with an R² score of 0.80 and a mean absolute error of 4.59 in estimating perceived cognitive workload of teleoperators. These findings suggest that physiological metrics can be effectively used to predict teleoperators' cognitive workload, aiding the development of future adaptive teleoperation interfaces to reduce high workload.",1,108 | Cobb Galleria Centre,Session 1: Cognitive Workload and Efficiency,193
644,6053.0,Development of a Surrogate Model for the Assessment of Crew Workload in Emergency Medical Services,,"Workload in Emergency Medical Services (EMS) has been historically measured by metrics such as call volume or Unit Hour Utilization (UHU). This paper investigates a physics-based surrogate model to calculate a measure of an EMS crew member’s utilization more accurately, considering indirect work tasks, such as documentation and shift start activities. Other surrogate models, Kriging, KPLS and RBF, were explored. The true measure of utilization was based on a trace-based simulation built with real-world data and estimates for the duration of indirect work. The estimates based on the proposed surrogate model, along with the conventional measures of workload, were compared to the simulation outputs for a years’ worth of data using the root mean square error (RSME), the symmetric mean absolute percent error (sMAPE), and Pearson correlation estimates. The best performing model included estimates of the average shift start time, averages of driving to post time, documentation time, and call response time. This best-performing model was further analyzed using uncertainty calculation and sensitivity analysis. General recommendations were given for use in other EMS systems.",2,108 | Cobb Galleria Centre,Session 1: Cognitive Workload and Efficiency,193
645,6297.0,Evaluating Team Efficiency in Manufacturing Industry Using Data Envelopment Analysis,,"The increasing global demand for sophisticated designs and specifications has forced manufacturing industries to depend on the efficiency and delivery performance of their employees who work in factories. Analyzing the efficiency of factory teams is essential to enable continuous improvement efforts and necessary interventions. This research employs Data Envelopment Analysis (DEA) to analyze team efficiency of manufacturing workers. The research utilizes a case study from a garments industry where various teams were evaluated based on input factors such as target productivity, standard minutes value, incentive, number of workers in the team, and overtime, and output factors such as production rate and work in process. Both constant return to scale (CRS) and variable returns to scale (VRS) DEAEDA models were utilized. Overall efficiency, technical efficiency, and scale efficiency were determined, and the results indicated that there is a discrepancy between overall efficiency and technical efficiency that indicates the teams are not working in an optimal-scale working environment. This study provides valuable insights into team efficiency assessment and the impact of various variables on team performance not only in manufacturing industries but also in other areas including healthcare, education, and service industries. Therefore, necessary interventions can be taken to facilitate improvements and achieve better efficiency.",3,108 | Cobb Galleria Centre,Session 1: Cognitive Workload and Efficiency,193
646,6675.0,Validating Virtual Reality Surgical Training Through Modeling Learning Processes with Hidden Markov Models for Workforce Development,Academician,"Training is a constant source of improvement for safe and reliable workforce development. One key area of focus is medical training, where the challenge is to safely and effectively improve training methods without compromising patient health. Virtual reality (VR) presents a promising solution, particularly for surgical training. However, two research questions arise: how can we accurately quantify and measure the growth of trainees' learning processes? How can we address the heightened concerns of validity in VR training, especially when assessing how trainees are learning without the use of a physical system? The authors propose a reinforcement learning framework, utilizing Hidden Markov Models (HMM), to provide insights into the learning growth of trainees. This model incorporates five states: novice, advanced beginner, competent, proficient, and expert. The observed states are derived from a combination of performance scores and completion times during VR training. Moreover, by constructing separate models for VR-based and real-world learning data, we investigate the validity of the VR-based training program and demonstrate its effectiveness in medical training. As a result, the proposed research is expected to provide valuable insights into the VR learning process and establish its validity as a training model, while also laying the groundwork for developing optimal guidance strategies in future VR training programs.",4,108 | Cobb Galleria Centre,Session 1: Cognitive Workload and Efficiency,193
647,5136.0,A Production Planning System Tailored for Small Businesses,Practitioner,"Small businesses face unique challenges in understanding how to align production with growing customer demand, often operating with limited resources and infrastructure. As these businesses grow, finding a balance with efficient production planning and accurate demand forecasting are critical to maintain efficiency and profitability at the early stages. The need to focus on small business is founded by the historical statistics of the U.S. Bureau of Labor Statistics that claims only ~20% of small businesses survive beyond the first 15 years attributed to many different factors. The study highlights how growing small businesses can avoid costly disruptions such as high-risk investments and inefficient resource allocations. This research develops methods that a small business can employ to address these aforementioned issues. This study focused on creating practical, scalable production planning guidelines specifically tailored to the needs of growing small businesses. Through examination of current and existing production planning models, this study simplified and adapted them to incorporate the unique aspects that define small business operations. This research will present the adapted production planning model and guidelines that any small business can use.",1,111 | Cobb Galleria Centre,Operations Management,194
648,6545.0,Enhancing Warehouse Efficiency through Data-Driven Inventory Management in High-Demand Distribution,Academician,"This project presents a comprehensive redesign of inventory and warehouse management for a food and personal care product distribution company facing inefficiencies that result in annual losses exceeding $50,000 due to delayed dispatches and suboptimal space utilization. The current system lacks structured inventory placement and demand forecasting, contributing to an average retrieval time of 20 minutes per order and frequent stock misplacements, directly impacting customer satisfaction and increasing operational costs. To address these issues, the proposed solution includes a demand-based product classification system, a 15% increase in storage capacity through optimized layouts, and an enhanced workflow to reduce retrieval times by 40%. Detailed time-motion studies and data-driven analysis pinpointed bottlenecks, and the subsequent reorganization aims to reduce non-value-adding movements by 30%. Using key performance indicators (KPIs), such as retrieval time, order accuracy, and space utilization, the project will track performance and adapt improvements continuously. The feasibility analysis projects an ROI of 18 months, with an estimated $20,000 in annual cost savings and a 25% increase in order processing speed. Furthermore, the system's scalability enables future expansion without significant additional investments. This framework establishes a replicable model for inventory optimization in high-demand, space-constrained environments, showcasing how strategic inventory and warehouse management improvements can yield substantial operational and financial benefits.",2,111 | Cobb Galleria Centre,Operations Management,194
649,8852.0,A Culture and Change Management Framework to Embrace Model-Based Systems Engineering Practices,Practitioner,"Model-Based Systems Engineering (MBSE) is a relatively recent approach to model systems within the systems design lifecycle. MBSE draws from the Unified Modeling Language (UML) that was used for software development. The SySML (Systems Modeling) language using nine graphical diagrams that enable integration within and across the models for easier review, improved efficiency, and consistency of modeling. There are many challenges that are faced when moving from a document-based based systems engineering practice and culture to one that embraces MBSE. This presentation will discuss the challenges and a framework that addresses both cultural and structural components of moving to MBSE in designing systems.",1,117 | Cobb Galleria Centre,Change Management,195
650,5050.0,Analyzing Outliers in the Organizational Change Capability Scale for Public Organizations,Practitioner,"To predict the success rate of change programs in public organizations, our previous research developed a scientific validated scale. During this validation process, the initial scale of 150 items was condensed to 77 items distributed across 15 components through the use of Principal Component Analysis (PCA), drawing on data from 333 entries. The PCA process excluded items that scored either consistently high or low across most entries, as these items typically had lower correlations and, thus, less explanatory power. However, with this research we argue that these excluded outliers—items that score consistently low or high—offer valuable insights into an organization's maturity regarding change capability. The objective of this research is to quantitatively identify these outliers and determine which items to retain. The items that consistently score low often reflect the most challenging aspects of organizational change; organizations that excel in these areas demonstrate exceptional change capability, which are often top performers, as supported by the data from our sample. Conversely, the typically high-scoring items represent rather foundational aspects of change capability; organizations that do not score well on these may struggle with broader change efforts. This paper concludes that while PCA is essential for developing a scientifically validated scale, the exclusion of outliers renders the scale blind to the real challenges of change that everyone is grappling with and also underestimates foundational elements essential for those lagging behind. These findings underscore the importance of including such items in organizational assessments to gain a fuller understanding of change capability.",2,117 | Cobb Galleria Centre,Change Management,195
651,6176.0,Safety-II approach toward Organizational Resilience in Higher Education Institutions,Academician,"The COVID-19 public pandemic created unprecedented disruption to institutions of higher education (IHEs) worldwide. Subsequently, the need for Organizational Resilience for IHEs has significantly increased. Despite such escalated needs, knowledge regarding organizational resilience in IHEs is severely lacking. To address this knowledge gap, this paper aims to understand Organizational Resilience in IHEs, especially using safety lenses: Safety-I and Safety-II. While Safety-I focuses on finding things that went wrong and taking corrective actions after an undesirable event has occurred, Safety-II considers both things that went wrong and right and aims to leverage lessons learned from accidents and daily practices to prevent future incidents. In line with the Safety-II perspective, the current study discusses Organizational Resilience as organizational behavior exhibited by an organization or ""what the organization does"", instead of characteristics that a system (or organization) “has”. This approach may help understand the actions or strategies taken by IHEs that show better organizational resilience than others. Further, considering this approach can lead to future research using tools derived from Safety-II to analyze the potential organizational resilience in IHEs such as Resilience Analysis Grid (RAG), as well as how to approach disturbances (e.g., looking at what went right helps in understanding best strategies for disturbances), how probable it is to sense a disturbance (i.e., event probability and ease of perception to manage it), and how the learning process can be approached from the Safety-II perspective (i.e., examining what went right instead of only focusing on what went wrong_).",1,111 | Cobb Galleria Centre,Safety Training and Organizational Resilience,196
652,6844.0,ASSESSING THE USABILITY OF A VIRTUAL SIMULATION GAME FOR EVALUATING PEER EFFECTS ON WORKPLACE SAFETY BEHAVIORS,Practitioner,"Evaluating and managing peer effects on workplace safety behaviors is essential, as unaddressed peer effects can foster the spread of unsafe behaviors, leading to accidents and a poor safety culture. Despite this, research in peer effects on safety behaviors remains limited due to the complexities of modeling social interactions and the inherent risks of testing safety interventions. Virtual simulation games offer a promising solution by providing a controlled and immersive environment for studying these behaviors. However, assessing the usability of such simulations is critical to ensure they accurately represent workplace scenarios and effectively engage participants, which is necessary for reliable data collection and meaningful analysis. This study aimed to evaluate the usability of a theory-driven virtual simulation game designed to study peer effects on workplace safety behaviors. The game was developed through a systematic, three-phase process: (a) modeling peer effects using information transmission and Festinger's social comparison theory, (b) meeting Manski’s criteria for identifying endogenous peer effects, (c) modeling safety behaviors based on workplace safety principles, and (d) designing the virtual simulation game grounded in these theoretical frameworks.",2,111 | Cobb Galleria Centre,Safety Training and Organizational Resilience,196
653,8931.0,Exploring VR and Entrepreneurial Mindset Integration for Effective Safety Training,Academician,"Studies highlight Virtual Reality (VR) as a powerful tool for safety training, offering immersive and interactive learning experiences that enhance engagement. However, a gap remains in understanding how theoretical frameworks align with training design and evaluation. This study explores two key areas: the application of theoretical frameworks in VR-based digital twins for safety training and the integration of Entrepreneurially Minded Learning (EML) principles into safety training design. This study emphasizes designing pedagogies that foster curiosity, connection, opportunity identification, and impact recognition, key components for fostering a mindset aligned with safety-oriented behavior. Incorporating an entrepreneurial mindset into VR-based safety training enhances its relevance and impact, aligning it more effectively with real-world applications. A case study is presented in which a VR-based digital twin of a machine shop is developed to enhance safety training, as compared to traditional passive instruction methods. In the traditional approach, trainees complete a safety quiz after watching a presentation, granting them access to the campus shop. The proposed digital twin enables students to explore a virtual shop, engage with safety equipment, and practice tasks in a risk-free environment, with assessments tracking their progress. The integration of theoretical frameworks with practical applications forms a foundation for enhancing VR-based safety training programs, making them both more engaging and effective. The study further discusses the challenges and opportunities involved in implementing such training and expanding VR-based safety initiatives within educational settings.",3,111 | Cobb Galleria Centre,Safety Training and Organizational Resilience,196
654,6195.0,Organizational Behaviors Impactful to Financial Resources,Practitioner,"In complex organizations, understanding the interplay between organizational behaviors and financial resources is crucial for project or program success. Various behaviors may impact financial resources, budgets, and cost estimates within an organization, in turn affecting how the budget may be allocated and how management may approach a project or program moving forward. Using a literature review of projects and programs associated with complex organizations, this paper identified behaviors that can potentially be impactful, either directly or indirectly, to the finances within an organization. These behaviors included communication behaviors, teamwork efforts, and working relationships among team members; clarity of individual roles and responsibilities on a project or program; clarity of organizational goals for a project or program; efficiency of planning and scheduling behaviors; and the overall culture of an organization. These behaviors may have direct impacts on the costs of operations or may indirectly influence another factor which in turn can impact the financial resources. By understanding behaviors that may negatively impact the financial aspects of an organization, management may evaluate current practices to identify areas for improvement with these specific behaviors in mind.",1,117 | Cobb Galleria Centre,"AI, Behavior, and Organizational Effectiveness",197
655,6451.0,Making hiring decisions with gender–biased AI recommendations,Academician,"The hiring process is crucial for organizational effectiveness but often encounters gender bias, particularly favoring men for roles perceived as traditionally masculine. This study investigates the interaction between Artificial Intelligence (AI) recommendations and human biases in recruitment. We examined how high-score and low-score AI recommendations, along with gender bias, affect candidate evaluations, focusing on perceptions of competence and likability. In a study with 600 participants, individuals assumed the role of hiring managers evaluating a candidate for a Chief Electrical Engineer position. The study employed a 3 (AI recommendation: none, high-score AI, low-score AI) × 2 (candidate gender: male, female) between-subjects design. Participants rated the candidate's competence and likability and completed Modern Sexism Scale. Results indicated that AI recommendations significantly influenced ratings: high-score AI recommendations increased, and low-score AI recommendations decreased competence and likability scores. Modern sexism negatively impacted evaluations of the female candidate; higher sexism scores correlated with lower ratings for the female candidate. Notably, the increase in competence ratings from the No AI condition to the high-score AI condition was significant for the male candidate, suggesting that the female candidate did not receive the same boost in competence evaluation from the high-score AI recommendation compared to the No AI condition. These findings highlight the interplay between AI recommendations and human biases in hiring decisions. The study shows the need for careful integration of AI in recruitment processes to mitigate biases and promote equitable hiring practices.",2,117 | Cobb Galleria Centre,"AI, Behavior, and Organizational Effectiveness",197
656,6159.0,ADVANCING STRATEGIC PLANNING IN HIGHER EDUCATION THROUGH HYBRID HUMAN-AI INTELLIGENCE,Practitioner,"As a new generation of students enters higher education, universities need innovative strategic plans in order to ensure alignment with the ever-changing environment of higher education. At the same time, the benefits and drawbacks of using artificial intelligence (AI) are increasingly being understood. The benefit of AI is a more efficient way to condense and identify problems based on data, but it is perceived to lack the emotional intelligence required in strategic planning. This study assessed the efficiency of implementing a hybrid approach to strategic planning by combining human expertise with AI capabilities. The focus of this project examines the trajectory of higher education to determine how universities can position themselves over the next five years. This takes into consideration the current image and procedures of the university to align the future with their mission, vision, and goals. A strategic planning case study was conducted with a medicine and health sciences university to examine how AI can be incorporated in this process. A detailed literature review on strategic planning in different industries was required to create a manual strategic plan using human capability. Then, AI was fed the same information and formulated its own strategic plan. Both strategic plans were then compared to the university’s current plan to develop the best process for hybrid strategic planning. Overall, the project showed how AI can be a new tool used in advancing higher education’s direction and future goals in a more efficient and effective manner.",3,117 | Cobb Galleria Centre,"AI, Behavior, and Organizational Effectiveness",197
657,6369.0,Enhancing Customers' Satisfaction in School Bus Service Via Goal Programming and Factor Analysis,Practitioner,"This study presents an approach for optimizing transportation services by integrating customer satisfaction into the decision-making process. The study utilizes the regression tree to identify key factors influencing user satisfaction. The central methodology employed is weighted goal programming, which allows for an adequate and flexible balancing of multiple objectives, improving service quality, and aligning with operational constraints. Critical performance indicators such as punctuality, reliability, seat availability, and safety are analyzed using surveys and operational metrics data. The model provides a decision-making framework for transportation service managers with actionable insights, offering optimized solutions that prioritize user satisfaction while maintaining operational efficiency and enhancing service quality subject to budgetary and operational constraints.",1,117 | Cobb Galleria Centre,Decision Analytics for Service and Supply Chain Optimization,198
658,5402.0,A Hybrid Approach Using AHP And TOPSIS Techniques to Improve Supplier Selection Scheme for Packaging Stage in Pharmaceutical Industries,Academician,"Supplier selection is crucial for maintaining product quality, ensuring safety, and supporting an effective and reliable supply chain. Choosing the best supplier is a key decision for any organization, not only in terms of providing optimal resources, goods, or services at an acceptable cost, but also in improving environmental performance. This study addresses the limitations of traditional supplier evaluation methods by incorporating both subjective and objective criteria. It proposes a hybrid approach that combines the Analytic Hierarchy Process (AHP) and the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) to enhance supplier selection for packaging materials in pharmaceutical industries. The proposed methodology is applied to a real application involving packaging material suppliers, utilizing 15 pairwise comparison matrices of main and sub-criteria. These criteria include quality and safety, delivery, cost/facility, and responsiveness. AHP is employed to determine the importance of evaluation criteria, while TOPSIS ranks suppliers based on their performances. By integrating these methods, the study aims to streamline supplier selection, contributing to a more efficient, effective, and reliable supply chain. The research highlights its contribution in reducing purchasing risks, maximizing value, meeting quality standards, and prioritizing sustainability.",2,117 | Cobb Galleria Centre,Decision Analytics for Service and Supply Chain Optimization,198
659,5350.0,Stakeholder Management Through Problem Structuring Lens: An Investigation of Ride-sharing Efficacy in Bangladesh,,"Ride-sharing services have transformed the mobility landscape in Bangladesh, capturing a substantial 23 percent share of the transportation sector. This transportation innovation has spawned a market valued at over $200 million. The country's market is populated by over ten platforms, with Uber and Pathao being the predominant services capturing most of the market share. However, the rise of ""contract riding,"" where drivers connect directly with riders outside of the app's purview, marks a significant bypassing of established platform rules. Reports show that many drivers are demotivated by concerns over commission rates, payment methods, and cost-effectiveness. As a result, the non-compliance with the platform regulations incidents is growing. The escalating tensions arising from systemic issues in the gig economy pose a significant threat to the stability and longevity of the ride-sharing industry. The complex web of issues within this industry is affecting riders, drivers, platforms, and the government. The multifaceted nature (i.e., multiple stakeholders) of this complex problem points to the need for a deep dive into the concerns and limitations of all parties. This study applies a Problem Structuring Method (PSM) to help untangle these issues by offering a structured approach to dissect these challenges to facilitate effective options. To gain a better understanding of the issue of non-compliance with platform regulations, a PSM will be utilized to systematically address these complexities and enhance understanding among stakeholders. The outcome of this study could be the key to improving the effectiveness of ride-sharing in Bangladesh.",3,117 | Cobb Galleria Centre,Decision Analytics for Service and Supply Chain Optimization,198
660,6959.0,A New Approach to Study the Influence of Project Contracts on Project Scheduling,,"The importance of Project Contract Management (PCM) has significantly increased recently due to the emergence of strategies like outsourcing and partnering. It should be paid attention that every activity of a project and therefore the whole project, does need at least one contract, either in a written form or in an oral form. So, it seems that there is an implicit simplifying assumption in existing project scheduling techniques that every activity of a project is ready to start ignoring whether its required contracts are ready or not. Therefore, the purpose of this paper is to present a framework for revision in project scheduling, considering the project contracts. In the framework, multi-criteria decision-making is applied as a tool to prioritize the contracts of a project. PCM has been studied from different perspectives, but very little attention has been paid to considering the required contract(s) of an activity, as a potential precedence for that activity; so that a higher accuracy for project scheduling can be gained. So, this issue can be regarded as the innovation aspect of the current paper. The main finding of this paper is that utilizing the proposed framework in project management, especially the impact of contracting times on project scheduling, will help project managers in better and more effective planning and controlling of projects.",1,111 | Cobb Galleria Centre,Project Management,199
661,6362.0,Strategic Resource Allocation in Construction through Critical Path Analysis and LP Programming,Practitioner,"In construction project management, the need to optimize resource allocation while managing cost and schedule is critical for project success. This study examines an integrated approach using the Critical Path Method (CPM) alongside Linear Programming (LP) to improve traditional project scheduling. Although CPM effectively establishes project timelines by identifying the sequence of critical tasks, it often lacks flexibility to account for sudden shifts in project dynamics under unpredictable conditions. Integrating LP enhances the standard CPM approach, allowing for a more flexible and cost-efficient scheduling that adapts to changing conditions and maintains profitability. The research used a residential construction project as a case study, analyzing a 5,000-square-foot, three-story building to examine both material and labor requirements. By employing a hybrid CPM-LP model, the study effectively identifies critical paths, optimizes resource allocations, and evaluates cost implications across various scenarios. The results highlight the potential of the linear programming (LP) model to significantly minimize costs while ensuring adherence to project timelines. Findings indicate that the combined approach can reduce project duration and expenses, offering construction managers an adaptable framework to optimize resource allocation. This approach not only promises to enhance current practices but also contributes to the theoretical advancement in construction project management by offering a scalable model for varying project sizes and complexities.",2,111 | Cobb Galleria Centre,Project Management,199
662,6860.0,A Novel Approach to In-Process Project Estimates to Completion,Practitioner,"Project managers are tasked with delivering the right project at the right time and cost. To meet the financial targets of the overall project, project expense tracking is done throughout the project’s execution. This gives the project manager an idea of how the project’s expenses line up with the expected costs for the tasks completed to date. The impacts of stopping a project to investigate a suspected cost overrun can be as impactful as completing it over budget. The methodologies currently in use for tracking the cost performance of a project also project the cost at completion based on certain assumptions related to how the current budget performance relates to the financial execution for the rest of the project. These assumptions range from projecting no impact on the remaining work’s financial performance, i.e., the rest of the work costs exactly what the planned expenses were, to calculating a probabilistic cost at completion using cost performance to date as the mean for a Gaussian correlation to relate to future financial performance. This paper challenges these assumptions by analyzing cases where the future cost performance is probabilistically calculated without assuming that the correlation to past financial performance is necessarily normal. The results of this research show that the projected final budget performance of a project can change by challenging the assumption that costing errors are either deterministic or always Gaussian. This opens interesting areas of research that can provide better tools for project managers as they execute large, complex projects in the field.",3,111 | Cobb Galleria Centre,Project Management,199
663,6295.0,Advancing Smart Policing Management: A Data-Driven Framework for Enhanced Police Operational Decision-Making,Academician,"The advancement of smart policing management is essential to today’s society, as data-driven methods and advanced technologies can be leveraged to enhance public safety and deter criminal activity. Today, police operations across the U.S. face ongoing challenges due to unpredictable crime rates leading to resource allocation inefficiencies. A large body of related literature focuses on predictive analytics for crime forecasting through the development of increasingly advanced statistical and machine learning models. However, these crime predictions often lack direct application into police operational decisions, highlighting a gap in prescriptive analytics. To address this research gap, we aim to develop a data-driven framework that integrates predictive and prescriptive analytics to support police operational decisions. Our approach begins with the collection of diverse data from multiple sources to build a comprehensive dataset for crime analysis. We then design a predictive model that forecasts crime rates, incorporating spatiotemporal dependencies by leveraging advanced machine learning techniques. Building on this, we develop an optimization model to improve response times by strategically allocating police resources. Finally, we validate our framework with a case study in the city of Seattle, demonstrating the applicability of our methodological framework. This framework can support informed decision-making in smart policing management by optimizing the allocation of police resources and enhancing operational efficiency.",1,111 | Cobb Galleria Centre,Decision-Making,200
664,5200.0,Assessing Decision Modeling Under Risk in Highly Hazardous Industries,Academician,"Every day, companies follow policies and procedures to comply with laws and regulations in the environment they are engaged in. However, industries that deal with the production of dangerous materials, the transportation challenges of hazardous products, or the operations in highly regulated environments, must make decisions every day. Many of these decisions include considerations associated with risk to the production, operations, or delivery of products or services. To make effective use of resources, operate as safely as possible, and comply with all the regulations imposed on such environments, managers and other executives rely on decision support tools that must account for risk in the decisions being made. This research will investigate current risk management and regulatory standards by evaluating the different mechanisms or methods that companies use to make decisions under risk and uncertainty in highly hazardous industries. Because of the business environment, these decisions can have significant consequences and are often done in highly regulated environments and many companies seek to make decisions with risk being as low as reasonably achievable. The emphasis will not be on the objective function, but on the analysis of the constricted solution space that regulations and risk impose on the ability to make decisions. Utilizing AI-assisted tools to help parse policy and classify impacts on risk, the expectation is to propose a framework that identifies effective constrictive and, sometimes, contradicting regulatory and risk challenges in highly hazardous industries.",2,111 | Cobb Galleria Centre,Decision-Making,200
665,6458.0,Model Management Practices,Practitioner,"Organizations require modeling tools that can faciliate strategic, operational, and tactical decision making related to the planning and execution of operating missions. Operations engineering processes are supported by analysts that build, execute, support, and maintain predictive and prescriptive models. Because of the time and cost associated with building and using operations engineering and data science models, work flow process requirements related to the management of models has emerged as a key organizational imperitive. Specifically, how should organizations manage their models as assets over time. This presentation provides background into model management practices, software support tools for the management of models, and the functionality that must available for organizations to manage their models.",3,111 | Cobb Galleria Centre,Decision-Making,200
666,6218.0,Data-Driven Sustainability: Leveraging Industry 4.0 and LSS for Operational Excellence,Academician,"In today’s rapidly evolving industrial landscape, the pursuit of sustainable operations has become essential for organizational resilience and stability. This research employs a systematic literature review to examine existing studies on the integration of Industry 4.0 (I4.0) technologies and Lean Six Sigma (LSS) methodologies, with a focus on how I4.0 capabilities, such as the Internet of Things (IoT), big data analytics, artificial intelligence, and smart sensors, can enable data-driven sustainability within the LSS framework. The findings from this literature review will inform the development of a proposed framework that integrates I4.0 methods into LSS to support real-time, data-informed decision-making, optimizing processes for sustainable outcomes. The proposed framework offers organizations a structured approach to leveraging continuous improvements in economic, environmental, and social dimensions.",1,111 | Cobb Galleria Centre,Data-Driven Systems for Quality and Operational Excellence,201
667,6469.0,Leveraging Problem Structuring Methods for Quality Excellence: Structured Problem-Solving in Engineering Management,Academician,"Quality assurance is critical to high product standards, reducing operational defects, and ensuring customer satisfaction. Quality management issues often arise from multifaceted problems involving people, processes, technology, and the environment. In addition, the quality management landscape has become increasingly complex. Some traditional quality tools, such as Ishikawa (Fishbone) diagram, have been widely used for root cause analysis. The Fishbone diagram's strength lies in its ability to identify potential causes of quality issues. It organizes these causes into categories, helping clarify where problems might originate. While effective, the Fishbone Diagram can sometimes fall short when complex interactions or causes are intertwined, especially in cases where multiple stakeholders have varying insights or opinions about the causes. The Fishbone Diagram can oversimplify complex quality challenges by limiting the exploration of interdependencies and cross-functional perspectives due to their linear structures. Problem Structuring Methods (PSM) provide a collaborative problem-solving approach that captures diverse perspectives and highlights relationships between issues. A PSM can potentially enhance the Fishbone Diagram’s effectiveness by facilitating a deeper understanding of problem dynamics, encouraging stakeholder engagement, and revealing interconnected causes. Through a systematic literature review, this paper investigates the potential of Problem Structuring Methods (PSMs) by exploring how PSMs can address quality management challenges (e.g., supporting stakeholder alignment, interdisciplinary collaboration, and continuous improvement initiatives). The expected results should demonstrate the potential of PSMs in enhancing traditional quality management practices.",2,111 | Cobb Galleria Centre,Data-Driven Systems for Quality and Operational Excellence,201
668,5376.0,"Communication, Coordination, and Collaboration: the 3 Cs Towards a Better Strategic Implementation for Product Development (PD).",Practitioner,"This paper describes a 3Cs approach for technical and non-technical innovators. The goal is to offer a systematic approach to streamlining the integration of technical and non-technical stakeholders throughout the PD process. This lifecycle approach model starts from customer requirements all the way to customer support. This 12-step approach considers processes, resources, tools and techniques, and integration to enable success at each step. Some of the reasons attributing to the low rate of product development success are due to the inherent complexities of PD such as misalignment of the product with marketing goals and customer signals. Additionally, inability to manage the multidisciplinary nature of PD, challenges of system integration, supply chain management failures, and many other technical factors are also culprits. Beyond technical challenges, there are interfaces with non-technical functions such as marketing, finance, supplying chain, and operations to consider. In most cases, the inability to properly implement and execute the 3Cs is a common theme. Inability to properly manage the 3Cs can lead to less than desirable results and can significantly impact the performance of the team and the end-product. There is nothing more devastating and demoralizing than to realize all the hard work and efforts is wasted and the end-product is not what the customer wants Learners and practitioners of engineering management, project management, business management, product management; and other Technical and Non-technical professionals such as Product Managers, Product Development Managers, Program Managers, Project Managers, System Engineers, and others may benefit from find this integrated model.",1,117 | Cobb Galleria Centre,Product Development Strategies,202
669,5604.0,Integrating Agile and Systems V Models for Complex New Product Development Initiatives: A Hybrid Framework for Modern Day Product Development,Practitioner,"Modern engineering projects, characterized by increasing complexity, challenge traditional methodologies like the Waterfall, Agile and Systems V models. While these methods are systematic and methodical, they often lack the flexibility needed to accommodate the evolving demands and uncertainties of large-scale new product development innitiatives. Innovations in digital transformation, entrepreneurial eco-system design, driven by AI, machine learning, and IoT, have identified the need for adaptive, iterative development methods that can reduce product time-to-market durations. Agile methods are typically used for software development and can be difficult to apply in hardware-centric projects. This paper presents an Integrated Agile V-Model framework, which merges Agile’s iterative flexibility with the V model’s structured, gate-driven approach. This hybrid framework streamlines complex system management, risk mitigation, stakeholder engagement, and cross-functional collaboration throughout the project lifecycle. The framework could particularly be beneficial in sectors such as aerospace, defense, and advanced manufacturing, where adaptability and oversight are critical to project success.",2,117 | Cobb Galleria Centre,Product Development Strategies,202
670,6765.0,Analysis of Factors Influencing Product Launch: A System Dynamics Approach,Academician,"The ever-increasing need to satisfy customers and maintain organizational competitiveness are the main forces driving the launch of new products. Product launch refers to a firm’s planned and coordinated effort towards introducing a new product to the market. Being a complex process, firms usually invest significant efforts in product launch planning because it is intrinsic to every stage of the product development (PD) process, and it is typically influenced by internal and external factors which can be within or beyond a firm’s control. Firms typically seek to achieve product launch success and, as such, take these factors into consideration during product launch planning. However, the high rate of product launch failures reported across various sectors in recent times points at a gap between what firms know about these factors and what actually occurs in reality. In this study, we re-investigate the factors that affect product launch decisions and develop a framework that best captures the interplay between these factors. We begin by conducting an extensive search for literature relevant to product launch and then proceed to investigate the core focus of each study to identify the factors considered as we develop the framework. We then develop a system dynamics model to establish the causal relationship between these factors and analyze their impact on product launch success. The outcome of this study will provide practitioners with an extended knowledge of the factors influencing product launch decisions, especially as it pertains to various business environments.",3,117 | Cobb Galleria Centre,Product Development Strategies,202
671,9186.0,Acceptance of Advanced Technology: Systematic Review and Integrated Model,Academician,"Modern technologies are developing rapidly and many of the technologies once considered novelties are now considered effective tools for a variety of work contexts. As organizations aim to stay competitive in an increasingly dynamic and complex operational environments, they invest in adopting advanced technologies that are expected to improve organizational performance. However, many organizations report facing significant challenges during adoption and often do not obtain the intended benefits of these technologies. While there has been significant research in the area of technology acceptance, many of the common models used were developed prior to the rise of modern technologies and may not adequately apply to cases where the technology may not be well-understood or appreciated by industry professionals. This paper presents the results of a systematic literature review of acceptance of modern technologies in industry. The review was conducted across two platforms and the selected studies were evaluating using a thematic analysis to investigate definitions of acceptance, the types of technologies being adopted, factors that affect acceptance, and outcomes related to acceptance.",4,117 | Cobb Galleria Centre,Product Development Strategies,202
672,6943.0,In-Situ Porosity Detection in LPBF Using Machine Learning - Augmented Ultrasonic Emissions,Academician,"Laser powder bed fusion (LPBF) is a metal additive manufacturing process that selectively fuses regions of a distributed metal powder bed using a high-energy laser beam. While LPBF enables the manufacture of complex geometries and processing of challenging materials like titanium alloys, producing defect-free components remains a significant challenge due to the process's highly dynamic nature. Among the various defects in LPBF, porosities are particularly concerning as they directly compromise the component's mechanical properties and structural integrity. Their subsurface nature makes detection especially challenging, as conventional optical monitoring methods prove ineffective. While X-ray computed tomography (XRCT) offers capable post-production inspection, its technical complexity and high costs render it impractical for in-situ monitoring applications. This work investigates ultrasonic emissions as a promising method for in-situ porosity detection, building upon its established use as a non-destructive testing (NDT) technique for defect detection in metal components. However, conventional pulse-echo ultrasonic NDT cannot be directly implemented in LPBF processes, as it fails to provide sufficient information about the size and distribution of clustered porosities which is crucial for robust process control. Additionally, the necessity of adapting the mounting configuration of the transducer to avoid interference with the LPBF process increases wave attenuation and signal interference. To address these challenges, machine learning (ML) is employed to decode and characterize the reflected signals, enabling rapid, real-time detection of porosity formation. This method could also provide a reference for mitigation of porosity formation through in-situ process control.",1,102 | Cobb Galleria Centre,M&D Best Student Paper Award Finalists,203
673,8912.0,Vision -Based Jetting Stability Monitoring in Electrohydrodynamic Inkjet Printing by Machine Learning,Academician,"Electrohydrodynamic (EHD) printing’s unique jetting behavior enables high-resolution patterning for a variety of applications. The jetting behavior can be classified based on the formed shape of the Taylor cone. The quality of EHD printed patterns is influenced by these jetting modes, and one major bottleneck in EHD research is the inability to monitor and control the jetting during the printing process. By adjusting the printing parameters, five jetting modes are achieved: dripping, spindle, tilted, cone jetting, and multi-jetting mode. This study presents a machine vision and learning monitoring system where the profile of the jet is measured from high-speed camera images. Various supervised machine learning algorithms are used to classify the jetting modes, including neural networks, k-nearest neighbor, random forest, and gradient boosting. Each model is evaluated on accuracy and computational time. The Taylor cone angle and jetting frequency are extracted from the data and used in Statistical Process Control (SPC) charts to monitor the stability of the jetting during the printing process. If an out-of-control point is detected, the classification model predicts which unstable jetting mode is occurring and follows a parameter tuning algorithm until jetting is stable, i.e., within SPC limits. The control system was trained and tested using the same nozzle diameter, then it was validated using experiments that varied this parameter and the background noise to demonstrate repeatability. This closed-loop system alleviates the bottleneck of manually detecting uncontrolled jetting while printing and enables real-time parameter adjustments that enhance pattern quality and consistency.",2,102 | Cobb Galleria Centre,M&D Best Student Paper Award Finalists,203
674,9062.0,Machine Learning-Assisted Optimization of Ink Properties for Electrohydrodynamic (EHD) Inkjet Printing,Academician,"Electrohydrodynamic (EHD) inkjet printing is a high-resolution additive manufacturing technology that uses an electric field to drive the printing process. By applying a high voltage to the nozzle tip, the liquid forms a cone shape and ejects droplets significantly smaller than the nozzle diameter, achieving submicron resolution. Due to its excellent compatibility with a wide range of ink materials, EHD printing has found applications in printed electronics, biomedical devices, and optical systems. Printing stability and performance are highly influenced by ink properties. However, despite research exploring the relationship between ink properties and printing behavior, there is no standardized method for optimizing ink formulations. Most optimization relies on trial and error, making new ink development time-consuming and challenging. This paper presents a machine learning model that establishes a relationship between ink properties and EHD printing behavior. The effects of ink properties—including viscosity, surface tension, and density—on printing regimes and parameters were systematically studied and clarified. The model was trained using liquids with varied properties and validated with commercial inks, providing a robust framework for guiding ink optimization and improving EHD printing efficiency.",3,102 | Cobb Galleria Centre,M&D Best Student Paper Award Finalists,203
675,9065.0,Enhancing Acoustic Absorption Performance Through 3D-Printed Infill Designs and Material Selection in Fused Filament Fabrication,Academician,"This study explores the potential of Fused Filament Fabrication (FFF) 3D printing to enhance acoustic absorption performance by leveraging innovative infill designs and material selection. Addressing a critical gap in current research, this work examines how geometric configurations and material properties influence sound absorption efficiency. Polylactic acid (PLA), acrylonitrile butadiene styrene (ABS), and polyethylene terephthalate glycol (PETG) are utilized to evaluate the acoustic performance of traditional infill patterns, including aligned rectilinear, honeycomb, triangles, gyroid, and grid, alongside a novel Kresling Origami (KO) structure. The results demonstrate that the KO pattern achieves superior sound absorption compared to conventional patterns, with material properties playing a significant role in performance variations across frequency ranges. PLA and PETG exhibit promising acoustic properties, while ABS demonstrates potential in specific configurations, highlighting the influence of material damping characteristics and density. Design parameters such as infill density, cell size, and wall thickness further influence the absorption performance, demonstrating the importance of integrated design strategies. This research underscores the potential of 3D-printed structures for customizable and sustainable noise management solutions. By enabling precise control over internal geometries and material combinations, 3D printing presents a unique opportunity to optimize acoustic absorption for various applications, including environmental noise mitigation and soundproofing systems. Future work will focus on refining geometric and material parameters and expanding the study to include additional materials and larger frequency ranges, paving the way for advanced, multi-functional acoustic absorbers tailored to specific needs.",4,102 | Cobb Galleria Centre,M&D Best Student Paper Award Finalists,203
676,6278.0,High-Quality Privacy Preserving Text Data Sharing for Manufacturing Machine Learning Data Market,Academician,"We investigated a multimodal dataset sharing method to share valuable text data from technical documents and narratives to improve Artificial Intelligence (AI) models in manufacturing. High-quality and informative datasets are essential for AI model training and deployment performances. The sharing of privacy-preserving proxy datasets distilled from numerical raw data owned by other manufacturing stakeholders can augment the local datasets and has proven to improve the AI model performance. However, it is challenging to share manufacturing text data under privacy preserving constraints, which is critical to protect know-how and IP information, due to limited access to such text data, thus, limited approach to share them. In this paper, we modeled manufacturing domain knowledge and perceptions by employing multiagent-based large language models (LLMs) to generate high-quality, personalized textual data. Then we integrate Multimodal Variational Autoencoder (MAVAE) to fuse textual and numerical datasets to achieve privacy-preserving data sharing. We validated the proposed method based on microbial fuel cell (MFC) anode design with a focus to use text data to improve the design feasibility prediction by AI models. Different LLM agents are tuned to simulate different design styles, such as design space preferences and design rule configurations. The MAVAE encodes both numerical and text data and integrates their features into a latent space, which demonstrates the effectiveness of the proposed privacy preserving text data sharing. This method is expected to be adopted by practitioners to share both numerical and text data in a data market.",1,102 | Cobb Galleria Centre,Machine Learning for Manufacturing 1,204
677,5985.0,Determining a Base Position of a 7-Axis Robotic Arm to Minimize Energy Consumption using Data-Driven Learning,Academician,"The use of industrial robots, especially robotic arms, is steadily increasing across manufacturing sectors. Reducing the energy consumption of industrial robotic arms therefore is one of the most important energy-related issues in the industrial sector. Although the base position of robotic arms is crucial for managing and saving energy consumption, this research problem has not been studied comprehensively so far. Moreover, rather than more scientific approaches based on data, the intuition or experience of engineers has been applied as the basis to determine the base position. This study proposes an optimization framework to determine the base position by minimizing energy consumption for robotic arms in various industrial tasks. Using data generated from this framework, we train a machine learning model to streamline the otherwise computationally expensive optimization process and enable rapid prediction of optimized base positions with reduced computational costs. This approach provides a practical method to minimize energy consumption across a range of tasks for both stationary and mobile robotic arms in manufacturing environments.",2,102 | Cobb Galleria Centre,Machine Learning for Manufacturing 1,204
678,6730.0,Integrated Machine Learning Models for Defect Analysis in Semiconductor Industry,Academician,"In the semiconductor industry, effective defect management is crucial to maintain high standards of quality and reliability. Components acquired from various suppliers with differing specifications can introduce defects that, if not promptly addressed, may spread throughout the supply chain. Traditional defect analysis methods, often reliant on trial and error, are inadequate for handling the complexity and volume of defects in modern manufacturing settings. This research presents an integrated approach that combines machine learning techniques to enhance defect analysis in semiconductor industry. A case study from a semiconductor manufacturing company known for its rigorous product testing processes to ensure product quality and reliability is considered. Our approach aids decision-makers in analyzing defects and devising proactive strategies to prevent them. Algorithms such as decision trees, random forests, and neural networks are applied to an extensive dataset containing over 5,000 defect instances from multiple structured and unstructured databases with varied features. The models identify root causes and predict the best resolutions for detected defects. Results from the case study demonstrate that machine learning significantly improves the accuracy and efficiency of root cause identification and resolution prediction.",3,102 | Cobb Galleria Centre,Machine Learning for Manufacturing 1,204
679,6901.0,Leveraging Large Language Models for Predictive Modeling of Tensile Strength in FFF-Printed ABS Polymers,Academician,"In recent years, the development of predictive models using large language models (LLMs) has gained significant attention, especially in complex manufacturing processes like additive manufacturing. This study explores the potential of LLM-based models to predict the tensile strength of Acrylonitrile Butadiene Styrene (ABS) polymers fabricated via Fused Filament Fabrication (FFF), a popular 3D printing method. Our research focuses on optimizing FFF process parameters such as nozzle type (enhanced nozzle vs. standard), printing speed, filament color (black and white), and the tensile coupon's distance to improve the mechanical performance of 3D-printed ABS components. For this purpose, two Ender 3 Pro 3D printers were employed to produce 162 ABS dogbone specimens, one using a standard 0.5mm nozzle and the other integrating a novel extruder with an in-situ annealing system, both operating in an enclosed room-temperature environment. Advanced LLM algorithms, including ChatGPT and Llama 3.2, are applied to develop predictive models, leveraging these parameters to accurately estimate tensile strength. The performance of each LLM model is evaluated based on mean absolute error (MAE), root mean square error (RMSE), and R-squared (R²) metrics. Through these analyses, we aim to identify optimal printing conditions that maximize the tensile strength of ABS prints, thereby offering valuable insights for additive manufacturing and contributing to the enhancement of material properties in 3D-printed polymers. The results of this study hold potential for establishing new benchmarks in predictive modeling for 3D-printed material performance using LLMs, a promising direction for the future of smart manufacturing.",4,102 | Cobb Galleria Centre,Machine Learning for Manufacturing 1,204
680,6734.0,Predictive Modeling for Mechanical Strength in 3D-Printed ABS Polymer Specimens Using Machine Learning Algorithms,Academician,"The advancement of machine learning (ML) offers significant potential for optimizing mechanical properties in 3D-printed polymer materials, which are widely used in industries requiring durable and precise components. This study focuses on developing predictive models for estimating tensile strength in 3D-printed acrylonitrile butadiene styrene (ABS) specimens, fabricated through Fused Filament Fabrication (FFF) technology. Optimizing FFF parameters such as nozzle type, printing speed, filament color, and tensile coupon’s distance is crucial for enhancing strength. Using two Ender 3 Pro 3D printers, we produced 162 ABS dogbone specimens. One printer utilized a standard 0.5mm nozzle, while the other incorporated a novel extruder system capable of annealing each layer during printing. All specimens were printed at room temperature within an enclosed box to maintain consistent conditions. Predictor variables included nozzle type (enhanced vs. standard), printing speed (categorical), filament color (black vs. white), and the distance of the tensile coupon. We implemented multiple ML algorithms, including Decision Trees, Random Forests, Gradient Boosting Machine, Support Vector Regression (SVR), and Neural Networks, to develop the prediction models. The performance of each model was evaluated based on RMSE and R² metrics, allowing us to determine the most effective model for accurately predicting tensile strength. Ultimately, our study seeks to identify optimal FFF conditions for improved material performance, providing valuable insights that could contribute to advancing additive manufacturing practices and encouraging data-driven decisions in 3D printing processes.",1,102 | Cobb Galleria Centre,Machine Learning for Manufacturing 2,205
681,6000.0,An application of an improved radial basis neural network to identify significant manufacturing factors in the state-of-health estimation of Lithium-Sulfur batteries.,Academician,"The phenomenon of the lifespan of lithium-sulfur batteries is characterized as a nonlinear phenomenon that presents difficulties for its characterization and prediction. The factors influencing it are various, and determining their effect is a task that requires information, time, and experimentation. Traditional experimental methods require extensive experimentation to identify the factors that affect the electrochemical phenomenon. This work addresses the problem through an experimental analysis based on the application of a Radial Basis Function Neural Network (RBFNN) to determine the significance of the factors affecting the lifespan of a lithium-sulfur battery, to identify their impact on the battery’s lifetime. For the experimentation, four manufacturing factors were considered, and to evaluate significance in a RBFNN, a bootstrap method was used since some statistical assumptions are not met. Through this method, it was possible to identify the factors contributing to the lifespan phenomenon of a lithium-sulfur battery.",2,102 | Cobb Galleria Centre,Machine Learning for Manufacturing 2,205
682,9037.0,Scheduling Mass Customized Production Planning Using Networks Intelligence Techniques,,"Emerging trends of industry 4.0 introduced new concepts such as cloud manufacturing. The multi-tenancy nature of this paradigm provides a suitable platform for mass customization by providing more variability in lower investment costs. Cloud manufacturing also provides a systematic collaboration system for sharing feedback for products and manufacturing processes. This research proposes a novel platform based on multi-agent systems to autonomously collect, optimize, and orchestrate manufacturing services and operations for mass customization. Supply chains adjusted for mass customization receive different set of orders that require combination of resources and processes that may incur high costs of investment to the manufacturing plant. Cloud manufacturing and production planning of shared resources is an efficient way to reduce costs and optimally utilize all existing resources and remove underutilization of the facilities and manufacturing plants. The proposed platform utilizes network intelligence techniques designed based on multi-agent systems to schedule production operations. Production in these systems is based on job shop platform which is a NP-hard problem with extra computational complexities. These systems can be mapped into a network of machines that heterogenous tasks moves inside these stations. Graph neural network is applied that can not only capture spatial and communication patterns in these systems, but also reflects temporal dynamics of the system. This data analyzing platform is also integrated with deep reinforcement learning to create an online and dynamic scheduling mechanism. The effectiveness of the proposed method is investigated in a cloud manufacturing case in which the system receives uncertain orders and service requests.",3,102 | Cobb Galleria Centre,Machine Learning for Manufacturing 2,205
683,8813.0,Image-Based Quality Control for Additive Manufacturing through Layer-by-Layer Analysis,Academician,"Quality control for additive manufacturing is investigated by leveraging deep learning-based segmentation techniques using neural networks. The majority of existing studies adopt methodologies from Convolutional Neural Networks (CNNs) that apply uniform spatial scopes across the whole image. In this study, we propose a novel approach incorporating Spatial Transformer Networks for spatial alignment and Siamese networks for one-shot learning investigations on segmentation accuracy. The state-of-the-art neural net architecture employed will feature in-network scaling for enhanced performance. Identifying gaps in existing methodologies, our method addresses unexplored avenues, specifically focusing on the in-site quantification of measuring over and under extrusion and layer-wise structural integrity. The envisioned experiments will compare the material properties of printed parts to their nominal digital representations in a layer-by-layer fashion. This methodology, with its application to extrusion processes, is poised to offer valuable insights into material properties that are challenging to discern conventionally, thus driving advancements in quality control for additive manufacturing. By providing precise geometric or dimensional quality control, the proposed monitoring system represents a significant step toward ensuring quality control and process efficiency in additive manufacturing.",4,102 | Cobb Galleria Centre,Machine Learning for Manufacturing 2,205
684,6049.0,Statistical Beam-Shaping Melt Pool Modeling,Academician,"Laser powder bed fusion is one of the most popular metal additive manufacturing methods. It uses a high-intensity laser to selectively melt fine layers of metal powder and creates parts with complex geometries. Conventional laser powder bed fusion uses a Gaussian profile laser as it is the most mature laser type. However, due to the high energy concentration, Gaussian beams may create a melt pool with high volatility and a strong keyhole, which leads to problems such as high residual stress and porosities. Recently, beam-shaping lasers have been investigated for attempting a more uniform melt pool and controllable thermal gradient. However, varying beam shapes can lead to complex melt pool thermal distributions, making thermal gradients challenging to describe. In this work, novel statistical models are applied to represent thermal gradient distribution in complex scenarios. By leveraging machine learning techniques, the statistical thermal gradient model parameters can be predicted with process parameters like laser power, scanning speed, and beam shape as inputs. The machine learning model that can achieve this prediction is trained by finite element analysis results. The analysis demonstrates that the machine learning model can predict the parameters of the twin-gaussian thermal gradient model parameter with significantly high R-squared values (R 2 > 0.9). Such information can be used to estimate the dominant microstructure with higher accuracy.",1,115 | Cobb Galleria Centre,Additive Manufacturing 1,206
685,6230.0,Impact of Surrounding Powder on Thermal and Mechanical Behaviors in Powder Bed Fusion Additive Manufacturing Process Simulations,Academician,"Finite element simulation provides a powerful tool for examining the intricate thermal and mechanical behaviors in additive manufacturing processes. By predicting temperature distribution, residual stresses, and deformation, simulations play a critical role in improving part quality and reducing defects. In Powder Bed Fusion (PBF) processes, steep thermal gradients caused by rapid heating and cooling cycles make materials particularly susceptible to residual stresses and warping. While many existing PBF simulations approximate the effect of surrounding unmelted powder using simplified convective boundary conditions, this approach often overlooks the detailed interactions between the powder and the solidified material. This study presents an explicit modeling method for the surrounding powder to investigate its thermal and mechanical interactions with the printed part. By considering the full powder bed in the simulation, we aim to evaluate the effects of the interactions on the process and explore the feasibility of assessing powder reuse potential. The simulation is performed using commercial software (i.e., ANSYS) and the results are validated against experimental data. The proposed approach is expected to enhance the accuracy of PBF process simulations and provide deeper insights into powder behavior and sustainability.",2,115 | Cobb Galleria Centre,Additive Manufacturing 1,206
686,5565.0,Reinforced Scan: A Reinforcement Learning Enabled Optimal Laser Scan Path Planning in PBF Additive Manufacturing,Academician,"Additive Manufacturing (AM) is an innovative technology that fabricates parts layer by layer. However, in powder bed fusion (PBF), printed metal parts often exhibit residual stresses, deformations, and other defects due to uneven temperature distribution during printing. To address this, an optimized scanning sequence within each layer can help mitigate temperature inconsistencies. Traditional optimization methods are based on domain knowledge, employing try-and-error or heuristic methods. Nonetheless, these methods are not universal and cannot achieve the optimal. The challenge of improving the scanning strategy is the large search space for optimizing the scanning sequence for the scanning tracks within the layer. To fill this gap, this work proposes an innovative scan strategy aiming to optimize scanning sequences for achieving uniform temperature distribution in PBF. The proposed Reinforced Scan approach uses reinforcement learning methods to determine the scanning sequence with a customized reward function intelligently. This reward function not only considers the temperature variance but also the spatial uniformity of temperature distribution. This method can significantly reduce the computational burden involved in scanning sequence optimization. The effectiveness of the proposed Reinforced Scan is validated through Netfabb Local Simulation involving laser scanning on a Ti64 thin plate, where its performance is compared with existing heuristic scan sequences. The simulation results demonstrate that Reinforced Scan yields superior outcomes, achieving reduced residual stress compared to conventional heuristic methods.",3,115 | Cobb Galleria Centre,Additive Manufacturing 1,206
687,5691.0,Self-Supervised Deep Learning and Statistical Feature-Based Approaches for Melt Pool Anomaly Detection,Academician,"In this work, we explore the efficacy of two distinct approaches for classifying melt pool images generated during laser powder bed fusion (LPBF), an additive manufacturing process. Our first approach utilizes a self-supervised deep learning model based on Bootstrap Your Own Latent (BYOL). The model is initially pre-trained on an extensive set of unlabeled melt pool images, capturing the intrinsic features of the images. Subsequently, it is fine-tuned on a smaller labeled dataset to improve its classification accuracy on melt pool anomalies. In parallel, we define a set of statistical features to quantitatively describe each image and employ these features in a conventional supervised classification model. The aim is to identify the optimal approach between self-supervised learning and statistical feature-based classification for detecting anomalies in LPBF melt pools. Following comparative evaluation, the more effective model will be leveraged in further analyses to deepen our understanding of melt pool dynamics and improve the LPBF process. This research contributes to advancements in LPBF quality assurance by addressing the challenge of anomaly detection in complex manufacturing environments with minimal labeled data.",4,115 | Cobb Galleria Centre,Additive Manufacturing 1,206
688,8910.0,Contour Scanning Ultrasonic Vibration-Augmented Fused Deposition Modeling,Academician,"Despite being the most common AM technique due to its low-cost equipment, affordable materials, and ease of processing, FDM is limited by the inferior mechanical properties of the produced parts, which result from interlayer voids formed during the freeform deposition of polymers. Consequently, we have yet to see FDM parts usage in high-strength and impact applications. In a previous study, we proved that applying ultrasonic vibration in between printing of layers can reduce voids and increase interlayer adhesion, resulting in better mechanical properties. Our current approach utilizes ultrasonic vibration as an in-situ reinforcing technique to consolidate deposited layers further through squeeze flow. This approach implements a custom image processing algorithm that detects the contour of a part being printed and creates toolpaths inside the contour. From the detected toolpath coordinates, a step-wise contour scanning algorithm is developed to control the positioning of a sonicator probe. An FDM printer is augmented with a custom motorized rig to mount and maneuver the sonicator probe in three dimensions, controlled via Raspberry Pi and Python programming. A tailored g-code sequence is employed to pause and resume the printing process after specific layers are deposited. Ultrasonic vibration was applied at 100% power through a sonicator probe across layers of the specimens. Up to 36% and 73% increase in tensile strength and impact resistance have been obtained following mechanical testing. SEM images of fracture surfaces have displayed significantly increased interlayer consolidation.",1,115 | Cobb Galleria Centre,Additive Manufacturing 2,207
689,6607.0,Multi-Material 3D Printing with Contactless Droplet Deposition Using Acoustic Levitation and Robotic Control,Academician,"On-demand droplet additive manufacturing opens new venues of creating heterogeneously materialized structures. In this research, we investigate the capability of a multiple material 3D printing system capable of contactless, multidirectional resin deposition. This method accommodates multiple resin types in a single fabrication process through acoustic levitation and robotic arm programming. Acoustic levitation suspends each droplet, enabling contact-free manipulation and real-time UV light photopolymerization, while the robotic arm maneuver to provide accessibility that traditional 3D printing technologies are limited at. This presentation will cover photopolymer material characterization, process development, process parameter optimization, material concentration optimization and the system’s potential applications in fabricating multi-material structures that response to external stimulation.",2,115 | Cobb Galleria Centre,Additive Manufacturing 2,207
690,5509.0,Four-Dimensional Printing of Reconfigurable and Programmable Polymer-Derived Ceramics,Academician,"The growing demand for complex ceramic applications has led to increased interest in ceramic additive manufacturing, particularly in polymer-derived ceramics (PDCs), which are valued for their excellent thermal and chemical properties. However, their shape design and manipulation is often constrained by manual or mechanical methods, limiting their practical use. Active materials have emerged as promising alternatives for creating reconfigurable systems. While material extrusion can make PDCs self-deformable, it suffers from drawbacks such as low solid phase content and resolution, which restricts its application in precision-demanding fields like aerospace. To address these limitations, a new vat photopolymerization technique has been proposed to enable the design and creation of reconfigurable, programmable PDCs. This method utilizes a two-step curing process to achieve controlled shape transformations, allowing for programmable structures. It is compatible with high-resolution four-dimensional printing and offers high solid content, thus filling a critical research gap and opening new possibilities for developing PDCs with programmable shape-shifting capabilities.",3,115 | Cobb Galleria Centre,Additive Manufacturing 2,207
691,5796.0,4D printing of shape-tunable blood vessels with magnetic stimuli-responsive hydrogels,Academician,"The vascular system is crucial for the functionality of vital organs, serving a fundamental role in sustaining life. The establishment of vascular structures in vitro significantly advances the field of tissue engineering. Recent efforts have introduced various methods for generating vascular architectures, encompassing both 3D and 4D printing technologies. This research focuses on the fabrication of a magnetic stimuli-responsive hydrogel for printing vascular structures that can be deformed under the influence of magnetic forces. Specifically, we utilized GelMA-based hydrogels with and without magnetic nanoparticles to print hollow structures with varying distributions of magnetic particles. Our study aimed to investigate the deformation of these printed vascular structures when subjected to magnetic forces. Our experimental findings revealed that vascular samples with a homogeneous distribution of magnetic particles exhibited an 8.07% deformation towards the magnetic source. Additionally, significant differences in the mechanical properties were observed between structures printed with and without magnetic particles. The distribution of magnetic particles both across layers and within layers of the printed structures was adjusted to create vascular constructs with distinct magnetic profiles and mechanical characteristics. By fine-tuning the distribution of magnetic particles, we aimed to print vascular structures with diverse deformation and mechanical attributes, facilitating the development of disease models related to vascular sclerosis and stenosis. The characterization of the printed structures included assessments of their deformation and mechanical properties, contributing to advancements in the field of tissue engineering and disease modeling.",4,115 | Cobb Galleria Centre,Additive Manufacturing 2,207
692,5904.0,A Comprehensive Framework for Implementing Industry 4.0 Technologies in Smart Manufacturing,Academician,"The digital age has revolutionized manufacturing with data capture, information technology, and networking advancements. This rapid pace of technological progress introduces a new set of challenges. Industry 4.0 addresses these by championing ""smart manufacturing"" a data-driven approach enhanced by cyber-physical systems, digital twins, cloud computing, the internet of things, and big data analytics. These technologies aim to improve efficiency, adaptability, and connectivity across production systems, marking a shift from traditional methods to data-intensive processes. A crucial factor in the success of Industry 4.0 is establishing global standards for interoperability. Frameworks such as the Reference Architecture Model for Industry 4.0 (RAMI 4.0) and the Industrial Internet Reference Architecture (IIRA) help manage complexity and ensure compatibility. RAMI 4.0, for instance, uses a three-dimensional model to simplify complex interconnection in manufacturing, while IIRA, developed by the Industrial Internet Consortium, offers an open, standards-based architecture suited for cross-industry applications. The effectiveness of these frameworks can vary depending on specific industrial needs, and ongoing refinement is essential. This study reviews existing architectures for Industry 4.0, exploring their support for smart manufacturing and identifying critical gaps to address. As digital technologies advance, organizations must adapt their structures and strategies to harness the full potential of digital transformation. Evaluating digital maturity becomes essential to develop strategies for sustainable growth. This research highlights the critical role of international collaboration and investment in standardization as vital components in realizing the full potential of Industry 4.0 and enhancing manufacturing practices on a global scale.",1,115 | Cobb Galleria Centre,Intelligence Manufacturing 1,208
693,6501.0,MTConnect for Communication within and across Manufacturing Enterprises,,"The manufacturing sector is transforming due to increasing demands of efficiency, connectivity, and interoperability between different systems. MTConnect has a key position in this respect, as a royalty-free, open-source protocol that can guarantee seamless communication among machines, devices, and software systems, either on conventional shop floors or distributed manufacturing environments. This paper provides a framework for implementing MTConnect in a manner that bridges traditional manufacturing with distributed systems using IIoT for real-time data exchange, monitoring, and analysis. Standardizing data formats, MTConnect not only optimizes shop floor operations because of better decision-making capabilities but also supports adaptive automation and scalability in distributed manufacturing networks. It is a framework that supports exploitation of data-driven insights, enhanced fault detection, and predictive maintenance but also allows non-expert technologists to get involved in data interpretation. The main challenges regarding integration with legacy systems and interoperability over distributed networks are covered through the application of semantics-based adapter models, publish/subscribe systems, and universally unique identifiers (UUIDs). Ultimately, this study demonstrates how MTConnect catalyzes operational efficiency, data accountability, and futureproofing for manufacturing systems in a rapidly evolving industrial landscape.",2,115 | Cobb Galleria Centre,Intelligence Manufacturing 1,208
694,6727.0,Optimizing Component Selection in Aerosol Can Production: A Practical Model for Industrial Application,Practitioner,"Optimizing component selection is essential in the production of three-piece aerosol cans, especially when balancing structural integrity with cost efficiency. Three-piece aerosol cans vary in diameter, height, and pressure ratings, and to maintain a constant diameter and height at higher pressures, manufacturers must use thicker components in the can's top, body, and bottom. Thicker components enhance the can’s ability to withstand higher internal pressures, meeting regulatory specifications, while lighter components suffice for lower pressure ratings. However, manufacturing processes typically require machine changeovers when transitioning between materials of varying thickness, which introduces additional downtime. This study addresses the trade-off between increased material costs and reduced downtime by implementing a non-linear binary programming optimization model to evaluate whether substituting thicker components in lower pressure applications can lead to overall cost savings. The proposed optimization model accounts for various can sizes, pressure ratings, component options, and associated costs. A real-world application of the proposed optimization model in an aerosol can manufacturing facility is presented. The results reveal that changing to three heavier components in lighter cans can indeed be cost-effective, confirming the efficiency of the current operation with the available components. These findings highlight the importance of strategic component selection in minimizing manufacturing disruptions and maximizing cost-efficiency in can production.",3,115 | Cobb Galleria Centre,Intelligence Manufacturing 1,208
695,6503.0,Leveraging Automation and Axiomatic Design for Efficient Pitch-Matching in Drumstick Manufacturing,Academician,"Manufacturing as an industry is rapidly evolving, driven by the increased application of systems engineering methodologies and the widespread adoption of automated technologies. A central challenge in this evolution is redesigning existing production processes to meet demands of high-volume, high-quality output. In this context, the automation of traditional processes has emerged as a critical strategy for streamlining manufacturing operations. In this project, we addressed one such challenge by developing an automated solution to improve the pitch-matching process at Vater Percussion, a renowned Massachusetts-based manufacturer of premium drumsticks. In the initial stages of the project, Vater Percussion produced approximately 3,400 first-quality and 5,600 second-quality drumsticks per day. To ensure uniformity in sound and feel, first-quality drumsticks undergo a Quality Assurance process that ensures each pair is pitch-matched within the same category of weight and resonant frequency tolerances. The automated solution developed in this project applied an Axiomatic Design approach. Specialized equipment was developed and employed to measure the weight and resonant frequency of each drumstick, with the data processed through a custom program that directs a collaborative robot to sort the drumsticks based on predefined weight and frequency tolerances. By leveraging Axiomatic Design principles, the automated system ensures an efficient pitch-matching process, capable of handling an increased throughput requirement of 5,000 drumsticks per day. Ultimately, this project demonstrates how the integration of automation and engineering can improve manufacturing processes, contributing to higher production efficiency, improved quality control, and the continued success of Vater Percussion in a competitive industry.",4,115 | Cobb Galleria Centre,Intelligence Manufacturing 1,208
696,5913.0,Accelerated Material Innovation in Additive Manufacturing: An AI-Driven Approach Using Bayesian Optimization,Academician,"This study examines how Bayesian Optimization (BO) can enhance additive manufacturing (AM) by guiding experimental processes to achieve specific optimal material properties. In AM, achieving high-quality and consistent outputs involves navigating a vast parameter space, and BO offers a targeted approach to do this efficiently. By prioritizing experiments that yield the most valuable data, BO refines AM settings iteratively, significantly reducing the time and resources needed to optimize results across diverse materials. In addition to improving experiment selection with Bayesian Optimization, automation streamlines the testing process, helping researchers and manufacturers reduce hands-on tasks and enhance workflow efficiency. This study presents an adaptable, AI-driven framework that powers a self-driving manufacturing lab, facilitating rapid and precise tuning of additive manufacturing parameters. This approach enhances productivity and accelerates the development of optimized material properties for specialized applications. It highlights the role of AI and automation in fostering faster innovation in manufacturing and advancing R&D in material design by simplifying the discovery of optimal configurations for high-performance, application-specific materials.",1,115 | Cobb Galleria Centre,Intelligence Manufacturing 2,209
697,5818.0,Accelerating Manufacturing Decisions using Bayesian Optimization: An Optimization and Prediction Perspective,Academician,"This research introduces a novel approach to optimize manufacturing processes by leveraging Bayesian optimization. Manufacturing processes are intricate and often involve balancing multiple, sometimes conflicting, parameters to achieve specific product properties. Traditional optimization methods struggle with this complexity, especially when experimental testing is costly and time-consuming. Our proposed framework addresses these challenges by employing a Gaussian Process-based surrogate model within a Bayesian optimization loop. This model progressively learns from experimental data, enabling the efficient exploration of the design space. Unlike standard optimization methods, this approach focuses on understanding the broader landscape of potential outcomes, providing valuable insights for decision-making. To enhance the efficiency of the optimization process, we incorporate an epsilon-greedy sequential prediction framework. This strategy dynamically balances exploration (gathering new information) and exploitation (utilizing existing knowledge) to improve model accuracy while minimizing the number of experiments. This refined acquisition strategy reduces experimental requirements while maintaining prediction reliability. Evaluations on real-world datasets demonstrate that this framework can achieve effective optimization and prediction with fewer experiments compared to traditional methods, promising a more efficient pathway for understanding and controlling complex manufacturing processes.",2,115 | Cobb Galleria Centre,Intelligence Manufacturing 2,209
698,6342.0,Digital Twin in High Precision Machining - A Systematic literature Review,,"This paper explores the emerging technology of Digital Twins and their applications in the high-precision machining industry in predicting the tool life and controlling the process variation. A Digital Twin is a virtual model that mirrors a real-world device throughout its lifecycle, incorporating real-time data from the physical system to provide insights and facilitate decision-making. Digital Twins may be created for various elements, including individual assets, production lines, end products, or any other real-world scenarios within the production process. One of the biggest challenges manufacturers faces is that production data is scattered across various systems and formats, making it difficult to extract meaningful insights. Through a systematic literature review and case study, we investigate Digital Twin adoption and application to performance and maintenance in critical machine tools in aviation. In aviation, complex geometry critical parts have very close feature tolerances and while machining these features, cutting tools, machining parameters and condition of the machine play a vital role in controlling the process. The paper emphasizes the role of Digital Twins in monitoring the geometry and lifespan of cutting tools, machining parameters and controlling the process in critical machine tools.",3,115 | Cobb Galleria Centre,Intelligence Manufacturing 2,209
699,7034.0,An Integrated Framework for Aerospace Components Repair Using Hybrid Laser Wire Additive Manufacturing and Machining,Academician,"The maintenance of mission-critical aircraft engine components faces significant challenges due to the high cost and long lead times associated with spare parts, particularly those made from high-performance alloys such as Inconel 625. This research aims to develop an integrated hybrid manufacturing framework that ensures precise restoration of part geometry and consistency in mechanical properties while minimizing inventory costs through on-demand repair. The framework combines 3D scanning, CAD/CAM, additive manufacturing, and machining to restore damaged parts to their original specifications efficiently and cost-effectively. The approach is divided into three phases. The first phase utilizes an integrated 3D scanning, computer-aided design/manufacturing (CAD/CAM), additive manufacturing, and machining to accurately repair damaged parts. The second phase involves process parameter optimization through experimental design to ensure that the repaired part’s mechanical properties meet functional requirements. The third phase employs a cost model to evaluate the economic benefits of the repair procedure. Our results demonstrate significant cost reductions and favorable mechanical properties, with all outcomes meeting the acceptance thresholds for the component specification. This study shows that the proposed framework provides a sustainable, cost-effective solution for aerospace part repair, offering a viable alternative to part replacement.",1,115 | Cobb Galleria Centre,Additive Manufacturing 3,210
700,6186.0,DCAE-enabled Thermal History Compression for Laser-based Additive Manufacturing Processes,Academician,"Additive Manufacturing (AM) process monitoring continues to improve for mission critical applications, such as aerospace, biomedical, and defense industries, to minimize process or component defects and meet strict tolerances. A major driving force that advances AM process monitoring is the advanced sensing modalities that collect more data of diverse formats and at faster sampling rates. A drawback to integrating more sensing modalities is the significant rise of the resulting data volumes, making it challenging to store, transmit, and share these overwhelmingly large datasets. Therefore, there is an urgent need for effective management of the big data generated from AM processes. One promising strategy to alleviate storage space and boost AM process data transmission speeds is data compression. This study proposes a new framework that leverages deep convolutional autoencoders (DCAEs) to compress, share, and restore thermal history data, which is one of the most critical sensing modalities in the quality control of laser-based AM processes. A case study based on a Laser Powder Bed Fusion (L-PBF) process is used to evaluate the proposed framework using multiple compression performance metrics, including compression ratio and reconstruction accuracy. The proposed DCAE-enabled framework allows AM users to preserve the most significant data characteristics in their compressed forms, leading to significantly reduced data volumes for more efficient data storage, transmission, and sharing.",2,115 | Cobb Galleria Centre,Additive Manufacturing 3,210
701,6149.0,Dynamic Extruder Speed Control in Large Format Additive Manufacturing via EAPPO,,"Large Format Additive Manufacturing (LFAM) utilizes a fused filament fabrication technique, where an extruder continuously deposits melted thermoplastic material layer by layer. Since the temperature of the base layer significantly impacts the quality of the final product, dynamically controlling the extruder’s speed becomes essential, as it enables precise temperature management, ultimately enhancing both efficiency and product quality. To effectively control the extruder speed, it is essential to understand the thermodynamic behavior of the part’s surface during printing. This paper employs a Transformer architecture to analyze the spatiotemporal relationships among cooling profiles across different locations on the part’s surface, providing accurate temperature predictions. Previous methods largely relied on nonlinear mixed-integer programming (MIP) for extruder speed control. However, when practical factors such as extruder acceleration are considered, these models often become unsolvable or computationally prohibitive. This paper addresses this challenge by training an agent with the environment-aware Proximal Policy Optimization (EAPPO) algorithm in a custom-designed environment that simulates the LFAM printing process. This setup enables the agent to iteratively explore the feasible region to identify an optimal solution. The proposed approach is validated in a hexagon case study, with results showing it can find better solutions when MIP fails in complex scenarios, thereby improving printing efficiency and quality.",3,115 | Cobb Galleria Centre,Additive Manufacturing 3,210
702,9081.0,Partially Observable Markov Decision Processes (POMDP) Framework for Decision-Making Under Uncertainty Using Current Detection Based Monitoring System,Academician,"In Electrohydrodynamic printing (EHD), maintaining consistent printing quality and specifications remains challenging due to the complex relationships between operational parameters. This study demonstrates using the Partially Observable Markov Decision Processes (POMDP) framework to create an EHD printing closed-loop control system designed to enhance the decision-making process under uncertainty. The dynamic nature of EHD printing facilitates modeling the process as a sequential problem where the actual printed pattern specification is not directly observable in real-time, however, operational parameters will be adjusted according to observational feedback from a current detection-based monitor system. The POMDP framework defines states as the line width segment of the printed pattern. These states are developed by unsupervised machine-learning algorithms that explore the printed lines and discover inclusive line width segments. These segments describe the state space. Note that image-processing techniques are used to engineer and extract features from final output microscope images which feed into machine learning algorithm. Actions modify key operational parameters such as the applied voltage and standoff distance, while observations are the signal received from the monitoring system after being processed. Different algorithms are applied to find the optimal solution for the POMDP problem in EHD printing settings. The results evaluated the performance and applicability of multiple solution algorithms including exact approaches, approximation approaches, and other planning-based approaches. This work offers a reliable and robust quality assurance framework for EHD settings that would facilitate automation, handle process variability, and improve final quality.",4,115 | Cobb Galleria Centre,Additive Manufacturing 3,210
703,7044.0,Designing Reverse Logistics Networks for Metal Remanufacturing with Cloud-Based Additive Manufacturing Systems,Academician,"As the circular economy becomes increasingly important in manufacturing, demand is rising for technologies that improve repair, restoration, and remanufacturing processes. Additive manufacturing (AM) shows great potential in metal remanufacturing and reverse logistics for industries like defense, aerospace, and automotive, due to its flexible processing and distributed manufacturing capabilities. While research has focused on metal AM’s repair precision and efficiency, few studies have examined its impact at the supply chain level. This study fills that gap by investigating cloud-based AM services (CA-RMfg), which support reverse logistics in metal remanufacturing while lowering capital and technical barriers for small and medium enterprises (SMEs). In this study, we developed a bi-objective reverse logistics model within a CA-RMfg framework, linking key facilities and managing network variability. A real-world case study involving aircraft engine repair providers in Texas, identified through NAICS-SIC codes, demonstrates the model’s effectiveness. Findings reveal that while a larger distribution of AM hubs can reduce wait times, it may raise unit repair costs for SMEs due to facility underuse. However, optimized AM hub placements significantly lower these costs for SMEs, achieving up to double or triple the reductions compared to in-house facilities.",1,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 1,211
704,8788.0,Enhancing Disaster Supply Chain Resilience Through the Integration of Mobile Additive Manufacturing and Truck-Drone Delivery Systems,Academician,"Supply chain disruptions from disasters cause delays, financial losses, and reputational damage. To tackle this, organizations aim to build resilient networks. This study integrates mobile additive manufacturing (MAM) with truck-drone delivery (TDD) systems to boost supply chain resilience (SCR) during disasters. The framework synchronizes truck-drone deliveries with customer time windows and MAM operations, uniting production and delivery processes. The research examines how vibrations and surface roughness impact AM processes in mobile settings, particularly during disasters. Strategies ensure high product quality despite these challenges. A mixed-integer linear programming (MILP) model minimizes operating costs, accounting for penalties from missed deliveries, timing deviations, and quality issues caused by environmental factors. A heuristic approach—referred to as the MAM-TDD variable neighborhood descent (MTVND) method—was introduced to solve the model. Additionally, three alternative models were evaluated for comparison: MAM combined with truck delivery, fixed additive manufacturing (FAM) paired with TDD, and FAM with truck delivery. A real-world case study was conducted to assess the performance of the primary model against these alternatives. The results emphasized the superiority of the MAM-TDD system under disaster conditions, particularly in extending delivery reach and ensuring timely fulfillment of orders. The study analyzed disaster severity levels—high, moderate, and low—demonstrating the robustness of the system. This research offers practical insights for managers, showcasing how MAM and TDD create flexible, innovative supply chain solutions in disaster scenarios.",2,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 1,211
705,6872.0,Integrating Multi-Skill Assembly and Logistics Labor Scheduling Optimization,Practitioner,"Traditionally, assembly factories distinguish logistic workers from production workers in task allocation and labor scheduling due to their nature of task differences, management complexity, and the consideration of robustness in large-scale operations, though many multi-skilled workers are capable of performing both assembling and material handling tasks. In our target research field of distributed manufacturing, a notable emphasis is placed on minimizing the cost of operations. Yet, to the best of our knowledge, there is a gap to fill within the research field of assembly and logistic worker integration to gain utilization improvement. This research paper attempts to explore the design and impact of assembly and logistics worker integration on labor scheduling and assignment in an assembly factory in distributed manufacturing. Based on a practical case, the study employs a labor scheduling optimization model to examine the effectiveness of two labor allocation strategies in the manufacturing environment. This study introduces an organizational structure that allows production and logistic tasks to be consumed and solved by the same optimization model. A comparison to examine the outcomes can be built and tested based on this organizational structure. In the baseline scenario, logistic tasks are limited exclusively to logistic workers, whereas in the alternative scenario, logistic tasks are allowed to be performed by certain production worker types. The outcome encompasses both the advantages and limitations of integrating production and logistic workers, with the goal of providing practical insights into improving productivity in assembly factories through the augmentation of labor flexibility.",3,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 1,211
706,6333.0,Parallel Batch Scheduling With Incompatible Job Families Via Constraint Programming,Academician,"This paper addresses the incompatible case of parallel batch scheduling, where compatible jobs belong to the same family, and jobs from different families cannot be processed together in the same batch. Existing constraint programming (CP) models for this problem fail to synchronize the processing of the jobs within their batch, resulting in batch interruptions. In the context of the diffusion area in the semiconductor manufacturing process, these interrupted solutions would disrupt the thermal stability required for a uniform dopant distribution on the wafers. This paper proposes three new CP models that directly tackle these interruptions in the formulation, including two adaptions of existing models for the compatible case, and a novel Redundant Synchronized (RS) model that adds redundancy to the problem structure to improve computational performance. These existing and novel models are compared on standard test cases, demonstrating the superiority of the RS model in finding optimal or near-optimal solutions quickly.",4,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 1,211
707,4904.0,eXplainable AI (XAI) Approach to Predictive Maintenance of Metro Public Transportation Systems,Academician,"This research applies a benchmarked AI-based approach to reactive maintenance, focusing on ""Repair Corrective Action"" for critical failure modes within the extensive MetroPT dataset, which contains over 226 million data points. Inspired by prior applications of SHAP (SHapley Additive exPlanations) in complex systems, we applied similar XAI techniques to enhance model interpretability in failure mode analysis. By adding new labels for failure modes and responsible components, we enabled detailed root cause analysis and deeper data enrichment. Time-series plots of sensor data for three failure modes revealed significant patterns essential for predictive maintenance. Using SHAP, XGBoost allowed us to interpret critical feature influences on failure modes, mirroring earlier benchmarks in AI explainability. The analysis identified essential predictors, such as ""Oil_temperature,"" ""TP3,"" and ""Motor_current,"" with analog sensors highlighting ""DV_pressure"" and ""Reservoirs"" and digital sensors emphasizing ""LPS"" and ""COMP."" SHAP’s decomposition of these feature relationships provided actionable insights into parameter interactions, aligning with benchmarked successes in enhancing model transparency. This AI-driven, XAI-supported approach facilitates targeted corrective actions, helping prioritize key failure indicators to reduce downtime and improve maintenance efficiency. Future work will extend SHAP measures for a more transparent, data-driven predictive maintenance model that metro operators can trust for optimized decision-making.",1,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 2,212
708,8962.0,Evaluating the Effectiveness of AR/MR Technologies in Manufacturing Training: A Multi-Modal Assessment Framework,Practitioner,"While Augmented and Mixed Reality (AR/MR) technologies show promise for manufacturing training applications, their implementation is often hampered by uncertainty about real-world effectiveness and return on investment. This study presents a comprehensive, multi-modal assessment of AR/MR training systems in a manufacturing assembly context, comparing traditional paper-based work instructions with projected AR and two head-mounted display implementations (AR/MR). Using a between-groups experimental design (n=54), we evaluated four instructional methods across three phases: learning, recall, and retention. Beyond traditional performance metrics like task completion time and error rates, we incorporated NASA Task Load Index (TLX) for cognitive workload and System Usability Scale (SUS) for user experience. This multi-modal approach revealed several counterintuitive findings about the relationship between technical complexity, perceived workload, and actual performance. Key results include: (1) a quantified speed-accuracy tradeoff between traditional and augmented methods, (2) evidence that technical complexity does not necessarily correlate with perceived workload or usability, and (3) identification of critical environmental and user characteristics that impact implementation success. The presentation will include practical recommendations for manufacturing organizations considering AR/MR training implementations, focusing on optimal use cases, potential pitfalls, and strategies for successful deployment. This research contributes to the growing body of knowledge on advanced manufacturing training methods by providing empirically-validated insights into the practical implications of AR/MR adoption, while highlighting the importance of comprehensive assessment approaches that go beyond simple performance metrics.",2,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 2,212
709,6129.0,Enhancing Medical Device Manufacturing: A CAD/CAM-Driven Approach to Structural Integrity and Process Optimization at MedDevices Inc,Practitioner,"This research examines the transformative role of CAD/CAM technologies in advancing the design and manufacturing of medical devices at MedDevices Inc., a Dominican company specializing in orthopedic solutions. The medical device sector’s rapid expansion, fueled by demographic shifts and technological innovations, underscores the necessity for precise and adaptable manufacturing methodologies. This study specifically addresses the production of a post-operative knee brace, targeting persistent issues in structural integrity, user safety, and process uniformity. By integrating CAD/CAM-driven design enhancements, this project proposes solutions to the frequent failures in the Range of Motion (ROM) limiters, including structural reinforcements that increase fatigue resistance and improve component durability. Additionally, a user-error mitigation feature was developed, introducing a spring-loaded latch mechanism that restricts excessive movement, thus reducing injury risk. These design optimizations were prototyped and iteratively refined using advanced 3D printing, enabling efficient testing without disrupting existing production lines. The implementation of CAD/CAM-enabled automation in assembly processes, including robotic vision systems and digital controllers, further addresses production consistency, minimizing human error and enhancing output precision. This approach not only promises a reduction in rework rates and production waste but also significantly elevates product reliability and customer satisfaction. The study’s findings position MedDevices Inc. to leverage CAD/CAM as a competitive advantage, setting a replicable precedent for digital transformation within the medical device industry. The success of these initiatives highlights CAD/CAM’s capacity to drive innovation in high-stakes manufacturing environments.",3,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 2,212
710,9339.0,A Study on Robot-Worker Quality Control for Manufacturing Using Ambiguous Puzzles as a Proxy Task,Academician,"Mid-sized manufacturing companies that produce customized, high variation products at medium scale more sensitive to fabrication flaws detected at the end of the assembly process (e.g., boats, fire trucks, etc). There is also less likely to be a uniform process for quality control inspection due to product variability, and increased sensitivity to worker skill in high turnover settings. Robotic quality control is one means to address worker skill and uniformity issues, however high-performance visual inspection technologies and algorithms are difficult or costly to obtain, or do not exist today. This paper studies the potential benefit of employing approximately correct visual inspection algorithms in coordination with a human worker. It does so through the use of visually ambiguous puzzles as a proxy for a manufacturing quality control task. If the inspection task has a complexity that scales poorly with product area or volume, then there is value to incorporating an imperfect robot inspection assistant. Justification for this arises from using a task complexity model for the human based on big-O concept from analysis of algorithms, and a task performance model based on false positive and false negative rates for automated classification algorithms. The inspection improvement outcomes are shown to depend on the inspection task complexity for the worker and the false inspection rates for the robot. Relaxing robot performance standards provides more flexible deployment options with different cost structures that may alter the decision to incorporate robotic assistants and support improved manufacturing outcomes.",4,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 2,212
711,6037.0,Accelerating Electrodeposition Simulations with Physics-Informed Neural Networks,Academician,"Electrodeposition plays a crucial role in applications like battery manufacturing, corrosion prevention, and metal finishing, where precise control is required to achieve optimal deposit characteristics. Traditional simulation techniques such as finite element and finite difference methods, which solve the governing partial differential equations, are often computationally intensive. To address these challenges, this study presents a machine learning framework leveraging Physics-Informed Neural Networks (PINNs) to accelerate electrodeposition simulations. PINNs combine data-driven approaches with physical laws, allowing the model to achieve high fidelity while respecting conservation principles, such as those governed by the Nernst-Planck equation. The methodology includes using a simplified diffusion model to train the PINN, focusing on capturing the dynamics of ionic concentration through Fick's Law. The developed PINN was trained on data generated from conventional finite difference simulations and validated against literature and experimental datasets. The results indicate that the trained PINN can predict deposit growth and concentration profiles at a fraction of the computational cost compared to traditional methods, without sacrificing accuracy. This work demonstrates the potential of using physics-constrained deep learning to advance electrodeposition modeling, significantly reducing computational overhead while maintaining the physical realism of the simulated outcomes.",1,102 | Cobb Galleria Centre,Intelligence Manufacturing 3,213
712,8537.0,Do you trust Digital Twins? How Verification and Validation can support the development of trusted Digital Twins.,Academician,"Digital Twin (DT) is a topic of growing interest for both industry and academia. Since its introduction, the DT concept has evolved and been applied to different fields leading to various definitions and abstraction levels. Despite the definitions utilized to describe these models, to use DTs for decision-making, it is fundamental to trust in all components of a DT. This is particularly important in manufacturing environments where DTs support decisions that directly impact cost and safety. Therefore, Verification and Validation (V&V) will be the foundation for building model trust (or model credibility) in DTs. Even though the importance of V&V is extensively recognized as a fundamental step in developing models that can be trusted and used in real-world environments, very few works report implementing such activities in the DT landscape. As DTs gain prominence, the question that arises is how can we create DTs that can be trusted and used in real-world environments to support decision-making? In this research, we explore how V&V can enhance the development of trusted DTs, using a case study in additive manufacturing to illustrate the discussed concepts.",2,102 | Cobb Galleria Centre,Intelligence Manufacturing 3,213
713,8863.0,Leveraging Value Stream Mapping for Effective Digital Transformation,Academician,"Digital Transformation (DT) is a critical yet complex, holistic approach that reshapes an entire organization to achieve substantial improvements in performance, reach, and operational scope. For organizations seeking to adopt Industry 4.0 standards, attaining digital maturity (i.e., the ability to respond to the emerging competitive digital environment appropriately) poses numerous challenges due to the intricate re-engineering of traditional processes and the complexity of integrating new technologies. Value stream mapping (VSM) is a well-established lean methodology traditionally used to visualize the current state of material flow and information flow, design a desired future, and develop a prioritized plan to systematically implement lean tools to achieve the future state. This study proposes that the VSM methodology can be used in same manner for digital transformation by considering and integrating Industry 4.0 technologies into the future state design. By applying VSM within the Industry 4.0 context, companies can strategically align operational changes with transformation goals and effectively evaluate the progress of a DT, focusing not only on machine integration but also on enhancing human collaboration and skills. This study aims to support companies in their DT journey by proposing VSM as a critical tool to define a vision and roadmap that determines the way to digital maturity.",3,102 | Cobb Galleria Centre,Intelligence Manufacturing 3,213
714,5924.0,Gathering Personalized Design Requirements via Digital Twin,Practitioner,"Industry 4.0 has had a profound impact on how things are manufactured. The advent of the Internet of Things, digital twins, big data, and extended reality has made manufacturing more flexible and given deeper insights into manufacturing systems. This has led to offering the customer more design choices. The average consumer has grown to expect this, demanding the ability to customize products even more. This has led to the manufacturing paradigms of mass customization and mass personalization. The difference between these two is the role the customer takes in the design process. In mass customization a set number of choices are offered to the customer. In mass personalization the customer takes an active role in the design of the product, offering input on requirements and functional needs. Most research around mass personalization has focused on the production of small to one batch sizes, but because of the increased customer involvement in the design phase more attention should be given to personalized design. The key issues with personalized design are the inability to gather accurate requirements and the resource intensive nature of the design phase. This problem will be solved in two phases. The first phase will focus on creating a customer/product interaction digital twin. The second phase will focus on creating a design system that generates design possibilities. Combining the outcomes of these two research phases will offer a feasible solution to the personalized design problem. This specific work will focus on phase one of the solution.",4,102 | Cobb Galleria Centre,Intelligence Manufacturing 3,213
715,5810.0,Transferable Deep Reinforcement Learning for Adaptive Control Across Varying Manufacturing System Configurations,Academician,"Deep reinforcement learning (DRL) offers a powerful approach to adaptive control of manufacturing systems under various uncertainties. Although existing DRL approaches can consider factors such as material supply fluctuations, shifts in production demand, or changes in energy prices, these methods are typically designed for training on a single, fixed manufacturing system configuration. Consequently, these DRL models must be retrained from scratch to accommodate new manufacturing configurations, limiting their adaptability and scalability. To overcome this limitation, this paper proposes a transferable DRL architecture that can be directly applied across diverse discrete manufacturing systems. Specifically, we propose a fully distributed multi-agent framework that categorizes manufacturing resources into distinct classes based on their functions. Each class of resources requires a similar structure of local information for decision-making, allowing each DRL agent to learn control policy for a class of resources rather than an individual resource. A new observation update method ensures that observable local information is sufficient for distributed agents to make collaborative decisions for a class of resources, regardless of their location within the manufacturing system. As manufacturing systems can be viewed as combinations of these resource classes, agents trained under this framework can be combined flexibly and adapt to various manufacturing system configurations. In case studies, agents are trained on randomly generated manufacturing configurations, and their performance is evaluated on new system setups. Results demonstrate the DRL agents' effectiveness in minimizing energy costs under dynamic pricing while satisfying production throughput constraints.",1,103 | Cobb Galleria Centre,M&D Best Track Papar Award Finalists,214
716,6261.0,Innovative Sequential Experimental Design for Efficient Digital Twins Development in Additive Manufacturing,,"Digital twins are an essential tool for advancing material design, process optimization, and quality control in additive manufacturing. However, challenges remain in developing models for solving significant forward and inverse problems. One critical issue lies in the need for a large number of high-fidelity simulations across an extensive parameter space, compounded by the high computational cost. Under resource constraints, this broad allocation of computational efforts often dilutes effectiveness, as resources are spread thin across the entire parameter space instead of being strategically focused on key areas. To address this, there is an urgent need for efficient sampling strategies that can precisely target and optimize critical regions, ensuring both accuracy and sampling efficiency. In this work, we propose a novel sequential experimental design framework that prioritizes targeted bounded ranges for critical quantities of interest, enabling more focused simulations. This approach provides researchers with the means to efficiently develop digital twins, substantially accelerating their efforts to tackle challenging inverse problems in additive manufacturing.",2,103 | Cobb Galleria Centre,M&D Best Track Papar Award Finalists,214
717,8637.0,Generative AI for Resilient Design of Manufacturing Processes,Academician,"Manufacturing systems often face unexpected disruptions such as machine failures or material shortages, which can severely impact the production performance. Traditional methods for addressing these disruptions tend to be time-consuming and resource-intensive, which cannot effectively maintain the resilience of manufacturing systems. Despite recent advances in digital twins (DT) and artificial intelligence (AI), very little has been done to mitigate high computational demands and generate resilient designs of process flows under uncertainty. Therefore, this paper presents a new generative AI (G-AI) approach for the on-the-fly designs of manufacturing systems in response to production disruptions, integrating digital twin models with neural networks to optimize process flows and increase system resilience. First, we propose a novel Generative Adversarial Network for system design (D-GAN) to generate diverse, adaptive system designs that align production performance with target key performance indicators (KPIs). Second, DT models are coupled with statistical metamodeling to optimize the sequential probability of design improvements. Experimental results show the high potential of new G-AI approaches to generate cost-effective system designs and enhance manufacturing resilience.",3,103 | Cobb Galleria Centre,M&D Best Track Papar Award Finalists,214
718,8704.0,Spatiotemporal Alignment of Manufacturing Lifecycle Data Through Feature-level UUIDs,Practitioner,"Four key stages of the manufacturing lifecycle, design, plan, execution, and inspection, have differing concerns regarding a manufactured part despite originating from the same functional requirements. In the design stage, a CAD model is developed along with some GD&T information for later inspection. The planning stage uses the CAD model to generate a manufacturing plan to be executed in the execution stage. Within the execution stage, the part is manufactured from some stock material following the previously established plan. Lastly, the inspection stage measures part characteristics to obtain boolean pass/fail results. A key concern with this common approach is the loss of key information when moving across lifecycle stages and the difficulty of informing changes in stages such as design and planning with the results of execution and inspection. In this work, an open standards-based part-level digital thread is established that spatiotemporal aligns data from each lifecycle stage to geometric features on the part. Using the design model as a base, the manufacturing plan is simulated to link design geometric feature UUIDs to the manufacturing plan. Upon execution, the UUIDs injected into the NC plan are cross-referenced to align NC block numbers to manufacturing timestamps. The inspection plan's nominal features are cross-referenced with the CAD model to find the geometric feature of closest fit. Using UUIDs placed on geometries of interest in the design stage, manufacturing design (STEP AP242), plan (NC), execution (MTConnect), and inspection (QIF) data can be referenced relative to geometric features for further analysis.",4,103 | Cobb Galleria Centre,M&D Best Track Papar Award Finalists,214
719,6405.0,Advances in Dip-Pen Nanolithography for Precision Nanofabrication and Applications in Nanobiotechnology,Academician,"In the coming decade, the research and development sector will face a critical need to advance the scientific basis, economic viability, and sustainability of nanomanufacturing while coordinating with nanobiotechnology and nanomedicine. Conventional techniques like photolithography and electron beam lithography sometimes have high prices, little material flexibility, and can be limited in resolution due to diffraction limits. Dip-pen nanolithography (DPN) is an emerging nanofabrication technique for creating molecular patterns on substrates with high precision by directly transferring materials to a surface using an atomic force microscopy (AFM) tip. DPN has advanced in efficiently transporting different inks to surfaces. Advanced DPN derivatives have improved throughput with multi-pen writing modes. We will review the advantages of the DPN technique over traditional lithographic methods regarding precision, material flexibility, and real-time monitoring. Focus will also be given to the principles of DPN ink transport, examining the factors influencing ink transfer onto surfaces. The review will also focus on the influence of environmental variables like humidity and temperature on ink transportation dynamics. Recent advancements in DPN technology, including its application in 3D printing, will also be reviewed, along with its different applications, such as hydrogel production for tissue engineering and biorecognition procedures. This review is significant for researchers and engineers, as it helps them understand the key trends and challenges of the DPN technique, which has the potential to revolutionize integrated circuit production and enable precise, cost-effective fabrication of nanoscale structures.",1,104 | Cobb Galleria Centre,Nanomanufacturing,215
720,6577.0,Process Optimization and Quality Control in Scalable Nanomanufacturing of 2D Semiconductor-based Wearable Sensors,Academician,"Scalable manufacturing of two-dimensional (2D) tellurium nanostructures presents significant opportunities for industrial-scale production of wearable sensors. However, challenges remain in process control and quality assurance. This study combines transfer learning with statistical process control (SPC) to optimize the scaling up of hydrothermal synthesis. Our manufacturing system design focuses on controlling critical quality characteristics, particularly the thickness uniformity of 2D nanostructures. Through design of experiments (DOE), we systematically analyze the effects of key process variables on product morphology and dimensional consistency. The scale-up process revealed significant thickness variations, which we address through process capability analysis and adaptive control strategies. Additionally, we demonstrate a nanomanufacturing approach for high-volume production of tellurene based wearable sensors, supported by in-line quality monitoring and electrochemical characterization. Our findings contribute to the development of robust manufacturing protocols for 2D tellurene-based wearable sensors, addressing critical needs in scalable 2D nanomaterial production as well as future sensing requirements.",2,104 | Cobb Galleria Centre,Nanomanufacturing,215
721,6929.0,"Versatile Bioinspired Interface Engineering for Scalable Nanomanufacturing of Skin-like, Self-Healing Laser-Induced Graphene Wearables",,"Skin-like sensors present a promising alternative to the bulky and rigid medical devices currently on the market. However, overcoming the mechanical mismatch between skin-like materials and traditional electronic components presents significant challenges. Since its discovery, laser-induced graphene (LIG) has been employed in various wearable health monitoring applications due to its facile, scalable manufacturing and excellent electrical properties. However, LIG's fragile, crack-prone nature severely limits its reliable integration into skin-like substrates. For instance, the poor interfacial affinity of LIG with most stretchable materials leads to interfacial stress concentration, often resulting in permanent damage and a severe loss of conductivity under and after large strains. Herein, we propose a facile and versatile manufacturing method to dynamically cross-link LIG to soft substrates through a bioinspired nanostructuring self-assembly route. Our approach provides a flexible pathway to enhance the performance of LIG-based electronics under mechanical stress introducing novel features such as room-temperature self-healability. To illustrate the benefits of this approach, we designed and fabricated a LIG triboelectric sensor showcasing skin-like mechanical properties, breathability, and self-healing with minimal changes in conductivity after tearing and healing. The demonstrated sensor can self-adhere to human skin, seamlessly tracking physiological markers such as heart rate, heart rate variability, and arterial stiffness index. This work demonstrates a unique approach to overcoming the intrinsic limitations of LIG, paving the way for cost-effective and scalable nanomanufacturing of wearable sensors in a wide range of applications.",3,104 | Cobb Galleria Centre,Nanomanufacturing,215
722,6945.0,Design And Scalable Nanomanufacturing Of Nanoscale Oxide Based Wearable Sensors For In-Vehicle Health Monitoring,Practitioner,"This study leverages manufacturing process optimization and design-for-manufacturability principles to develop and evaluate zinc oxide (ZnO) nanostructure-based wearable sensors for continuous pulse monitoring during vehicle operation. Utilizing design of experiments (DOE) and statistical process control methods, we conduct comparative analyses between pure and doped ZnO sensors, evaluating key performance indicators such as signal-to-noise ratio, reliability under varying environmental conditions, and robustness against operational stressors like vibration and temperature fluctuations. The DOE incorporates both bottom-up and top-down manufacturing approaches. Specifically, the ZnO nanowire growth was achieved via a hydrothermal process—a bottom-up method—while device fabrication involved multiple top-down manufacturing steps. By optimizing these manufacturing processes, we enhanced production efficiency and product quality, ensuring scalability for mass production. The fabrication process was designed to be cost-effective and scalable, employing materials and methods conducive to high-throughput manufacturing. The user-centered design approach ensured that the sensors are not only functionally effective but also comfortable for long-term wear, enhancing wearability and user compliance.",4,104 | Cobb Galleria Centre,Nanomanufacturing,215
723,6215.0,Optimizing Prosthetic Socket with Additive Manufactured Structure,Academician,"A prosthetic socket, as one of the critical components of a prosthesis, is responsible for holding the amputee’s residual limb and supporting the patient’s body weight. It is crucial to improve patients’ user experience and quality of life by reducing the sockets’ weight while maintaining strength and flexibility. As a result, additive manufacturing (AM), which can fabricate customized complex structures, is an ideal manufacturing process for the socket. Among the AM printers, Fuse 1 (Formlabs, Boston, MA), using Selective Laser Sintering (SLS) technology and nylon 12 powder, is applied to the production. In addition, replacing solid structures with lattice ones will provide the socket with a high stiffness-to-weight ratio. In order to develop an optimized structure, the lattices are printed and redesigned for several iterations. Additionally, 3-point bending tests are implemented to evaluate the strain and stiffness of the structure. Furthermore, statistical methods are utilized to analyze the data.",1,115 | Cobb Galleria Centre,Computer Aided Manufacturing,216
724,8782.0,Autonomous Robotic Assembly through  AI-based Computer Vision,Practitioner,"As manufacturing tasks become increasingly complex and dynamic, traditional automation solutions struggle to keep up with rapidly changing markets and the demand for highly customized products. Smarter, self-governing manufacturing systems are needed.​ This research aims to develop an autonomous robotic assembly system driven solely by Artificial Intelligence (AI) algorithms and computer vision (CV), reducing human effort, increasing efficiency, and optimizing overall productivity in robotic automated production systems. Utilizing a 6-DOF robotic arm and a vision module consisting of cameras at different angles, this project demonstrates a use case for autonomous robotic assembly. The project has proven the concept of autonomous assembly with the capability to identify multiple objects in basic shapes and create robotic assembly tasks and the corresponding control commands. After executing each task, a quality check is performed in which the vision module detects any mismatch between the expected position and actual position of the assembled part. If a mismatch is detected the vision module issues instructions to the robot to execute corrective action (rework) until the quality check is passed. Furthermore, all the executed tasks and the associated quality data are stored in quality database. This helps in performing root cause analysis for any error generated due to rework or placement of the parts. This work demonstrates the feasibility of autonomous robotic assembly, contributing to advancements in smart manufacturing and Industry 4.0.",2,115 | Cobb Galleria Centre,Computer Aided Manufacturing,216
725,6792.0,Robot Manipulation via Action Decomposition and Composition,Academician,"The ability to efficiently acquire generalized skills from demonstrations and apply them across diverse real-world scenarios is a central challenge in robot manipulation. Unlike conventional robot learning tasks that rely on extensive action demonstrations for single-task performance, zero-shot manipulation demands that robots leverage multiple learned skills to accomplish novel tasks. In this work, we propose an action decomposition and composition framework designed to efficiently transfer foundational manipulation skills to various new derivative tasks. Our approach first decomposes a demonstration into fundamental skills such as foundation movement and rotation, which can then be recombined and applied to a new task not present in the initial training. Using an action prediction model, we generate potential manipulation poses, including trajectory poses and other potential interactive poses, for each subtask within the derivative task, guided by both robot actions and video frames captured from the robot’s cameras. To ensure accuracy, we further refine these generated poses by filtering out misleading actions and selecting the most probable manipulation poses to guide the robot effectively. Experimental results on hotdog preparation task demonstrate that our framework can successfully enable both robotic arms and automated guided vehicles to perform derivative tasks, showcasing its versatility and efficiency.",3,115 | Cobb Galleria Centre,Computer Aided Manufacturing,216
726,8618.0,Enhanced Characterization of Elastoplastic Properties of Additively Manufactured Specimens Using Stochastic Inverse Modeling of Indentation Data,,"Rapid characterization of mechanical properties and material structure of additively manufactured (AM) components via non-destructive techniques (NDT) is crucial for their wider adoption. However, accurate characterization of AM components using NDT remains a challenge. To address this, our work focuses on characterizing the elastoplastic properties of AM components from instrumented indentation measurements, addressing the inverse indentation problem. Previous approaches to this problem have limitations in generalization and in estimating the variability of elastoplastic properties. In this work, we explore a stochastic inverse problem (SIP) formulation, estimating a distribution over elastoplastic properties—Young’s modulus (E), yield strength (σy), and strain hardening exponent (n)—that aligns with observed indentation data. By implementing this methodology for AM components subjected to different heat treatments, we achieve predictions of n, E, and σy within 1.1%, 1%, and 5% of the actual values, respectively. The recovered distributions closely match those obtained from standard tensile tests, indicating our methodology’s accuracy in characterizing mean elastoplastic properties and their variability through high-throughput indentation measurements.",4,115 | Cobb Galleria Centre,Computer Aided Manufacturing,216
727,6497.0,The influence of hot isostatic pressing process on the fatigue life of EB-PBF printed Ti-6Al-4V.,Academician,"In the electron beam powder bed fusion (EB-PBF) process, Ti-6Al-4V (Ti64) is a promising material used in additive manufacturing. This study investigates the influence of the hot isostatic pressing (HIP) process on the fatigue life of EB-PBF printed Ti64. The samples are printed with EB-PBF (A2X, Colibrium), and half of the samples are HIP treated, following the ASTM-F2924-14 standard. The HIP treatment was done by Quintus using their HIP furnace. All samples are CNC machined to ASTM specs for fatigue testing using an INTEGREX I-100ST by Mazak. The samples are tested using a rotating beam fatigue setup with the samples loaded in four-point bending using the eXpert 9300 by ADMET. Samples are tested at three different stress levels. The comparison of material porosity, microstructural heterogeneities, defects, and associated mechanical properties among the two groups are used to study the influence of the HIP process on 3D-printed Ti64.",1,115 | Cobb Galleria Centre,Material Characterization,217
728,5980.0,Early-Stage Lifetime Prediction for Semiconductor Products during the Design Stage,Academician,"Predicting the lifetime of semiconductor products early in the design phase is essential to ensuring reliability and optimizing development cycles. This study presents a method for early-stage lifetime prediction for semiconductor products using finite element analysis (FEA) at the chip-design stage. By modeling thermal and mechanical stresses that components undergo during use, this approach identifies potential failure mechanisms and estimates product durability well before physical testing or production. The FEA model incorporates material properties, design parameters, and operational stressors, offering detailed insights into regions prone to failure. Our methodology demonstrates high accuracy in predicting product lifespan. The findings highlight that FEA-based lifetime predictions can reduce costs and design time by identifying vulnerabilities early in the development process, leading to improved yield rates and reliable performance. This work advocates for integrating FEA-driven predictive analytics in semiconductor design, aiming to establish these techniques as a standard for early reliability assessment. The proposed model supports accelerated design iterations and provides a foundation for embedding predictive lifetime assessments within automated design workflows, ultimately contributing to more robust and dependable semiconductor products.",2,115 | Cobb Galleria Centre,Material Characterization,217
729,6495.0,Transfer learning framework for fatigue life prediction of additively manufactured parts,Academician,"Additive manufacturing has the potential to make things more efficient because of its ability to build customizable products quickly and with fewer resources required. The issue that additive manufacturing faces is the inferior performance relative to conventionally manufactured parts in the long run, thus hindering their adoption in mission-critical applications. The main cause of this inferior performance in as-built parts is surface roughness, which can not be avoided due to process physics. The deep valleys in the surface act as stress concentration areas that become crack initiation sites, thus leading to part failure. The scarcity of fatigue testing data compounds this problem, as fatigue tests are expensive in terms of time and cost. Transfer learning is a machine learning technique used to handle the problem of smaller datasets for training by leveraging the model learned for a related task and using it for the target task. This work aims to use the transfer learning technique and fatigue test data to build a framework that learns from the surface roughness data and makes predictions about fatigue life. This framework can potentially solve the data scarcity problem in studying the fatigue behavior of additively manufactured parts, thus facilitating their adoption in a wide spectrum of applications with confidence.",3,115 | Cobb Galleria Centre,Material Characterization,217
730,8856.0,Assessing the Impact of Laser Characteristics on Direct Energy Deposition through In-Situ Spectroscopic Monitoring,Academician,"The multiphysics nature of Direct Energy Deposition (DED), involving complex interactions among thermal, optical, and material phenomena, drives advancements in metal additive manufacturing but also presents challenges in understanding the underlying mechanisms that govern the process quality and stability. Current research efforts focus on realizing real-time monitoring of process parameters and detecting anomalies such as in melt pool size, shape, and temperature, and the development of corresponding closed-loop control strategies. On a systems scale, ex-situ data such as micro-morphology and mechanical properties have been integrated into physics-based or data-driven models to establish the process-property relationship. We hypothesize that laser characteristics also have significant effects on melt pool formation and evolution, and in-situ laser signature monitoring allows us to reveal the interaction between laser and metal materials. However, this optical perspective remains underexplored due to limitations in effective laser monitoring techniques. In this study, we employed a versatile laser spectroscopy approach to monitor the intensity of incident laser beams across a broad range of wavelengths, aiming to identify the material melting and deposition behaviors under varying printing conditions. We will present the laser spectra and their implications for laser-material interactions, offering new strategies to optimize laser parameters and enhance process quality and stability.",4,115 | Cobb Galleria Centre,Material Characterization,217
731,6665.0,Bioprinting Collagen-based Hydrogel Using Digital Light Processing (DLP) for Cultivated Meat Applications,Academician,"The challenges associated with traditional livestock farming and the growing global demand for sustainably sourced nutritious food have accelerated research into alternative proteins such as cultivated meats. Achieving scalability and biofunctionality (e.g., taste, texture, nutrition) in cultivated meat production requires advancements in biomanufacturing technologies. Toward this, this study investigates Digital Light Processing (DLP) bioprinting of a new edible hydrogel formulation (8 mg/mL type I collagen with 0.1% riboflavin) for cultivated meat scaffolds. In particular, rheological, mechanical, and biological assessments were performed to validate the collagen-riboflavin hydrogel’s suitability for these applications. First, key rheological properties, storage modulus (G'), loss modulus (G''), and loss tangent (tan δ), were measured across a strain range of 0.01–500%, which demonstrated the gel’s enhanced stability under deformation. Next, the mechanical properties were assessed to evaluate the gel’s structural stability. Results indicated that the DLP-printed hydrogel exhibited higher compressive modulus and maximum stress (p<0.05) than collagen-only samples, which were in the range relevant for natural meat. Finally, the ability of the bioprinted hydrogel to support cell viability and proliferation over 4 days in culture was assessed via Live/Dead and alamarBlue assays. Both assays highlighted significant increases in fibroblast cell viability, cell density, and live cell area (all p<0.05). Taken together, these results demonstrate the potential of DLP bioprinting of collagen-riboflavin hydrogel as an efficient and scalable option for producing cultivated meat towards advancing cellular agriculture and sustainable food production solutions.",1,115 | Cobb Galleria Centre,Biofabrication,218
732,6635.0,3D Bioprinted Approach to Biofilm-based Corrosion Control Coatings,Academician,"This research proposes a 3D bioprinted biological coating system structurally designed to effectively inhibit corrosion on metal surfaces. Corrosion poses a significant economic burden, costing the United States nearly half a trillion dollars annually. Traditional chemical coatings used for corrosion mitigation are costly, environmentally hazardous, and often require frequent reapplication. Microbiologically influenced corrosion inhibition (MICI) presents a promising solution, as biofilms can provide protective barriers through self-healing and bioactive properties. However, translating MICI into a practical, scalable application requires precise manufacturing techniques that maintain biofilm integrity and functionality. Layer-by-layer biofilm prototypes will be developed using extrusion-based 3D bioprinting with a focus on refining bioink composition, adjusting factors like hydrogel viscosity, bacterial cell density, and extracellular polymeric substance (EPS) concentration to achieve high print fidelity and structural stability. Prototyping efforts are centered on ensuring that printed biofilms adhere well to steel surfaces and retain their corrosion-inhibiting properties over time. Results from initial prototypes indicate that printed biofilms can reduce corrosion rates on steel by over 50% and maintain their structure and function in accelerated corrosion tests for up to eight weeks. This study demonstrates that 3D bioprinting can provide a scalable platform for biofilm-based corrosion inhibitors, offering a viable, environmentally friendly alternative for industrial applications. In conclusion, this research advances the use of 3D bioprinting in developing biofilm coatings and opens pathways for broader adoption of MICI in corrosion control across infrastructure and manufacturing sectors.",2,115 | Cobb Galleria Centre,Biofabrication,218
733,6672.0,Assessment of bovine collagen manufactured via novel cellular agriculture technology for biomedical applications,Academician,"Cellular agriculture is revolutionizing protein production by enabling lab-made alternatives to typical animal-derived biomaterials to address environmental concerns and safety challenges associated with traditional animal farming. Lab-made collagen, in particular, holds great potential for applications in the biomedical and food industries. This study characterizes, for the first time, key properties of collagen produced via mammalian cellular agriculture. Briefly, a bovine fibroblast cell line optimized for collagen production was cultured in a scalable bioreactor system to produce collagen (>85% type I collagen), which was then extracted and purified. This collagen was lyophilized, reconstituted at 10 mg/mL, and molded into self-assembling fibrillar hydrogels (⌀8×4 mm) at neutral pH and 37°C. Traditional animal-derived bovine collagen hydrogel served as a benchmarking control. Mechanical tests (n = 5/group) showed that the lab-made collagen hydrogel possessed significantly higher compressive and dynamic moduli (both p<0.05) than the control, indicating superior structural integrity and shape retention. Surface characterization showed that the lab-made collagen possessed similar hydrophilicity (contact angle = 44°) as native collagen. Fibroblast cells encapsulated in the lab-made hydrogel responded favorably over 7 days in culture, indicated by increases in cell density from 320 ± 34 cells/mm² to 907 ± 14 cells/mm² and average live cell area from 1.21 ± 0.26 mm² to 3.04 ± 0.39 mm² via Live/Dead image analysis. This comparative analysis provides early insights into the functional equivalency of lab-made collagen and will inform the optimization of the cellular agriculture production process and the design of application-specific hydrogel formulations in future.",3,115 | Cobb Galleria Centre,Biofabrication,218
734,6677.0,A Review and Perspective on Smart Biomanufacturing Strategies for Tissue Engineered Knee Meniscus,Academician,"Menisci serve a critical function in the knee joint by maintaining mechanical stability, distributing weight, and absorbing shock to protect the articular cartilage, thereby promoting flexible movement. Incidences of injuries and degradation resulting from joint disorders of these load-bearing tissues are highly prevalent, and can be attributed, in part, to their complex multiscale architecture that is critical to the biomechanical function and their limited healing ability. Current clinical treatments are not optimal, which had led to extensive research in the development of tissue-engineered solutions in recent years. Herein, we review recent advancements in 3D biofabrication strategies for functional meniscus scaffolds, highlighting primary material-process-structure considerations and interactions. Furthermore, based on a review of digital tools used in the broader biomanufacturing literature, we provide perspectives on incorporating digital tools including computational modeling, artificial intelligence/machine learning, and digital twins toward optimizing meniscus biofabrication. This work will inform the critical next steps in engineering smart, efficient, and scalable manufacturing solutions to accelerate the development and translation of clinically viable tissue-engineered solutions for meniscus repair and replacement.",4,115 | Cobb Galleria Centre,Biofabrication,218
735,4828.0,Real-time process monitoring and closed-loop control in directed energy deposition via a coaxial photodiode array,Academician,"In-situ monitoring is crucial in developing control strategies, novel materials processes, tool path design, and a fundamental understanding of laser, powder-blown directed energy deposition (DED). Despite the flexibility of DED in geometries and materials, fabricating parts with uniform quality is still challenging due to localized processing variations. This study establishes an in-situ, parallel monitoring framework for DED, employing a coaxial photodiode array to capture relative melt pool temperature data, laser light, and visible-range melt pool emissions. The coupling between laser absorption and material emission was able to be characterized by correlating photodiode signals with process parameters and spectrometer data. A subset of the photodiodes was integrated into a closed-loop control system to maintain a desired relative melt pool temperature throughout fabricating a single part. This approach mitigates heat accumulation across various build geometries, which can address non-uniform microstructural features within the manufactured component.",1,115 | Cobb Galleria Centre,Process Monitoring 1,219
736,6063.0,Predicting Process Parameters with In-situ Image Analysis and Machine Learning for Fused Filament Fabrication Processes,Academician,"Process parameters are important in additive manufacturing (AM) for the quality assurance of final parts. For example, fused filament fabrication (FFF) is a widely used AM process for product prototyping, and its product quality is highly sensitive to various process parameters, including extruder and print bed temperature, extruder print speed, and various infill pattern related parameters. On the other hand, in-situ imaging contain various product/process design information, especially process parameter and printing path information. However, there is still a fundamental knowledge gap about to what extent in-situ imaging could possibly disclose AM product and process design information for reverse engineering purposes. This study generates in-situ image data from a large assortment of parts that systematically vary multiple process parameters such as print speed, print temperature, and multiple infill pattern related parameters. These images are subsequently used to train image processing and machine learning models, and their effectiveness of reversely predicting various process parameters are evaluated. The proposed method can be extended to other sensing capabilities and other AM processes to establish a holistic framework of characterizing the release of AM product/process design information, providing insights to privacy preservation in in-situ additive manufacturing process monitoring.",2,115 | Cobb Galleria Centre,Process Monitoring 1,219
737,6591.0,Digital Image Correlation-Based Non-Destructive Defect Inspection of Additive Manufactured Metamaterials,Academician,"Metamaterials are artificially engineered structures constructed by unit cells and are designed to have unique mechanical, thermal, and electromagnetic properties. Therefore, they have a broad application in the mechanical, aerospace, and medical industries. However, due to their complex structures, their quality inspection can be challenging as internal defects, such as a missing beam due to design error or cyber-attack can only be observed by x-ray imaging, which can be costly and time-consuming. This study investigates a machine-learning-assisted digital image correlation approach as a non-destructive evaluation technique for identifying defects in metamaterials. Digital image correlation is an image-based strain field measurement technique. By tracking the speckle painted on the part, this method can detect deformation or movement. When under controlled load, If the part being examined has a defect, it will have different deformation compared to the standard part. A machine learning model has been trained to take the observed strain field as input and predict the location and the size of the defects. Preliminary results demonstrate that the proposed machine learning-assisted method can effectively identify various types of defects, such as in-complete beam connections and missing cells, and their locations, providing a cost-effective and rapid non-destructive method for 3D printed metamaterials.",3,115 | Cobb Galleria Centre,Process Monitoring 1,219
738,6771.0,Explainable Vision Transformer-Based Real-Time Anomaly Detection for Fused Deposition Modeling,,"Fused Deposition Modeling (FDM) is one of the most widely used additive manufacturing (AM) techniques, especially in consumer-grade 3D printing. However, it suffers from a lack of real-time quality assessment and process control, leading to common print defects such as underprinting, overprinting, and inconsistencies in surface quality. This research proposes the integration of Vision Transformers (ViTs) to enhance real-time anomaly detection in the FDM process using 2D laser scan depth maps. A high-speed KEYENCE LJ-V7000 laser profiler scans the printed surface layer-by-layer, producing depth maps that are analyzed by the ViT model to identify surface defects. The model classifies the surface into four categories: underprinting, overprinting, normal, and empty regions. By utilizing the self-attention mechanism of Vision Transformers, the model captures subtle spatial dependencies and detects complex anomalies that traditional methods might miss. To improve interpretability, we incorporate explainability techniques like attention maps and Grad-CAM to visualize which parts of the depth map contribute to anomaly detection, offering transparency and actionable insights for operators. Our model achieves state-of-the-art (SOTA) performance in anomaly detection, outperforming traditional methods in accuracy and robustness. The proposed solution can improve defect detection accuracy and enable real-time process adjustments based on detected anomalies, enhancing the quality and reliability of FDM printing. This work demonstrates the potential of Vision Transformers for advancing data-driven quality control in additive manufacturing.",4,115 | Cobb Galleria Centre,Process Monitoring 1,219
739,6854.0,Manufacturing Security: Simulating Attacks in a Cyber-Physical Manufacturing Range,Academician,"As manufacturers increasingly adopt automation and other advanced technologies, manufacturing equipment becomes more vulnerable to cyber-physical attacks. Cyber-physical attacks leverage cyber and physical pathways to compromise machine components, leading to potential system-wide infection and physical damage. Due to many unknowns about these attacks and their effects, manufacturers need a controlled and safe environment to research the different possibilities. A Cyber-Physical Manufacturing Range consisting of both a virtual and physical component is key to demystifying the potential consequences of attacks. This presentation will show how the virtual component of a Cyber-Physical Manufacturing Range can be used to demonstrate an attack. This virtual component consists of a Digital Twin Ecosystem built using a game engine platform and a multi-fidelity security layer added to that Digital Twin. Incorporating varying degrees of granularity enables the Digital Twin to capture different types of attacks. By replicating attack effects, researchers can begin to understand the various attack vectors and create defense mechanisms based on this knowledge, leading to a safer and more secure manufacturing environment.",1,116 | Cobb Galleria Centre,Cybermanufacturing,220
740,8969.0,Privacy Preserved Melt Pool Data Sharing in Metal Additive Manufacturing Processes,Practitioner,"Collaborative process-defect modeling can significantly advance the development of generalized, reliable machine learning models for anomaly detection and defect prediction in metal additive manufacturing processes. However, sharing data, such as melt pool images, raises concerns over data privacy and Intellectual Property (IP) protection. This study addresses these challenges by proposing two privacy-preserving mechanisms for sharing melt pool images: (1) binary segmented melt pool images and (2) binary segmented images with random rotation. To prevent scan pattern leakage, melt pool images are shared in randomized order under both mechanisms, ensuring spatial analysis capabilities without compromising privacy. To evaluate these mechanisms, two privacy attack models, defined as Scan Pattern Recognition Attacks (SPRA), were developed: SPRA-PP (plume-position-based) and SPRA-MF (motion-feature-based). While SPRA-PP identifies scan directions from the melt pool images in over 92% of cases, demonstrating privacy leakage from raw melt pool image data sharing, it fails when the first privacy mechanism is implemented. Conversely, SPRA-MF defeats the first privacy mechanism and identifies the scan direction in over 98% of cases. However, the second, more conservative privacy mechanism is effective against both attacks, with SPRA-MF's success rate dropping from 98% to approximately 60% when the second privacy mechanism is implemented. Our findings also reveal an inverse relationship between data utility and privacy retention, with the second mechanism maintaining above 80% of data utility. The proposed data-sharing approach can facilitate secure and collaborative LPBF research, advancing additive manufacturing by balancing data privacy with innovation.",2,116 | Cobb Galleria Centre,Cybermanufacturing,220
741,9084.0,Assessing Cyberinfrastructure Resilience in Manufacturing Organizations,Practitioner,"Manufacturers are experiencing more frequent cyberattacks causing costly operational shutdowns and cyber incident remediation costs impacting businesses. Of significant concern are cyber-physical systems in industrial environments that comprise the U.S. critical infrastructure. The U.S. federal government recently published a strategy to increase cyber-physical resilience focusing on designing cyberinfrastructure to withstand cyberattacks. Industry organizational leaders report sensing that increased regulatory pressure is on the rise. While regulation can be a driving force for security investments, there is still much work to be done in formal assessment frameworks, methodologies, and measurement tools to characterize compliance or to systematically rate an organization’s cybersecurity posture or cyberinfrastructure resilience. The purpose of this study is to move toward a framework for assessing cyberinfrastructure resilience in manufacturing organizations. A two-step approach will include: Investigating current frameworks, methods, and tools for assessment and measurement from information systems, reliability engineering, and systems engineering literature. The goal of this investigation is to consider what aspects from these more established contexts might be transferrable to industrial cybersecurity, particularly manufacturing. Investigating manufacturing organizational performance measurement and management, seeking opportunities to integrate cyberinfrastructure resilience into these methods. A framework for assessing cyberinfrastructure resilience should be holistic, considering both business and technology perspectives. This study is meant to be a starting point toward measurement frameworks to support understanding and reporting industrial organization performance and progress toward cyberinfrastructure resilience and their ability to report their status to regulatory agencies in the future.",3,116 | Cobb Galleria Centre,Cybermanufacturing,220
742,5766.0,Characterization and Applications of a New Zn-Al Alloy,Academician,"Zn-Al alloys are widely utilized in various applications due to their advantageous properties, including excellent corrosion resistance, good mechanical strength, and ease of manufacturability. These alloys have become essential in industries such as automotive, aerospace, and electronics. However, many variants, particularly certain Zamak alloys, remain underexplored, limiting the understanding of their full potential. This study introduces a previously uninvestigated Zn-Al alloy, characterized by a composition of approximately 82% Zn and 15-16% Al. An evaluation of the mechanical properties and characteristics of this alloy is conducted to determine its performance in practical applications. A brief comparison is made with similar Zn-Al alloys to highlight differences in their mechanical behaviors. Additionally, the study discusses the typical applications of this novel alloy, emphasizing its potential to meet the demands of various industries and contribute to advancements in material science and engineering.",1,102 | Cobb Galleria Centre,Manufacturing of Alloys,221
743,5812.0,"Surface Roughness and Material Removal Analysis for Long-Stretch, Ultra-Precision Polishing",Practitioner,"Ensuring consistent and reliable high-yield inertial confinement fusion (ICF) requires precise control over fuel capsule quality, with an emphasis on minimizing surface defects, achieving accurate dimensions, and optimizing surface roughness. At Lawrence Livermore National Laboratory (LLNL), the polishing process for preparing fuel capsules to be used in controlled nuclear fusion experiments is a long-stretch, ultra-precision process. Each polishing stage takes approximately 24 hours, and 7 to 10 stages are used to achieve surface smoothness down to tens of nanometers. This study presents a significant advancement in surface roughness prediction through the application of a functional regression model to vibration data, addressing the critical need for precision in this multi-stage polishing process. The proposed data-driven approach enhances decision-making processes, enabling a 50% reduction in polishing time—saving over 10 hours during a 24-hour polishing cycle. Additionally, we employ an analysis of variance (ANOVA)-based framework to model material removal rates by correlating key process parameters, further optimizing the efficiency of the lapping and polishing process.",2,102 | Cobb Galleria Centre,Manufacturing of Alloys,221
744,6002.0,Optimizing steel bar selection and allocation for turning processes in automotive gear manufacturing,Practitioner,"This work addresses the critical challenge of reducing material waste in the turning process of gear manufacturing, where material is removed from steel bars to achieve a gear diameter. Currently, N part numbers with varying diameters are produced from M fixed types of steel bars. The allocation of part numbers to specific bar types has been determined empirically, leading to suboptimal material utilization. The objective is to optimize the selection of steel bar diameters and the allocation of part numbers to minimize material waste. The steel bar supplier has agreed to provide custom bar diameters, subject to the condition of meeting a minimum consumption. A Mixed Integer Linear Programming (MILP) model was developed to address this problem. While optimal solutions can be obtained for small problem instances, defined as (N,M) where N represents the number of parts and M the types of steel bars, the computational complexity increases significantly for larger instances. The model prescribes decisions such as the optimal number and diameters of steel bar types, as well as the allocation of part numbers to each bar type. Preliminary results, using CPLEX solver, indicate a 12% reduction in material waste for small to medium-sized instances up to (N, M) compared to the current allocation. For larger instances, ranging from (N=40,M=15) a constructive heuristic method was employed, allowing the model to efficiently handle real-world problem sizes. Finally, this study highlights the strategic value of combining exact optimization techniques with heuristics to enhance efficiency and sustainability in manufacturing processes.",3,102 | Cobb Galleria Centre,Manufacturing of Alloys,221
745,8908.0,Advancing Sensor-Driven Maintenance: An Attention-Based Solution for Leased Manufacturing Systems,Academician,"Optimizing operations and maintenance (O&M) in leased manufacturing systems is a complex challenge due to the intricacies of managing dynamic industrial processes, diverse equipment standards, and geographically dispersed sites with varying cost structures. Integrating sensor-driven predictive maintenance into these systems further increases the complexity, as the dynamic nature of the environment requires solutions to adapt to changing operational conditions and external circumstances. Maintenance scheduling must be recalculated frequently to reflect real-time data, necessitating solutions that are both highly adaptive and computationally efficient. This makes solving the problem challenging, as it must deliver accurate and feasible schedules quickly to minimize disruptions and maintain productivity. This presentation introduces an attention-based framework to address these challenges in leased manufacturing systems. By leveraging real-time sensor data, the framework optimizes O&M by generating cost-effective maintenance schedules, coordinating crew routes efficiently, and minimizing production disruptions. The model uses a multi-head attention mechanism to capture essential temporal and non-temporal features, ensuring scalability and adaptability for large-scale, real-world applications. Key contributions include: (1) integrating sensor-driven prognostic insights into maintenance planning, (2) embedding operational constraints and objectives directly into the attention-based architecture for feasibility and optimality, and (3) enabling end-to-end learning for robust performance. Computational experiments demonstrate the framework’s ability to adapt quickly to changing conditions, improve decision-making, reduce costs, and increase system reliability. This work highlights the transformative potential of attention mechanisms and advanced machine learning methods in addressing complex, fast-paced scheduling and operational challenges in leased manufacturing systems.",1,102 | Cobb Galleria Centre,Process Optimization 1,222
746,6543.0,Process adaptable bio-compatible polymer inks enabling scalable manufacturing of self-powered triboelectric sensors,,"Triboelectric nanogenerator have been developed to create self-powered devices for power generation or sensing applications. There are many sensor applications for triboelectric nanogenerators in systems where there may be any residual mechanical energy that would have otherwise been unutilized. One such system is the human body; hence, a prominent application of triboelectric sensors is skin-integrated health monitoring sensors. Due to the self-powered, easy to operate and bio-compatible nature of these sensors, there is growing interest in more widespread adoption of these sensors in real-world applications. This can be achieved by developing new methods of enhancing the triboelectric performance and investigating reliable and scalable manufacturing of these sensors. Moreover, there is a need for development with a holistic approach combining device performance and scalable manufacturing. Here we present a novel water-based ink formulated for skin-integrated, highly sensitive triboelectric sensors. The ink synthesis utilizes non-toxic, bio-compatible polymers that enable safe and direct skin contact, ensuring reliability in triboelectric output. With tunable viscoelastic shear-thinning properties, the ink can be optimized for various printing techniques, allowing for the production of uniform, thin films with minimal defects. Additionally, the ink features recoverable cross-linking, which ensures it reverts to its original structure once the shear forces applied during the printing process are removed. This design framework provides a versatile, safe, and efficient approach for fabricating next-generation wearable sensor systems.",2,102 | Cobb Galleria Centre,Process Optimization 1,222
747,6388.0,Binder jetting of ceramic monolithic adsorbents for carbon capture applications,Academician,"Global warming, driven by greenhouse gas emissions, has led to rising temperatures, more frequent natural disasters, environmental disruptions, and economic strain. CO₂ accounting for 76% of greenhouse gases, is the primary culprit. Reducing atmospheric CO₂ through carbon capture can mitigate this crisis. Ceramic materials, particularly zeolites, offer a promising alternative to traditional amine-based absorbents, as they require less energy for regeneration and are potentially more cost-effective. While ceramic materials have versatile applications, conventional manufacturing of complex shapes is costly, largely due to tooling expenses. Additive manufacturing (AM), particularly binder jetting, can reduce these costs by eliminating tooling. Binder jetting involves spreading powder layers and spraying binder to build a part, layer by layer, until complete. This process supports complex geometries, minimizes binder usage, and holds a strong potential for large-scale production. Binder jetting enables the creation of interconnected pore structures in ceramics, allowing efficient CO₂ adsorption. In this research, zeolite monolithic adsorbents were fabricated through a binder jetting AM process. A gyroid structure was successfully printed. A compressive strength of 1.06 MPa was achieved on green parts. Following printing, the parts were debound at 470°C and subsequently sintered at 750°C, achieving a strength of 0.45 MPa. The sintered samples demonstrated excellent CO₂ adsorption capacities although the adsorption performance did not fully match that of raw zeolite powder. These findings indicate the potential of binder jetting to produce structurally robust, porous zeolite adsorbents for carbon capture applications, with opportunities for further optimization to enhance adsorption performance.",3,102 | Cobb Galleria Centre,Process Optimization 1,222
748,6042.0,Reinforcement Learning-Based Smart Toolpath Planning for Optimized Machining Performance,Academician,"Smart machining toolpath planning is crucial for reducing machining time, tool wear, and energy consumption. Conventional toolpath generation strategies are based on predefined operation templates and standard contouring and infilling algorithms that focuses solely on creating the geometry, which is undoubtably the primary priority. However, while existing algorithms are offering more sophisticated toolpaths with more manufacturing capabilities, other secondary priorities such as the cycle times and energy consumptions are normally ignored during toolpath generation. There is a great potential of creating smarter toolpaths that can achieve both the primary and secondary priorities in a manufacturing operation. This study proposes a novel toolpath planning method using reinforcement learning (RL), wherein the toolpath will be adaptively generated according to the geometry of the machined component. An RL agent is trained so it can interact with a simulated machining process environment to obtain optimal strategies for toolpaths across a wide variety of shapes and levels of complexity. The agent will develop a strategy that optimizes both linear and rotational motion to minimize the process time and/or energy while maintaining an acceptable geometrical accuracy. This work seeks to explore a unique approach for toolpath generation, resulting in a more efficient and accurate machining process, especially for 5-axis machining of intricate part geometries. As a preliminary study, this work reduces the complexity of the 5-axis environment to a 2-axis of linear motion with 1 axis of rotational motion environment and has proven the feasibility of applying RL for multi-axis machining toolpath planning.",1,102 | Cobb Galleria Centre,Process Optimization 2,223
749,6349.0,Selective Wave Soldering Parameter Optimization for PCB Assembly,Academician,"Selective wave soldering, a process in electronics manufacturing, encounters issues such as insufficient barrel fill and solder bridging, which necessitate rework and touchup, impacting productivity. This study addresses these problems by optimizing the process parameters of selective wave soldering equipment using a design of experiment approach. This research concentrates on identifying key process parameters that affect selective wave soldering with SAC305 solder on a 1.6 mm thick 10-layer board. Key response variables that would impact the yield and reliability of the product, like insufficient barrel fill and solder bridging, were considered, and the results were analyzed using the Analysis of Variance (ANOVA) approach. The methods involved precise control of flux application, optimized solder wave parameters, and advanced motion control to ensure consistent and high-quality solder joints. The research aimed to find a global solution that would accommodate Printed Circuit Boards (PCBs) with similar thicknesses, pitch, and plated through-hole (PTH) construction. The results demonstrated a low defect rate and rework requirements, leading to enhanced productivity and improved reliability of electronic assemblies.",2,102 | Cobb Galleria Centre,Process Optimization 2,223
750,9117.0,Linear Mixed Model for the Surface Roughness Prediction in Hard Turning Operation,Academician,"Surface machining using hard turning is an intricate operation due to the influence of multiple machining parameters, their non-linear interactions, and the inherent variability introduced by different experimental trials. This study proposes a Linear Mixed Model (LMM) for predicting surface roughness, effectively addressing the challenges in traditional linear models, posed by the influencing factors, non-linearity, and interactions. The LMM incorporates variability from both fixed effects, such as cutting parameters (feed rate, depth of cut, and cutting speed), and random effects arising from tool wear across experimental runs. As a result, it provides a more comprehensive understanding of how these factors interact to impact surface roughness. The model’s performance is evaluated using the Akaike Information Criterion (AIC), with results demonstrating that the LMM outperforms conventional linear models by achieving lower AIC values, indicating better fit and predictive capability. Validation technique such as cross-validation, is employed to assess the model’s reliability. The findings highlight the importance of accounting for random effects due to multiple machining trials and complex interactions in surface roughness prediction, offering a framework for more accurate and reliable modeling in hard-turning operations.",3,102 | Cobb Galleria Centre,Process Optimization 2,223
751,6791.0,Hierarchical Long-horizon Manipulation with Failure Recovery,Academician,"Developing a backtracking solution based on executed tasks when unexpected failures occur offers a promising strategy for assisting robots in completing lengthy and complex tasks. However, challenges remain in dynamically managing manipulation planning for long-horizon tasks and determining effective backtracking strategies upon failure. We introduce an interactive task planning system integrated with a hierarchical task planning agent to support long-horizon robotic manipulation. The system's key capabilities include (1) real-time failure detection based on a vision-language model that learns from failure cases as the robot becomes more familiar with the task, and (2) generating suitable failure recovery strategies based on hierarchical manipulation planning for extended tasks and human-robot interaction. The hierarchical task planning agent utilizes a tree-structured planning model to dynamically adjust the next execution plan. We validate the system through two demanding long-horizon tasks—a vehicle chassis assembly and a hot dog serving task. Experimental results show that the proposed system successfully generates backtracking strategies, enhancing task completion rates for long-horizon objectives.",4,102 | Cobb Galleria Centre,Process Optimization 2,223
752,6902.0,Generative model-based super-resolution framework for improved defect detection in LPBF process,Academician,"Recent technological advancements have positioned metal additive manufacturing, particularly laser powder bed fusion (LPBF), as a transformative technology in high-value industries. Despite its potential, challenges in process reliability and reproducibility continue to impede its widespread industrial adoption, underscoring the need for robust in-situ monitoring systems for quality assurance. Although high speed camera-based monitoring systems show promise, the inherent constraints of the LPBF environment and prohibitive costs of advanced sensors often require the use of lower-specification cameras. This limitation in image acquisition significantly impacts the data resolution and thus its applications, particularly in artificial intelligence-based defect detection. This study proposes a novel generative model-based framework comprising enhanced super-resolution and defect detection models. The proposed super-resolution model improves layer-wise images collected from the built-in camera of a commercial LPBF machine by leveraging defect-specific textural features during training. Using the improved images, a subsequent defect detection demonstrates robust performance in identifying process anomalies. Comprehensive quantitative and qualitative analyses are performed to validate the effectiveness of the proposed approach. The results demonstrate substantial improvements in both image resolution and defect detection accuracy, suggesting promising implications for advanced process monitoring and control within LPBF systems.",1,115 | Cobb Galleria Centre,Process Monitoring 2,224
753,7101.0,Towards Reliable Monitoring of Laser Powder Bed Fusion Additive Manufacturing: Robust Image Processing Method for Melt Pool Segmentation,Academician,"Laser Powder Bed Fusion (LPBF) additive manufacturing is used to fabricate complex, lightweight, and customized parts with high precision, making it ideal for mission-critical applications such as aerospace and healthcare. Monitoring the characteristics of the melt pool, created during the laser-melting of metallic powders, is crucial for ensuring the quality of printed parts. In doing so, melt pool images must be accurately segmented first to reliably extract and monitor the melt pool’s morphological features, which are essential for downstream analysis to detect process anomalies and identify potential defects such as lack of fusion and keyhole porosities. However, due to the dynamic nature of LPBF, melt pool images are noisy and stochastic in nature challenging conventional image segmentation methods, such as Otsu, thresholding, k -means clustering, and active contour, which lack robustness under these conditions. In response, we developed a robust multi-step image processing framework that strategically integrates edge filtering, morphological operations, and adaptive smoothing to effectively segment the melt pool region. This methodology is specifically designed to address the unique challenges posed by noise and variability in LPBF melt pool images, offering a more reliable solution compared to conventional segmentation techniques. We evaluated our method using three datasets, including both in-house and public sources, and compared its performance to conventional methods. Our method consistently achieved high segmentation accuracy (~90%), when compared to ground-truth segmentation done by expert annotators, outperforming traditional methods. This robust segmentation method will enable reliable monitoring of LPBF processes, facilitating improved anomaly detection and process optimization.",2,115 | Cobb Galleria Centre,Process Monitoring 2,224
754,8830.0,Real-Time Predictive Maintenance with Edge-Orchestrated Multi-Agent Pipelines,Academician,"Emerging technologies such as artificial intelligence, cloud-based infrastructures, and edge computing offer highly scalable solutions in modern manufacturing applications. This paper presents an innovative Machine Learning Function Orchestrator that seamlessly integrates with edge computing and multi-agent systems. Machine Learning Function Orchestrator features two key components: MAS pipeline manager and service management system. The MAS pipeline manager is responsible for the coordination and deployment of multi-agent systems. The Service Management component gathers data from MAS agents and deploys performance supervisor service for each MAS pipeline. These components work together to dynamically deploy, configure, and manage Multi-Agent System (MAS) pipelines to enable efficient coordination and real-time control of distributed multi-agents. Machine Learning Function Orchestrator integrated with Open Horizon, an open-source decentralized edge computing framework is used to facilitate the deployment and reconfiguration of Multi-Agent System pipelines. The framework is demonstrated in a predictive maintenance use case using visual inspection with multiagent-based control. The edge agents keep track of equipment status, identify anomalies, and forecast likely failures, allowing agents to perform autonomously on intervention planning and optimization with minimal impact on the production. The proposed framework provides a robust foundation for flexible, high-performance manufacturing environments, enabling smart factories to achieve superior adaptability, resilience, and operational efficiency.",3,115 | Cobb Galleria Centre,Process Monitoring 2,224
755,7059.0,Predicting Melt-pool Morphology in powder-based Directed Energy Deposition  using Deep Learning-based Generative Adversarial Networks,Academician,"In powder-based Directed Energy Deposition (DED) additive manufacturing, melt pool stability is impactful for achieving quality depositions. Monitoring and controlling the melt pool in real-time are challenging due to its dynamic response to varying process parameters and the prevalence of abnormal patterns. A robust deep learning model could enable effective classification and identification of discrepancies in melt pool morphology. However, the limited availability of large, high-quality datasets for visual inspection poses substantial obstacles, making data acquisition time-intensive, costly, and complex. To resolve this issue, the study proposes Generative Adversarial Networks (GANs) to analyze high-speed DED process images, providing critical insights into melt pool behavior and generating synthetic images to augment existing datasets. By leveraging GANs for realistic data augmentation, this approach improves the dataset’s balance and supports subsequent classification models aimed at accurate melt pool morphology prediction. Enhanced by high-fidelity synthetic data, these models can forecast melt pool characteristics before printing, facilitating preemptive adjustments in process parameters for improved quality control. This research demonstrates the potential of GAN-based augmentation to enhance data-driven predictive modeling in DED, offering a pathway toward more adaptive and precise additive manufacturing systems.",4,115 | Cobb Galleria Centre,Process Monitoring 2,224
756,5278.0,Traffic Scheduling of Networked Multiple Mobile Robot Systems in Constricted Environments,,"We address some new traffic scheduling problems for multiple mobile robot systems (MMRS), where a fleet of robots must execute a set of inspection tasks in a constricted operational environment like an underground utility or pipeline network. During the execution of these tasks, the robots must maintain a multi-hop wireless communication network connecting them with each other and with a command-&-control center that supervises the entire operation – this class of MMRS is characterized as networked MMRS in the robotics community. Employing networked MMRS in constricted environments introduces new resource allocation structures and traffic dynamics that transcend the state of the art of the corresponding theory and challenge our current understandings and insights for these dynamics and their effective management. In a series of our previous works, we have provided a systematic introduction of the considered problem, a mixed integer programming (MIP) formulation for it, a formal analysis of its worst-case computational complexity, and additional structural results for its optimal solutions that also enable a partial relaxation of the original MIP formulation preserving optimal performance. In this work, we leverage these structural results to develop a powerful heuristic algorithm capable of efficiently solving larger problem instances that are not amenable to the previous methods. Numerical experiments demonstrate the effectiveness and computational efficiency of the heuristic algorithm.",1,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 3,225
757,6859.0,Multi-Mode Assembly Line Balancing with Multi-Skilled and Traveling Workers,Academician,"The assembly of complex products frequently necessitates workers with different skill profiles. Those workers could be human or robot workers, and may be suited to conduct a single or multiple types of tasks, grouped by skill sets. However, due to high labor cost and low availability, business owners face challenges in selecting and allocating workload to workers. We propose a solution that balances assembly lines while allowing workers to travel among stations to address aforementioned issues. We establish a novel mixed integer programming (MIP) optimization model with the objective of minimizing worker and station costs. In addition, we employ a constructive heuristic and a neighborhood search method to accelerate the solving process for large-scale problems. Lastly, through experiments inspired by a real-world case study, we distill some insights regarding the application of traveling workers in such assembly environments.",2,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 3,225
758,7041.0,"""Increasing the Numerator"", Upskilling the Manufacturing Workforce",Practitioner,"It is estimated that 2.1 million skilled manufacturing jobs in the United States wiil be difficult to fill by 2030 due in large part to a decrease in birth rates starting 15 to 20 years ago. Industry is growing and the workforce cannot keep up. The northeast and midwest U.S. has an abundace of workforce development and technical training centers, but more are needed in growing areas of the country. An innovative initiative led by Mississippi State University in collaboration with our commuity college partners is addressing this need through the Advances in Manufacturing Upskilling Program (AiM UP). AiM UP’s mission is to develop a workforce that is happy, healthy, and highly effective. This mission is achievable through our vision to turn Mississippi into an innovator and early adopter of the Industry 4.0 concept, which will attract companies to Mississippi by providing them with access to skilled workers and opportunities for their employees to continue to grow industry skills. This effort is changing the narrative in Mississippi from “the best kept secret” to “the early obvious choice” for companies looking for a new home and location, making Mississippi a manufacturing technology leader in the southeast and creating a stronger economy. This presentation provides a roadmap for other states to address their current and future manufacturing workforce needs. The process is straightforward, robust, and highly attainable. While targeted towards practitioners, AiM UP is led by acadmics and academics in the field of manufacturing will find value from the presentation as well.",3,116 | Cobb Galleria Centre,Manufacturing Systems Engineering 3,225
759,6034.0,Advancements in Cold Spray Additive Manufacturing: A Review of Materials and Processes,Academician,"Cold Spray Additive Manufacturing (CSAM) is a cutting-edge high speed Additive Manufacturing process enabling the production of high-strength components without relying on traditional high-temperature methods. It accelerates metal particles to supersonic speed and deposit materials to a substrate layer by layer to form 3D object. The key advantage is its high material deposition rate which is more than 100 times faster than other metal 3D printing processes. CSAM produces oxide-free deposits and preserves the feedstock's original characteristics. This review paper explores strategies for improving material quality, focusing on nozzle design, particle size distribution, and fine-tuning of process parameters such as gas pressure, temperature, and spray distance. These factors are key to achieving efficient deposition and optimal bonding, which enhance the mechanical properties of the final products. The ability to produce high-performance, durable components positions CSAM as a promising additive manufacturing technology.",1,115 | Cobb Galleria Centre,Emerging Technologies in Advanced Manufacturing 1,226
760,6291.0,The Impact of Additive Manufacturing on Supply Chain Resilience in Small and Medium-Sized Enterprises: A Literature Survey,Academician,"The contribution of Industry 4.0 to the resilience of supply chains is undeniable. One of the Industry 4.0 technologies that has gained popularity in different industries, ranging from aerospace to online shops, is Additive Manufacturing (AM). The potential of AM to contribute to the resilience of the supply chain in different areas, such as the manufacturing of spare parts for the maintenance, repair, and overhaul (MRO) industry, is immense. In this paper, we will analyze the literature and provide case studies on the impact of AM on resilient and flexible supply chains applied to small and medium-sized enterprises (SMEs). Recommendations will be made on how AM can transform the competitiveness of SMEs while bidding on large contracts. In this regard, we will discuss how SMEs can combat marketing and advertising challenges, cyber security, provenance, and teaming.",2,115 | Cobb Galleria Centre,Emerging Technologies in Advanced Manufacturing 1,226
761,8686.0,Integrating Sustainable Manufacturing with Industry 4.0: Strategies for Enhancing Efficiency and Innovation,Academician,"The integration of sustainable manufacturing within the framework of Industry 4.0 combines sustainable practices with advanced technologies to address environmental challenges such as waste, emissions, and resource depletion. This research explores how real-time monitoring, data analytics, and automation can optimize operational efficiency, reduce costs, and enhance sustainability performance. It identifies methods and strategies for incorporating sustainability into Industry 4.0, highlighting the potential for manufacturers to improve both environmental and economic outcomes. The findings highlight the potential for manufacturers to meet sustainability goals while remaining competitive in a changing market.",3,115 | Cobb Galleria Centre,Emerging Technologies in Advanced Manufacturing 1,226
762,5556.0,Measuring the Impact of Innovation Programs on STEM Students’ Mindsets: Development of the Transformative Learning Scale,Academician,"Innovation competitions and programs (ICPs) are becoming pivotal in STEM education by immersing students in experiential learning that fosters an innovation mindset. However, measuring the transformative impact of ICPs on students remains a challenge due to the lack of validated frameworks. This study introduces the Transformative Learning Scale for the Innovation Mindset (TLSIM), designed to assess shifts in self-awareness, open-mindedness, and innovation capabilities among STEM students. Using Transformative Learning Theory (TLT) and Kern Entrepreneurial Engineering Network's (KEEN) 3Cs framework (Curiosity, Connections, Creating Value), TLSIM was developed and validated through expert reviews, focus groups, and psychometric analysis with data from 291 STEM students, 70.2% of whom were engineering majors. Confirmatory Factor Analysis revealed strong psychometric properties, demonstrating TLSIM’s reliability and validity in capturing transformative learning in ICP contexts. The scale emphasizes the multidimensional nature of the innovation mindset and focuses on both the outcomes and processes of developing an innovation mindset. Thereby, the TLSIM provides a new way for educators and program designers to assess and enhance the effectiveness of ICPs in fostering innovation mindsets in STEM education. The paper also presents preliminary findings about the connections between ICP processes and innovation mindset. The TLSIM tool has the potential to significantly improve the assessment and development of ICPs, which could result in more impactful innovations in STEM education. Future research will focus on refining the instrument and exploring its cross-disciplinary applications.",1,Ansley | Renaissance Waverly Hotel,Experiential Education,227
763,6655.0,Industrial Training and Assessment Centers (ITACs) as Hubs for Project-Based Learning in Smart Manufacturing Practices,Academician,"For over 45 years, Industrial Training and Assessment Centers (ITACs) have offered engineering students valuable practical and experimental learning experiences. Recently, these centers have expanded to include smart manufacturing concepts, moving beyond traditional mechanical and industrial engineering topics to emphasize systems engineering and automation. At Kennesaw State University, ITAC students actively engage with small and medium-sized enterprises (SMEs) to identify and explore smart manufacturing solutions that provide cost-effective, high-return-on-investment (ROI) opportunities. Given the limited budgets typical of SMEs, the recommendations made by ITAC students focus on initiatives that balance practicality and impact. This paper discusses key strategies for enhancing SME operations, including the development of predictive maintenance systems, the use of automated guided vehicles (AGVs) for material movement, and installation of automated packaging systems. These recommendations are cataloged within the ITAC’s Assessment Recommendation Codes (ARC) list, under the sections for Energy Management and Direct Productivity Enhancements and were curated based on direct field insights by ITAC program’s field managers. Additionally, a high-level systems model of these smart manufacturing solutions was developed using Model-Based Systems Engineering (MBSE) software. This model reflects students' findings and experiences from industrial assessments, illustrating the broader methodology for implementing smart manufacturing practices within SMEs. The paper concludes with a discussion of future project-based learning opportunities, aimed at enabling future students to deepen their understanding of smart manufacturing concepts and further the practical applications of systems and automation engineering in real-world SME environments.",2,Ansley | Renaissance Waverly Hotel,Experiential Education,227
764,6938.0,Experioental Learning in the Global Arena: The future of Learning Together,Academician,"Experiential Learning (EL) is an emerging hands-on approach where learners are exposed to learning experiences and then reflect on the insights and knowledge gained from the exercise. With EL, learners perform activities, then reflects on the observations during the physical performance and the outcomes, form theories and principles from the experience, and further experiment with insights gained. In group settings, participants may experience enhanced enrichment from synergistic ideology and thinking from differing views and perspectives. Global learning creates pathways for learners to understand global issues from diverse cultural, economic, political, environmental, and geographical perspectives. It fosters a global worldview and challenge learners to critically consider global issues and instill cultural and regional awareness in an interdisciplinary fashion. When EL is conducted in the global arena, geography, time zone, culture, social, economic, politics, norms, and others barrier to learning in a global sense becomes opportunities and launch points for further exploration and experimentation. Learners from around the world can share and experience “real time” and “real world” events offering learners to contemplate intercultural and interdisciplinary problem-solving methods that can change the world. In essence, EL in the global arena can create value in unimaginable ways. This paper focuses on a model for creating, implementing, and foster a Global Centric EL platform for engineers. Learners, faculty, administrators, and planners may benefit from this model. Further, it is envisioned that this and other similar research in this field will continue to transform the way people learn together around the world",3,Ansley | Renaissance Waverly Hotel,Experiential Education,227
765,8956.0,Building an Innovation Culture: Lessons from Developing and Scaling a Product Innovation Program in Engineering Education,Academician,"Engineering graduates increasingly need innovation and product development skills to succeed in industry, yet traditional engineering curricula often emphasize technical knowledge over innovation mindset and practical development experience. This presentation examines the development, implementation, and scaling of a Product Innovation (PI) specialization within Auburn University's Master of Engineering Management, which has grown into a top ranking program in just five years. We present a comprehensive analysis of the program's evolution, including: (1) the integration of academic frameworks with industry-driven content through guest speakers and real-world projects, (2) quantitative and qualitative assessment of student outcomes across both distance and on-campus delivery modes, and (3) research on key success factors for building sustainable innovation ecosystems in engineering education. The presentation will showcase specific examples of student projects and outcomes, including several that have progressed to commercial development. Our findings highlight critical elements for successful innovation education: cross-disciplinary collaboration, industry partnership integration, flexible delivery methods, and strong institutional support. We also examine how the program's success has catalyzed broader innovation initiatives across the college, including an undergraduate makers program, international collaboration opportunities, and new research directions in engineering education. These insights provide actionable frameworks for other institutions seeking to enhance their innovation and product development education capabilities. Particular emphasis will be placed on assessment methods and metrics that demonstrate both educational effectiveness and program sustainability. The presentation will conclude with recommendations for scaling similar programs at other institutions and future directions for research in engineering innovation education.",4,Ansley | Renaissance Waverly Hotel,Experiential Education,227
766,5787.0,Empowering Problem-Solving Confidence and Real-World Relevance: The Impact of a Scalable Summer Camp for Youth in Underserved Communities,Academician,"This study explores shifts in attitudes toward computational data science among underserved high school students participating in the Computational and Data Science for Engineering Summer Camp. Targeted at students with limited or no prior data science or programming experience, the camp introduces key concepts in data analysis, programming, and computational thinking, to equip students with skills for future engineering or related field studies. The camp is longitudinally designed, spanning one week each summer for three consecutive years, and is scalable due to its well-crafted instructional design. To assess the impact of the camp on students' confidence, problem-solving strategies, interest, and understanding of data science’s real-world relevance, pre- and post-surveys were administered. Analysis of survey responses reveals statistically significant improvements across students' problem-solving approaches, interest in data science, and recognition of its real-world relevance. These insights from this analysis are valuable for developing educational practices that foster positive attitudes toward data science and computing among underserved students. Ultimately, this research offers guidance for creating more inclusive and effective educational experiences in these fields. Lessons learned from this study, including strategies for enhancing student engagement and overcoming common challenges, will be described to inform future educational interventions and program designs.",1,Ansley | Renaissance Waverly Hotel,K12 and Engineering Education,228
767,6181.0,Engaging Young Minds Toward Industrial Engineering,Academician,"We present a dynamic outreach program structure designed to introduce high school students in advanced mathematics courses to Industrial Engineering (IE) and spark early interest in engineering. Designed collaboratively by Northwestern University faculty and undergraduate students, this program invites students from a local stem-focused high school to explore the versatility, mathematical rigor, and practical applications of IE. Our program includes engaging interactions with current IE students, accessible research presentations by faculty, and classroom observations that connect academic concepts to real-world engineering. Participants are introduced to IE’s key principles—flexibility, quantitative analysis, and applied problem-solving—while engaging with inspiring career stories from alumni. We emphasize the practical value and interdisciplinary nature of IE, underscoring its relevance across a wide range of industries. We present results of an assessment in this program and conclude with a discussion of effective outreach strategies for engaging pre-college students.",2,Ansley | Renaissance Waverly Hotel,K12 and Engineering Education,228
768,9053.0,Systems Literacy and Precision: A Category Theory Approach,Academician,"The complex problems of today's world require a transdisciplinary approach, and systems thinking offers essential skills for everyone to address these challenges. Fostering systems literacy—understanding a common systems language—is vital for creating a systems-literate society. This involves grasping four key concepts: distinctions, systems, relationships, and perspectives, structured within three learning levels: sensibility (awareness), literacy (knowledge), and capability (understanding). Recent research has explored how to define and measure the systems thinking learning process in non-experts. It conducted an experiment with 97 middle and high school students from the SMILE Program at Oregon State University. The study revealed a statistically significant increase in the understanding of elements, interactions, and roles/purposes, indicating that teaching these concepts enhances systems thinking skills. In this proposed work, we expand on these outcomes by formalizing how students identified elements, interactions, and roles/purposes by incorporating category theory. Category theory will provide a general framework to analyze any kind of system while increasing precision in their analysis. Our belief is that our ability to engage in transdisciplinarity will be enhanced by fostering systems literacy coupled with a precise language in the form of category theory.",3,Ansley | Renaissance Waverly Hotel,K12 and Engineering Education,228
769,8604.0,Leveraging Multimodal Data to Understand Computational Thinking in Young Learners: Augmented Reality and Social Robots,Academician,"This research examines the computational thinking abilities of young children (2nd-year elementary students) within an embodied learning framework aimed at enhancing STEM education. The study investigates how augmented reality and social robots can support problem-solving in STEM-related tasks. Over four days, students engaged with a specially designed educational tool that encouraged physical interaction and social engagement with a robot. The interactions were tracked to monitor changes in their movement, problem-solving behavior, and robot engagement. To analyze learning behaviors, the research team manually annotated video recordings, focusing on key indicators of engagement and cognition. Simultaneously, data from motion capture systems and facial muscle sensors were collected to provide objective measurements of physical responses. A statistical analysis will identify patterns between the sensor data and human annotations. Subsequently, a machine learning model will be developed to link sensor data with behavioral annotations, enabling automatic recognition of learning behaviors in future studies. This model aims to streamline the assessment of children's interactions in tech-enhanced learning environments. The study will also explore essential measures that reflect how children approach and adjust to STEM challenges, offering insights into the cognitive and physical aspects of embodied learning. This work has the potential to advance evaluation methods in STEM education, leading to improved educational tools and strategies for engaging young learners in computational thinking.",4,Ansley | Renaissance Waverly Hotel,K12 and Engineering Education,228
770,6427.0,Dynamic Directional Freight Routing in Physical Internet Networks,Academician,"This study explores the impact of dynamic freight routing protocols within the Physical Internet (PI) framework, focusing on a sector-based directional routing approach to optimize transportation network efficiency. We introduce two innovative routing protocols, the Cardinal Directional Routing Protocol (CDRP) and the Dynamic Density Directional Routing Protocol (D3RP), which organize the network across distinct geographic sectors. By leveraging real-time data on traffic, container positions, and disruptions, these protocols enhance adaptability and routing effectiveness. Our approach comprises two key phases: an area discovery phase and a node selection phase. In the area discovery phase, the network is segmented into candidate hubs that define potential routing paths, through fixed or dynamically adjusted sectors based on density and flow. This process narrows the search space for efficient routing. The node selection phase uses dynamic routing tables that continuously update to reflect the network's real-time state. Each entry in these tables evaluates critical routing metrics such as transit time, processing delays, cost, and environmental impact A flexible goal-weighting system customizes routing priorities, optimizing for speed, cost, or sustainability. Simulations tested various routing strategies to evaluate decision-making impacts on performance, including travel distance, truck utilization, and hub congestion. Results indicate that this sector-based strategy reduces total travel distance, improves truck fill rates, and mitigates congestion, minimizing idle time and boosting efficiency. This highlights the potential of sector-based dynamic routing to support more sustainable and resilient logistics networks. Future research will integrate multimodal transport, refine dynamic adjustments, and optimize routing for evolving logistics demands.",1,Lenox | Renaissance Waverly Hotel,16. Routing Applications,229
771,6461.0,Physical Internet Enabled Autonomous Trucking System: Modular Containerization and Fuel-Efficient Planning,Practitioner,"Autonomous trucking, particularly in the domain of truck platooning, is an emerging technology with the potential to revolutionize the freight transportation sector. Beyond addressing the shortage of truck drivers, autonomous truck platooning aims to significantly reduce fuel costs and emissions. This reduction is achieved through two main mechanisms: (1) operating in close platoon formation substantially decreases air drag, and (2) the absence of labor costs associated with drivers shifts the route optimization objective from minimizing travel time to minimizing fuel consumption. Motivated by the innovative concept of the Physical Internet, this paper focuses on designing a paradigm for an autonomous trucking system within a hyperconnected transportation network. The goal is to minimize fuel consumption while ensuring a 99% robust on-time delivery rate under stochastic conditions. We propose a data-driven, two-stage stochastic optimization model. In the first stage, flexible modular containers are designed based on historical data, and truck quantities and sizes are strategically deployed at each hub. In the second stage, an algorithm dynamically adjusts truck transfers between hubs based on real-time demand and traffic forecasts, determining optimal daily platooning routes. Lastly, the model’s energy efficiency and robustness are validated using real-world cargo transport data.",2,Lenox | Renaissance Waverly Hotel,16. Routing Applications,229
772,6596.0,Accelerate Logistics: A High-Performance Integration Plugin for the Open Logistics Interconnection (OLI) Model,Academician,"The Open Logistics Interconnection (OLI) model presents a promising framework for connecting logistics services. However, large-scale routing and scheduling demand high computational efficiency. This presentation introduces a C++-based plugin designed to enhance OLI functionalities through parallel computing and publish-subscribe data exchange mechanisms. Specifically, Open Multi-Processing (OpenMP) is utilized for node-internal parallelism, enabling multi-threaded scheduling and route computation within each logistics node. The Message Passing Interface (MPI) facilitates distributed task coordination across the system, managing tasks that require multi-node collaboration. Additionally, Data Distribution Service (DDS) establishes a publish-subscribe communication channel between nodes, allowing asynchronous data exchange without strict synchronization. This approach reduces communication complexity and supports efficient real-time data sharing. The presentation will detail the plugin architecture, the integration of parallel computing and DDS within the OLI framework, and their impact on the efficiency of logistics networks.",3,Lenox | Renaissance Waverly Hotel,16. Routing Applications,229
773,6611.0,The Full Truckload Pickup and Delivery Problem with Truck Platooning,Academician,"Truck platooning is a promising technology for reducing energy consumption, increasing vehicle safety, and improving traffic efficiency. In this study, we examine the cost-effectiveness of truck platooning from the perspective of a freight company fulfilling full truckload pickup and delivery requests over a transportation network. During transportation, trucks can form platoons on the traversed road sections to reduce the travel costs of the following trucks. The problem is how the routing and scheduling of trucks should be determined to take full advantage of truck platooning and minimize the total transportation cost. We propose two model formulations over a time-expanded network for this problem: a direct delivery model and an indirect delivery model, where the indirect delivery model allows trucks to visit intermediate locations during deliveries to facilitate the formation of platoons. In both models, trucks are permitted to wait at any traversed node provided that time windows of requests are not violated. We develop an improved dynamic discretization discovery (DDD) algorithm to solve the two models exactly. Through extensive computational experiments, we find that (1) the improved DDD algorithm can increase solution accuracy with much less computational effort compared with the basic DDD algorithm; (2) the cost-saving effect of truck platooning is favorable; and (3) for freight companies operating on small transportation networks, using the direct delivery model may be more appropriate.",4,Lenox | Renaissance Waverly Hotel,16. Routing Applications,229
774,5542.0,"Decarbonizing Freight Transportation: Joint Optimization of Intermodal Cargo Routing, Scheduling, and Service Networks",Academician,"Intermodal transportation, which exploits the strengths of alternative modes of transportation, such as rail and water, offers a promising solution for the growing freight transportation sector to reduce carbon emissions and operating costs. Optimization of the intermodal transportation system has been widely studied in the literature, but conventional approaches typically focus on the broad averages of emission and cost statistics or rely on existing inflexible timetables that lead to suboptimal results. In this study, we optimize the U.S. intermodal system consisting of highway, railway, and waterway freight transportation. We construct a mixed integer programming model on a muti-layered transportation network to jointly optimize freight routing, scheduling, and service network design decisions. The model minimizes intermodal operating costs, carbon emission costs as carbon taxes, and late penalties for delayed freight arrival. The complexity of the intermodal system imposes great computational challenges on state-of-the-art solution methods. To address this, we investigate mathematical properties of the model to explore structural benefits. We further propose a vehicle scheduling decomposition approach that expands the Lagrangian cutting plane methods towards nonlinear linking constraints. Structural properties are integrated into the decomposition to derive novel algorithms. Real-world intermodal infrastructure, cost, and freight projection data are collected to validate the model and the algorithms through comprehensive experiments. Carbon emission data from combinations of vehicle engines and fuel types are utilized in large-scale case studies to provide insights on intermodal decarbonization, targeting a carbon-neutral U.S. freight transportation sector by 2050.",1,Lenox | Renaissance Waverly Hotel,15. Barge / Intermodal Applications,230
775,6777.0,From Roads to Rivers: Adapting Reinforcement Learning Solutions for Barge Transportation,Academician,"Barge transportation is a vital component of global logistics, providing a cost-effective and environmentally friendly means of moving bulk goods. Despite its significance, this mode of transport faces considerable uncertain and dynamic challenges, including fluctuating water levels, variable weather conditions, and infrastructure disruptions. Reinforcement learning (RL), a branch of machine learning that is well-suited for decision-making under uncertainty, has emerged as a powerful tool for addressing complex logistical problems in dynamic environments. Currently, there is a notable scarcity of research focusing on advanced computational methods, such as RL, to address the complexities in barge transportation. This article seeks to bridge this gap by first reviewing the limited existing literature on barge transportation and the strategies employed to manage its inherent uncertainties. Furthermore, we examine studies from other transportation modes such as truck and rail, that encounter similar operational challenges and have successfully implemented RL solutions. By analyzing these approaches in the context of routing, scheduling, and multimodal planning, we explore how RL-based methods can be adapted for barge transportation. The findings highlight significant opportunities to enhance the efficiency and reliability of barge transportation systems through the adaptation of reinforcement learning techniques proven in other transportation sectors.",2,Lenox | Renaissance Waverly Hotel,15. Barge / Intermodal Applications,230
776,8892.0,Sustainable Multi-Modal Transportation and Routing focusing on Costs and Carbon Emissions Reduction,Academician,"Transportation plays a critical role in supply chain networks, directly impacting cost efficiency, delivery reliability, and environmental sustainability. This study provides an enhanced optimization model for transportation planning, emphasizing environmental sustainability and cost-efficiency. An Integer Linear Programming (ILP) model was developed to minimize the total transportation costs by considering organizational and third-party vehicles' operational and rental costs while incorporating constraints on carbon emissions. The model incorporates multi-modal transportation routing and emission caps to select the optimized number of organizational and rental vehicles of different modes in each route to ensure adherence to sustainability goals. Key innovations include adding carbon emission constraints and optimizing route selection to reduce overall emissions. The model was implemented using the Gurobi solver, and numerical analysis indicates that integrating environmental considerations significantly influences vehicle type selection and allocation, route selection, and operational strategies. The results reveal a trade-off between cost minimization and carbon footprint reduction, with the enhanced model showing a higher objective function value due to added emission constraints. These insights provide actionable guidance for industries aiming to enhance both economic performance and environmental responsibility.",3,Lenox | Renaissance Waverly Hotel,15. Barge / Intermodal Applications,230
777,4823.0,An alternative solution to congestion relief of U.S. seaports by container-on-barge: a simulation study,Practitioner,"Port congestion has become a big challenge for many seaports in the United States (U.S.). There are multiple factors contributing to port congestion, such as the booming market of international container trading, limited space and capacity at a seaport, labor shortages, equipment failures, and supply chain disruptions (i.e., COVID-19 pandemic). Without appropriate management, port congestion will increase transportation costs, cause delays in goods movement and cargo delivery, reduce port operation efficiency, and lead to supply chain disruptions. Traditional strategies, such as infrastructure investment, truck appointments optimization, and party cooperation enhancement, have been implemented by ports for years, but the results are often unsatisfactory. To learn the needs of U.S. seaports and seek an efficient method for congestion relief, we develop a discrete event simulation model incorporating Container-on-Barge (COB) as an alternative transportation mode in this study. The simulation model mimics port operations involving COB and investigates congestion times at berth, yard, and gates. A case study on the Port of New Orleans (Port NOLA) is conducted to demonstrate the implementation of the simulation model and to evaluate the potential of COB for seaport congestion relief. The simulation results suggest that with a good level of COB development, the congestion at berth, yard and entrances of a seaport can be decreased significantly.",4,Lenox | Renaissance Waverly Hotel,15. Barge / Intermodal Applications,230
778,5201.0,Forecasting Intravenous (IV) Fluid Demand Amid Supply Disruptions: A Predictive Model for Hospital Supply Chain Management,Practitioner,"Effective supply chain management in hospitals is critical, especially during unexpected supply chain disruptions. This project, which started as emergency management and not as traditional supply chain management, focused on developing a predictive model to forecast the required volume of intravenous (IV) fluid items for a hospital in Maryland and a hospital in Delaware. This addressed a supply shortage caused by a hurricane affecting the primary manufacturing plant in North Cove, North Carolina. With the supplier now reducing the allocation of the supply to as low as 25% but on average 40% from the normal supply quantity allocation, the hospital faces significant challenges in meeting patient care needs. Using historical usage data and seasonal demand patterns, we aimed to provide an early-warning system for inventory shortfalls, enabling proactive resource allocation, sourcing, and contingency planning. This project highlights the potential of predictive analytics in safeguarding healthcare operations against supply chain vulnerabilities and ensures continuous, high-quality patient care despite external disruptions. By showing a Mean Absolute Percentage Error (MAPE) of 15.64%, our findings demonstrate that such models can be instrumental in maintaining resilience in healthcare logistics, and guiding resource planning across hospitals facing similar challenges.",1,Lenox | Renaissance Waverly Hotel,8. Supply Chain Disruption,231
779,7036.0,Metaheuristic framework for Expedited Emergency Logistics Planning,Academician,"We address a logistics scenario where multiple locations are struck by an emergency, necessitating rapid delivery of several different critical commodities to prevent severe repercussions. Each affected site has a unique demand profile, with varying severity levels for different commodities. This presents a significant challenge to an emergency logistics operator tasked with coordinating a fleet of vehicles with diverse capacities, ranges, and speeds. We model this scenario as a mixed integer program. In this high-stakes context, we focus on minimizing penalties associated with unmet or delayed demands, prioritizing this over transportation costs. The model employs a time-expanded network which encompasses all locations involved in the emergency and the response efforts. Due to the MIP’s complexity and the urgent need for quick solutions, directly solving the MIP within the required timeframe is impractical. To address this, we strategically divide the time horizon into two stages. The first stage concentrates on solving a first-mile problem in which we maximize the movement of commodities to a selection of key hubs near the emergency zone. The second stage then addresses the last-mile problem, focusing on effectively distributing these commodities to their final demand locations. We develop heuristics for each phase to expedite the search for viable solutions, ensuring timely delivery of the critical commodities.",2,Lenox | Renaissance Waverly Hotel,8. Supply Chain Disruption,231
780,6140.0,Food Pantry Potential Spatial Accessibility Before and After Hurricane Disruptions,Academician,"Food insecurity, defined as having limited or uncertain access to adequate food, affected almost 1 in 7 U.S. households in 2023. Research has established that after disasters such as hurricanes, the food insecurity rate doubles. Food pantries can help meet the food needs of food insecure households after disasters if they’re resilient to disruptions (e.g., are able to refrigerate non-shelf stable food) and accessible to people in need. It is important to understand the potential vulnerability of food pantries to power outages as well as their spatial accessibility for different communities, which is a measure of the complex interactions between food pantry supply and demand within and between regions. This enables vulnerable areas to be identified and supported. In this research, we use the two-step floating catchment area (2SFCA) method to measure food pantry potential spatial accessibility before and after power outages from Hurricane Florence in 2018. Our case study area covers three coastal counties in North Carolina: Brunswick, Pender, and New Hanover. We used population data aggregated at the census block level to model food pantry demand and combined food pantry network data with each pantry’s distribution volume to model supply. Power outage reports from the days following Hurricane Florence landfall were used to model disruptions. Our results indicate which census blocks in the study area can benefit most from interventions to improve access to food pantries in both disrupted and non-disrupted scenarios.",3,Lenox | Renaissance Waverly Hotel,8. Supply Chain Disruption,231
781,8715.0,Adapting the Inventory Routing Problem to Improve Water Delivery Scheduling for Water Insecure Households,Academician,"Access to clean drinking water can be a luxury in impoverished communities in the United States. Nationally, more than 2 million people lack access to safe drinking water and indoor plumbing, and this figure does not include homes with a working tap of unsafe drinking water. Therefore, a far larger number of Americans face water insecurity. A recent examination of US Census and Safe Drinking Water Act violation data reveals that rural low-income and minority communities are significantly more likely to be burdened with unavailable or unsafe in-home drinking water. This research explores an alternative to traditional water infrastructure: how to optimally deliver drinking water to households without access to reliable, clean water sources by adapting the inventory routing problem (IRP) and creating heuristic algorithms to schedule efficient water delivery. The IRP and heuristics inform three key decisions: when to serve a customer, how much to deliver, and which delivery routes to use.",4,Lenox | Renaissance Waverly Hotel,8. Supply Chain Disruption,231
782,5143.0,Dynamic Less-Than-Truckload Transportation Planning in Hyperconnected Hub Networks with Multi-Carrier Operations,Practitioner,"Less-than-truckload (LTL) shipment is vital in modern freight transportation yet is in dire need of more efficient usage of resources, higher service responsiveness and velocity, lower overall shipping cost across all parties, and better quality of life for the drivers. The industry is currently highly fragmented, with numerous small to medium-sized LTL carriers typically operating within dedicated regions or corridors, mostly disconnected from each other. This paper investigates the large-scale interconnection of LTL carriers enabling each to leverage multi-carrier networks for cross-region services exploiting their mutual logistic hubs, in line with Physical Internet principles. In such a network, efficient open cooperation strategies are critical for optimizing multiparty relay shipment consolidation and delivery, transport and logistic operations and orchestration, and enabling inter-hub driver short hauls. To dynamically plan relay truck transportation of involved carriers across hyperconnected hub networks, we develop an optimization-based model to build loads, coordinate shipments, and synchronize driver deliveries. We report a simulation-based experiment in a multiparty LTL network covering the U.S. in two scenarios: 1) each carrier operates separately and serves its clients in and out of its service region, and 2) all carriers operate jointly and serve clients in the interconnected multi-carrier network. By comparing these two scenarios, we evaluate the impact of carrier cooperations on cost savings, service efficiency, client acquisition, and greenhouse gas emissions. Overall, this research advances operational efficiencies through an effective collaborative solution across the LTL industry and contributes to the pursuit of sustainable logistics networks.",1,Tyndall | Renaissance Waverly Hotel,9. Studies of Hyperconnected Hubs/Networks,232
783,6472.0,Driver Preference Based Load Assignment & Scheduling  in Relay Truck Transportation Planning  with Hyperconnected Network Topology,Practitioner,"The trucking industry, particularly in the middle-mile segment, faces significant challenges, including driver shortages, high turnover rates, and safety concerns. Reports from the American Transportation Research Institute (ATRI) have consistently ranked driver shortages and turnover among the top challenges since 2012. This shortage is currently estimated at over 60,000 drivers and could exceed 160,000 by 2030 if trends continue. Contributing factors include an aging driver population, challenging working conditions, and a lack of interest from younger generations due to poor work-life balance. Current scheduling practices focus on service-level agreements and operational efficiency, often neglecting driver needs, resulting in long-haul assignments that contribute to fatigue, safety issues, and dissatisfaction. Over 47% of drivers report falling asleep at the wheel heightening the risk of accidents. Additionally, high driver turnover costs places a financial strain on the trucking industry and broader supply chain. We leverage a relay-based approach inspired by the Physical Internet’s concept of hyperconnected networks. By segmenting long-haul routes into manageable sections connected through strategically located hubs, drivers can complete trips within regulated hours and return home more frequently. In this paper, we develop an optimization model to assign these shorter loads to the drivers incorporating driver preferences like shift duration, days off, rest periods, using a fuzzy logic approach for load assignments and scheduling. This model enhances work-life balance, reduces fatigue, and improves retention. This adaptable system enhances resource utilization, minimizes emissions, and promotes safety, sustainability, and resilience in the trucking industry.",2,Tyndall | Renaissance Waverly Hotel,9. Studies of Hyperconnected Hubs/Networks,232
784,6530.0,Leveraging Large Language Models for Risk Assessment in Hyperconnected Logistic Hub Location Deployment,,"The growing emphasis on energy efficiency and environmental sustainability in global supply chains introduces new challenges in the deployment of hyperconnected logistics hubs. In such volatile, uncertain, complex, and ambiguous (VUCA) logistics networks, risk assessment becomes essential to ensure successful hub deployment. However, traditional methods often struggle to effectively capture and analyze unstructured textual information. In this paper, we leverage large language models (LLMs) with tools to conduct risk assessment for logistics hub deployment. We use LLMs to identify potential risks by analyzing unstructured data, such as geopolitical factors, regulatory changes, and environmental risks. These data are integrated into tools, which can be automatically called by the LLMs to support our decision-making process. In addition, we design prompts to instruct LLMs to assess the feasibility of hub selection by analyzing risk types and levels, and then providing insights for hub location recommendations. Finally, we conduct a comparative analysis of new and existing hub locations to evaluate improvements in risk mitigation and overall network resilience.",3,Tyndall | Renaissance Waverly Hotel,9. Studies of Hyperconnected Hubs/Networks,232
785,6984.0,Hyperconnected Multi-Carrier Multi-Hub Less-Than-Truckload Freight Transportation and Logistic System,Practitioner,"Many less-than-truckload (LTL) carriers operate independently, retaining control over their fleets, networks, and limited hub facilities, shaping the LTL industry into a myriad of small carriers with low interconnectivity. This solo practice often leads to inefficiency and unsustainability: trucks often less than 60% loaded with about 25% empty trips, low freight consolidation potential, long order-to-delivery times, high service costs, excessive greenhouse gas emissions, constrained market reach, and limited flexibility. To breaks away from the current pitfalls, this paper examines leveraging the Physical Internet concepts, models, and principles to reimagine LTL freight transportation and logistics as a highly scalable hyperconnected system, with open dynamic multi-carrier multi-hub cooperation and consolidation. The paper focalizes on identifying transformational gaps and pathways from the current to a hyperconnected LTL freight system, with emphasis on the conceptual and pragmatic systemic change requirements and avenues at the operating, transactional, and orchestration levels.",4,Tyndall | Renaissance Waverly Hotel,9. Studies of Hyperconnected Hubs/Networks,232
786,5499.0,Long Term Optimization of Facility Locations in Supply Chain Network,Academician,"This research models optimal decision-making strategies for facility location management covering warehouses, sort centers, delivery stations, and other assets within the supply chain network for long term profitability and continued operational excellence. Effective asset management is essential for organizations maintaining large inventories of assets, often accumulated gradually or acquired due to sudden demand surges. Unanticipated events can create a bullwhip effect, leading to a misalignment between asset holdings and actual needs, thereby impacting profitability and operational efficiency. The study addresses the challenges of underutilized assets, low return on investment (ROI), and elevated operational costs associated with uncertainty in demand and suboptimal asset placement. While broader in application, this research utilizes facility location logistics problem to demonstrate applicability and improvements from a myopic asset management policy. The presented model offers a systematic approach to adapt facility portfolios over time, improve facility network utilization and cost management for sustainable asset deployment. We demonstrate the effectiveness of our model through computational experiments, as well as discuss other potential use cases and performance of the model.",1,Tyndall | Renaissance Waverly Hotel,12. Facility Location Decisions,233
787,6327.0,Learning-Based Multi-Criteria Decision Making Model for Site Selection Problems,Academician,"Strategically locating sawmills is critical for the efficiency, profitability, and sustainability of timber supply chains, yet it involves a series of complex decision-making affected by various factors, such as proximity to resources and markets, proximity to roads and rail lines, distance from the urban area, slope, labor market, and existing sawmill data. Although conventional Multi-Criteria Decision-Making (MCDM) approaches utilize these factors while locating facilities, they are susceptible to bias since they rely heavily on expert opinions to determine the relative factor weights. Machine learning (ML) models provide an objective, data-driven alternative for site selection that derives these weights directly from the patterns in large datasets without requiring subjective weighting. Additionally, ML models autonomously identify critical features, eliminating the need for subjective feature selection. In this study, we propose integrated ML and MCDM methods and showcase the utility of this integrated model to improve sawmill location decisions via a case study in Mississippi. This integrated model is flexible and applicable to site selection problems across various industries",2,Tyndall | Renaissance Waverly Hotel,12. Facility Location Decisions,233
788,6556.0,Hyperconnected Facility Location Contracting,Academician,"Facility location problems (FLP) involve determining the optimal placement of facilities to support the the operation of a distribution network and meet customer demand. In a traditional FLP, decision-makers must weigh the costs of allocation and location, which typically include transportation expenses to demand points and the construction and operational costs of the facilities. Key decisions in FLPs include selecting the most suitable locations for facilities from a set of potential sites and determining how to allocate demand to these facilities. A variant of the problem also allows capacity dimensioning at facilities, where a decision maker can determine the capacity to be installed at each operational facility. In light of hyperconnected fulfillment, decision makers can now access a much wider range of open source fulfillment centers in the form of contracts, instead of building an entire fulfillment network on their own. This work studies the Hyperconnected Facility Location Contracting problem that generalizes several FLPs and consists of determining contracts for open source facilities to meet e-commerce demand over a multi-period planning horizon. A contract for a specific facility is characterized by the following four features: capacity, length, timing (peak season or not), and how much in advance. The problem is modeled as a Mixed Integer Linear Program (MILP). Dimensionality issues of the model arise as the number of potential open facilities and the size of the product portfolio carried increase. This work aims to explore various algorithmic and analytical approaches to effectively manage and mitigate these challenges.",3,Tyndall | Renaissance Waverly Hotel,12. Facility Location Decisions,233
789,5404.0,Optimized Production Scheduling for a Continuous Flow Food Manufacturing Environment with Sequential Operations and Parallel Machines,Academician,"The United States Department of Agriculture (USDA) estimates that food waste accounts for 30-40% of the total food supply in the United States, with food manufacturing contributing approximately 15% of the total waste. While manufacturing may not be the largest contributor, it represents a substantial portion of the overall issue. This research develops a comprehensive, multi-stage optimization methodology to find optimal production scheduling for a real-world company in a continuous-flow food manufacturing environment, which utilizes both continuous and batch processes sequentially and in parallel. Our approach integrates the interdependencies across various production stages to maximize yield, minimize costs, and achieve target service levels within a make-to-stock environment. By accounting for production constraints, resource limitations, raw material perishability, and other critical factors, we compute the optimal weekly production schedule through a stochastic mixed-integer program combined with a novel solution algorithm.",1,Tyndall | Renaissance Waverly Hotel,6. Agri-food Supply Chain Applications,234
790,9058.0,From Farm to Food Hub: Analyzing Logistics in Cooperative Agricultural Systems,Academician,"This study explores the logistical challenges within cooperative agricultural systems through an ongoing collaboration between researchers and practitioners at the Wisconsin Food Hub Cooperative (WFHC). Operating under a cooperative model, WFHC enables small-scale farmers in rural regions to access shared logistics infrastructure, reducing individual costs while expanding their market reach. Drawing from interdisciplinary perspectives and fieldwork conducted in partnership with WFHC members, this research examines the route operations required to collect products from dispersed farming locations for consolidation at the food hub. Using process mapping and workflow analysis, the study identifies the needs in route planning, pickup scheduling, and load optimization. Further, this collaboration highlights opportunities to enhance the efficiency and sustainability of product aggregation. Findings contribute to resilient, community-driven food networks while supporting underserved farming communities.",2,Tyndall | Renaissance Waverly Hotel,6. Agri-food Supply Chain Applications,234
791,5919.0,Network Design for Not-for-Profit Food Distribution Supply Chain,Academician,"The study focuses on helping a third party logistics company design a network of consolidation centers to support the equitable distribution of products from suppliers to food banks. Part of that decision is to also determine the number of vehicles to purchase or own. We propose a two-stage stochastic model to solve this problem, and we develop multiple scenarios to study.",3,Tyndall | Renaissance Waverly Hotel,6. Agri-food Supply Chain Applications,234
792,5775.0,Supply Chain and Inventory Management of Spare Parts Incorporating Additive Manufacturing,Academician,"The operation of the spare parts supply chain has been dramatically impacted by the advent of additive manufacturing due to its rapidity and flexibility. Additive manufacturing can be implemented at various supply points to fulfill the demand for products with the rapid production of small lots. Additionally, the capability of producing multiple part types in the same batch provides more flexible alternatives to meet customers’ orders. Despite the fact that additive manufacturing has advantages in spare parts production, traditional manufacturing (e.g., casting) typically has lower variable costs, leading to a trade-off between the two production methods. Hence, mathematical analyses are conducted to determine profitable inventory control and logistics planning for spare parts supply chain considering additive manufacturing.",4,Tyndall | Renaissance Waverly Hotel,6. Agri-food Supply Chain Applications,234
793,5157.0,Innovating Supply Chain Strategies: A Study of Strategic Flaws and Technological Gap in Supply Chain Management,Practitioner,"Supply chain management has developed as a critical function in businesses worldwide, specifically with the increasing complexity of globalized markets. Behemoth companies like Walmart and others have created senior-level supply chain roles, underlining its strategic importance. Furthermore, the demand for supply chain professionals is projected to grow by 19% between 2023 and 2033, faster than the average for all professions. However, despite these advancements, supply chain methodologies remain scarce, leading to persistent challenges like demand-supply misalignment and inefficiencies in management. This research paper inspects two core hypotheses behind the persistent inefficacies in supply chain strategies: the inadequacy of current cost-minimization approaches and outdated supply chain technologies. The research will also propose an improved strategy leveraging real-time data, enhanced supply volume prediction, and technological progressions such as AI to build a more adaptive and resilient supply chain model.",1,Lenox | Renaissance Waverly Hotel,3. Supply Chain Risk,235
794,6760.0,Model-Based System Optimization to Assess Cybersecurity Risks in Supply Chains,,"The acquisition process in modern aircraft supply chains is becoming more complex due to increasing stakeholders, varying demands, more diversified requirements, unprecedented disruptions, etc., calling to the attention of many manufacturers the need for robust supply chain planning. With more Advanced Air Mobility (AAM) initiatives like the Electric Vertical Takeoff and Landing (eVTOL) aircraft, eVTOL manufacturers, during the acquisition stages, would need a support tool that captures information about stakeholders, dynamically accounts for the demand, monitors the requirements of stakeholders and the eVTOL being designed, and assists the manufacturer to select suppliers. The novel nature of the eVTOL supply chain differentiates it from the traditional aircraft manufacturing supply chain, in that, the eVTOL supply chain comprises different standards, fewer suppliers, and a broader spectrum of clients. A model-based system analysis of the eVTOL supply chain would therefore allow manufacturers to test different scenarios before deployment, paving the way to better understand and assess structure, behavior, and possible risks. Also, impactful tasks like supplier selection, quantity allocation, among others could be optimized to achieve higher success rates in meeting objectives. This raises concerns about possible cyber vulnerabilities (e.g., data poisoning) that may interfere with being able to meet such objectives. We propose a model-based system approach using SysML that 1) captures the structure, behavior, requirements, and parametrics relevant to the acquisition process and the eVTOL supply chain, 2) optimally selects suppliers to maximize profits and meet customer requirements using an embedded optimization algorithm, and 3) accounts for cyber risks using attack-defense graphs.",2,Lenox | Renaissance Waverly Hotel,3. Supply Chain Risk,235
795,8757.0,RISE to Resilience: A New Index to Measure Supply Chain Strategic Resilience,Practitioner,"In today’s increasingly complex and unpredictable global economy, ensuring supply chain resilience is more crucial than ever. Despite its importance, most companies rely on intuition rather than a clear, measurable understanding of how resilient their supply chains are or how strategic adjustments could improve resilience. When discussing supply chain resilience, two levels are often mentioned: operational resilience (short-term, reactive responses to disruptions) and strategic resilience (long-term, proactive planning). While existing research and practices mainly focus on how quickly supply chains recover from disruptions, they lack tools to measure resilience from a forward-looking, strategic perspective and guide future improvements. To address this gap, we developed the Resiliency Index for Strategic Evaluation (RISE) . RISE quantifies supply chain resilience by evaluating three key factors: the complexity of the supply chain structure, the sourcing risks related to the countries of suppliers, and the dependence on specific suppliers or partners. This index serves two main purposes: it provides companies with a benchmark to assess the current resilience of their supply chain, and it evaluates the potential impact of strategic adjustments aimed at improving resilience. We applied RISE to real-world case studies, demonstrating its effectiveness and providing actionable insights for industry leaders.",3,Lenox | Renaissance Waverly Hotel,3. Supply Chain Risk,235
796,5504.0,Relay-Hub Network Design for Consolidation Planning Under Demand Variability,Academician,"We study the problem of designing large-scale resilient relay logistics hub networks. We propose a model of Capacitated Relay Network Design under Stochastic Demand and Consolidation-Based Routing (CRND-SDCR), which aims to improve a network’s efficiency and resilience against commodity demand variability through integrating tactical decisions. We formulate CRND-SDCR as a two-stage stochastic optimization program where we locate relay logistics hubs and decide their capacities in the first stage and design a minimum-cost consolidation plan in the second stage. As an exact solution approach, we design a branch-and-cut algorithm with a nested Benders decomposition and integer L-shaped method. We decompose CRND-SDCR twice: (i) across the stochastic demand scenarios, and (ii) across each origin-destination pair within the scenario-dependent subproblems; and utilize Benders decomposition at each of these decomposition stages to add the associated Benders feedback cuts. We guarantee the exactness of our solution approach by adding integer L-shaped cuts, obtained by solving the second-stage subproblem exactly through Benders decomposition as well. We apply our methodology to design large-scale resilient relay networks to be used for finished vehicle deliveries for a US-based car manufacturer partner. Our computational experiments demonstrate that our developed approach can obtain near-optimal solutions for practically relevant instances using sample average approximation. The resulting logistics networks showcase a significant improvement in capabilities to sustain commodity demand variability, in comparison with relay networks designed to fulfill average commodity demand. Our analysis provides decision-makers with recommendations regarding inducing network flexibility to hedge against commodity demand uncertainty.",1,Lenox | Renaissance Waverly Hotel,10. Network Design Applications,236
797,6533.0,Evaluating the Multimodal Freight Network in the Pacific Northwest,Academician,"The objective of this research is to develop network models to evaluate the performance of multimodal freight transportation networks. A mathematical programming formulation has been developed that considers the multimodal transportation network as a multi-layer, multi-commodity network in which decisions are made about flows (i.e., selection of transportation modes and routing for shipments in the network) that minimize total operational costs. The model also incorporates the capability of evaluating resilience metrics associated with topological characteristics of the multimodal freight transportation network such as network complexity, as well as service-related metrics like unmet demand. A test case scenario for the Pacific Northwest is used to illustrate the application of the approach.",2,Lenox | Renaissance Waverly Hotel,10. Network Design Applications,236
798,8644.0,The Fulfillment Regionalization Problem,Academician,"In today’s retailing formats, retailers can choose which inventory location or Fulfillment Center (FC) to fulfill from, bringing opportunities of inventory pooling and providing a much broader product selection. The fulfillment decision, although complex, can bring financial gains when optimized for resources and operating costs. With the unprecedented growth of the retail industry, companies now have the opportunity to strategically divide their fulfillment networks into regional networks. Such a method, called regionalization, simplifies the fulfillment decision, is scalable, and has helped to increase fulfillment speed while lower cost at Amazon. The region definitions affect speed of delivery, cost to serve, and selection of products offered, which influences customer experience. We present a heuristic approach on how to partition the national network into regions that fulfill customer demand predominantly from a set of pre-assigned FCs, and analyze contiguity of solutions. We benchmark solution quality against efficient lower bound models, and evaluate region designs.",3,Lenox | Renaissance Waverly Hotel,10. Network Design Applications,236
799,8877.0,Modeling the Impacts of Policy and Infrastructure Investments on Freight Emissions and Congestion: An Agent-Based Simulation Approach,Academician,"The growing demand for freight transport has increased emissions and congestion in traditional road and rail systems, emphasizing the need for sustainable alternatives. Inland waterways offer significant potential, but limited research has examined the combined effects of policy interventions and infrastructure investments on freight mode choices and overall system performance. This study addresses this gap by employing an Agent-Based Simulation (ABS) model using SimPy to evaluate the impacts of various scenarios on emissions, travel times, and congestion levels. The simulation focuses on freight movement between Houston and San Antonio, testing scenarios with freight shifts to waterways ranging from 10% to 75%. It incorporates policies such as carbon taxes, barge and rail subsidies, and road pricing alongside infrastructure investments aimed at alleviating bottlenecks. Results demonstrate that a 75% shift to barge transport achieves the lowest CO₂ emissions, while dynamic congestion charges effectively reduce peak truck usage and road congestion. Infrastructure enhancements in barge and rail capacity further improve network efficiency by mitigating bottlenecks and enhancing flow. These findings provide practical insights for policymakers, highlighting effective combinations of policies and infrastructure investments to achieve a balanced and sustainable freight transport system. Future research could explore strategies for optimizing freight distribution networks through multi-modal logistics and adaptive infrastructure planning. Integrating real-time data analytics and digital platforms could facilitate improved coordination among trucks, trains, and barges, reducing delays and maximizing capacity utilization. These advancements improve operational efficiency and provide support for long-term environmental sustainability by minimizing resource wastage and emissions.",4,Lenox | Renaissance Waverly Hotel,10. Network Design Applications,236
800,6510.0,A Realistic Case Study in Adaptive Disaster Response,Academician,"To assess the applicability of models that optimize the routes of emergency fleet vehicles in disaster response, it is essential to validate the results by applying the model to a real-life case study. To this end, this paper presents a realistic case study to illustrate the effectiveness of dynamic emergency vehicle routing models in producing optimal routing solutions for large-scale, realistic, and complex disaster scenarios. The objective of this study is to collaborate with a local governmental entity that is responsible for maintaining information on all vehicles and routes in the area of interest. This collaboration includes obtaining data and information on the types, sizes and initial locations of the emergency fleet vehicles, locations in need of aid and rescue, types of aid needed, and locations of road closures and hazards. This paper leverages the aforementioned data to evaluate the applicability of dynamic emergency vehicle routing models, aiming to advance the field of emergency fleet routing through the application of a realistic and adaptive approach. This approach reoptimizes the routes of emergency fleet vehicles based on real-time updates regarding road conditions and other evolving factors, ensuring the most efficient deployment of resources in high-stakes scenarios. The findings underline the significance of utilizing an evaluation framework to assess the performance of routing models by examining how well they account for critical real-world factors in realistic situations. Results show that the model evaluated is a practical tool for disaster management, enhancing response times while accounting for the unpredictable nature of emergency situations.",1,Lenox | Renaissance Waverly Hotel,2. Humanitarian Logistics / Disaster Response,237
801,6683.0,Optimizing Intermodal Humanitarian Logistics Considering Carbon Emissions,Academician,"Disasters frequently disrupt transportation infrastructure, impeding the timely delivery of critical relief supplies. For this, intermodal transportation can be a crucial solution for delivering these supplies, especially when traditional road networks are compromised. However, the urgent need for immediate relief often overshadows the environmental impact of humanitarian operations. To address this, a mathematical model is proposed to optimize intermodal relief distribution while minimizing costs, unmet demand, and carbon emissions. This model considers various transportation modes, disaster scenarios, and the strategic location of relief facilities. By optimizing facility location, resource allocation, and intermodal operations, the study aims to balance the immediate need for relief with long-term environmental sustainability.",2,Lenox | Renaissance Waverly Hotel,2. Humanitarian Logistics / Disaster Response,237
802,8748.0,A Qualitative Study to Explore Gaps Between Theory and Practice in Disaster Response Logistics Path Planning,,"In disaster response logistics, efficient transportation management is essential. For theoretical models to be effective, they must closely align with the practical challenges faced by responders navigating disrupted road networks. To that end, the aim of this study is to compare current logistics planning practices in disaster response organizations in the United States with assumptions found in the literature for online path planning under uncertainty. Data were collected from in-depth interviews with emergency responders across federal, state, private, and nonprofit organizations and coded to identify common planning practices. Findings were then compared to existing literature across several dimensions, including path-planning responsibility, network representation, planning goals, information sources, methods of discovering network status, and knowledge sharing. This study reveals several key differences between current practice and assumptions in online path planning models in literature. Most notably, current practices are unlikely to support models that require algorithms to assume immediate updates on road conditions, run within drivers’ vehicles, rely on knowledge of road status probabilities, or prioritize speed over reliability. Despite the availability of advanced online path planning models in literature, few are implemented in real-world settings due to these limitations. The study suggests that, for research advancements to be practical, practitioners must adopt formal information-sharing practices, while researchers should align models with the operational constraints and objectives of responders. These findings can help align research with practitioners’ needs, potentially improving logistics, expediting disaster response, and ultimately lessening the impact on affected communities.",3,Lenox | Renaissance Waverly Hotel,2. Humanitarian Logistics / Disaster Response,237
803,8533.0,Analyzing the impact of Hurricane Florence on Freight Transportation in North Carolina using Telematics data,Practitioner,"Freight transportation plays a critical role in supply chain. Any disruptions caused by natural or man-made disasters could have a significant impact on freight transportation. There are robust data about the movement of passenger vehicles or people, but there is lack of reliable movement data for freight transportation. This research aims to use telematics technology which tracks truck movements to analyze the impact of hurricane Florence on freight transportation in North Carolina. We retrieved telematics data from RobinSight from September 5 th , 2018 to September 25 th , 2018 that covers time periods of before, during, and after Hurricane Florence. The major highways and road segments that were heavily impacted by hurricane Florence include I-40, I-95, and US highways 17 and 70. The trip and visit data for the trucks travelling across those roads were provided by RobinSight. The collected data contains 587,000 features and was preprocessed and cleaned for analysis of changes in freight traffic flow as roads were closed and reopened. Through analysis of the telematics data, we were able to study the impact of hurricane Florence on freight transportation such as the extent of freight traffic rerouting, the resilience of the network, and how long it took freight movements returned to normal. The findings of this research allows us evaluate the capacity and resilience of the freight transportation network and provide design suggestions for improvement.",4,Lenox | Renaissance Waverly Hotel,2. Humanitarian Logistics / Disaster Response,237
804,5813.0,Designing Scheduled Service Networks for Expeditionary Logistics Operations,Academician,"In this work, we study a scheduled service network design problem applied to expeditionary logistics environments in which transportation costs are negligible compared to unmet or late demand penalties. To model the environment, we consider a multi-commodity capacitated vehicle routing problem with time windows for demand satisfaction. By leveraging the cost-agnostic nature of the problem, we design a branch-and-cut-and-price decomposition approach in which a subset of vehicle routes are generated with LP column generation, and cuts are generated by solving a second-stage separable single-commodity max-flow problem. We demonstrate the performance of our approach against off-the-shelf optimization solvers on realistic expeditionary instances.",1,Lenox | Renaissance Waverly Hotel,17. Routing Optimization,238
805,5939.0,Novel optimization model for vehicle routing with relaxed triangular inequality,Academician,"The Vehicle Routing Problem (VRP) has a wide range of applications, including logistics networks, airline scheduling, pricing strategies, risk management, and more. A key assumption in most research focused on these applications is that the triangle inequality holds, allowing the shortest path cost between nodes to be reliably used. However, there are applications where the triangle inequality does not hold—such as when network congestion causes the travel time of a direct path to exceed that of an indirect path. In such cases, relying on methods that assume the triangle inequality holds can lead to suboptimal solutions, with solution quality degrading as the frequency and severity of violations increase. Additionally, there may be instances where revisiting a node is required to minimize overall travel distance or time. In this work, we develop a mixed integer program (MIP) for a variant of the VRP applied to networks where the triangle inequality may not hold and allow vehicles to revisit nodes if it reduces the overall cost and improve explainability of output without requiring any postprocessing as required in TSP based VRP variants. We prove, and demonstrate through a computational study, that this formulation finds the optimal solution to this problem and can also be used in networks where the triangle inequality does hold (e.g., the traveling salesman problem).",2,Lenox | Renaissance Waverly Hotel,17. Routing Optimization,238
806,6941.0,Optimization of Load-to-Pool Assignments in Shared Trucking Operations with Capacity and Cost Constraints,,"The dynamic landscape of shared trucking necessitates optimization models that enhance load assignment to pools, balancing cost efficiency, capacity utilization, and strategic decision-making. This study introduces a mixed-integer linear programming (MILP) framework tailored for load assignment in shared trucking, with a focus on two key components: pool creation and load assignment. In pool creation, we structure pools to meet specific operational and cost-efficiency goals, incorporating unique constraints on capacity, load compatibility, and cost-effectiveness based on observed demand patterns. The MILP model then utilizes binary decision variables in the load assignment phase, ensuring each load is matched to a pool in a way that minimizes costs while satisfying operational and spatial constraints. To anticipate future impacts of present assignments, the MILP framework employs a forward-looking strategy with a rolling horizon approach, enabling iterative re-optimization as new data becomes available. This adaptability helps to balance immediate cost reduction with long-term operational resilience, accommodating potential variations in load and pool availability over time. Preliminary computational results, based on a case study with real-world demand data from an operating broker will demonstrate the model’s effectiveness in achieving cost-efficient and robust load-to-pool assignments. This approach offers a scalable, sustainable solution for shared trucking operations, supporting both operational and environmental objectives.",3,Lenox | Renaissance Waverly Hotel,17. Routing Optimization,238
807,6838.0,"A Hybrid Truck-Drone Pollution Routing Problem with Variable Truck Speeds, Time Windows and Topographical Considerations",Academician,"The integration of drones into last-mile delivery presents transformative potential for enhancing efficiency, providing a cost-effective, rapid, eco-friendly, and flexible solution for deliveries across urban and rural areas. Consequently, hybrid truck-drone systems have garnered significant attention for optimizing last-mile logistics. Despite extensive studies on drone-aided logistics systems, the environmental and operational dimensions remain underexplored. This research introduces the Truck Multi-Drone Pollution Routing Problem with speed, time window and topographical considerations (TMD-PRP-SWT) to address critical gaps in existing research. Key characteristics of this collaborative system—such as variable truck speed, delivery time windows for priority customers, and slope/elevation characteristics of delivery areas—are incorporated to better capture real-world conditions of last-mile parcel delivery. A mixed integer linear programming (MILP) model is formulated to minimize truck fuel consumption, drone energy expenditures, and driver labor costs. For efficiently solving large instances, an Adaptive Large Neighborhood Search (ALNS) algorithm is developed and validated. Extensive numerical experiments demonstrate substantial cost savings within this collaborative framework. Notably, adjustments in truck speed yield significant cost reductions when topographical characteristics are considered. These findings highlight promising research directions and implications for sustainable last-mile logistics.",4,Lenox | Renaissance Waverly Hotel,17. Routing Optimization,238
808,5417.0,Quantifying Policy Impacts on South Texas Cross-Border Supply Chains,Academician,"The importance of global supply chains was made evident during the COVID-19 pandemic, leading to companies and countries alike focusing on bolstering resilience of their own supply chains. These initiatives aim to improve responses to natural and man-made disruptions. In the United States, trade with its North American neighbors has increased by almost 50%, compared to 2014. Mexico has now become again the largest trading partner for the United States, and this growth underscores the need to understand how U.S. policies impact the cross-border supply chains. This is particularly true for South Texas, a critical hub for U.S.-Mexico trade. This research aims to identify and quantify policies which have a significant impact on different cross-border supply chains, by examining rules and regulations from agencies such as the Department of Homeland Security (DHS), Environmental Protection Agency (EPA), and Texas Department of Transportation (TxDOT). To aid with the streamlining of policy analysis, AI tools are employed to process regulations, categorize areas of influence, and assess their impact on Supply Chain metrics, including recovery time, resilience, and cost efficiency. By using an AI-assisted approach, this research aims to provide a framework to recognize pinpoint contradictions and overlaps that may increase cost and introduce unnecessary burden to the supply chains. The goal is to provide actionable insights for decision makers in their operations of South Texas cross-border supply chains, ensuring they can manage sustainably and effectively amid evolving regulatory and operational challenges.",1,Lenox | Renaissance Waverly Hotel,11. Hyperconnected & Global Supply Chains,239
809,8687.0,Revolutionizing Textile Supply Chains: Harnessing Blockchain for Enhanced Transparency,Academician,"Textile supply chains are characterized by their global reach, fluctuating demand, and short product life cycles, leading to challenges in transparency, operational inefficiency, and consumer trust. These limitations have also raised concerns about ethical practices and sustainability in the industry. This research explores blockchain technology as a potential solution to these issues by enabling secure and traceable data sharing across all stakeholders. Using discrete event simulation, a comparative analysis of blockchain-enabled and traditional supply chains was conducted. The model evaluates key performance indicators such as lead times, service levels, and operational efficiency. The findings reveal that blockchain significantly enhances transparency, traceability, and operational efficiency while also providing tools to monitor environmental impacts like carbon and water footprints. These findings highlight the transformative potential of blockchain technology in revolutionizing textile supply chains.",2,Lenox | Renaissance Waverly Hotel,11. Hyperconnected & Global Supply Chains,239
810,6521.0,Multi-Objective Strategic Network Design Model for Hyperconnected Mobile Supply Chains,Practitioner,"With the increasing demand for high-quality, rapidly available, and cost-effective products, traditional production and delivery models face significant challenges, particularly in industries handling large-sized products, such as modular construction. While the concept of mobile supply chain has been developed and applied in several contexts, existing literature often centers on single-tier production networks and small-scale applications. Our approach expands the scope to encompass multi-tier, multi-party interactions, essential for effective optimization in large-scale supply chains. By connecting multiple mobile production networks across tiers using modular, plug-and-play mobile production units, the framework of Hyperconnected Mobile Supply Chains enables flexible, responsive, and sustainable supply chains aligned with the economic, environmental, and societal goals of the Physical Internet. In this work, we propose a multi-objective mixed integer programming formulation for the strategic network design of Hyperconnected Mobile Supply Chains to serve a dynamic set of customers across extensive territories, leveraging existing open production facilities and determining the location and capacity of supplementary facilities. We propose a metaheuristic-based solution algorithm capable of solving large-scale instances and present experimental results from an industrial case to assess its computational performance and effectiveness relative to established approaches for multi-objective optimization. Our findings highlight the economic and environmental benefits enabled by Physical Internet-driven Hyperconnected Mobile Supply Chains.",3,Lenox | Renaissance Waverly Hotel,11. Hyperconnected & Global Supply Chains,239
811,6930.0,Data-Driven Hyperconnected Supply Chain Networks: Integrating Practical Constraints for Sustainable Development,Academician,"Present supply chain networks operate unsustainably economically, environmentally, and socially. Prior research in hyperconnected networks highlights solutions for many of these issues by creating a multi-tiered network of regional, local, and gateway hubs to connect sets of origin-destination pairs. These networks enhance efficiency, resilience, and sustainability by seamlessly linking transportation hubs, warehousing, and distribution centers, enabling faster, cost-effective, and low-emission goods movement across regions. While hyperconnected network research focuses on theoretical optimizations, it often overlooks spatial, infrastructural, and environmental constraints. Including the location of airports, train yards, ports, major highway intersections, and major metropolises can increase the model's fidelity. Networks based solely on flow may lose out on the practical implementation. There are often regulations surrounding types of facilities and truck movement throughout a city. This could change the design of the network to include two smaller hubs bordering the city vs. one larger hub within the city. Connecting the network to the real word by leveraging GIS and Costar data, this research integrates physical, environmental, and practical constraints encompassing hyperconnected networks. This network’s design will be linked to the United Nations Sustainable Development Goals for reducing emissions and an emissions reduction portfolio will be provided along with a projected timeline to net zero. Further, we assess this approach by applying it to multi-tier hyperconnected network for freight movement in Southeast USA. The results suggest that this approach can readily be extended to larger geographical regions, including nationwide applications or internationally.",4,Lenox | Renaissance Waverly Hotel,11. Hyperconnected & Global Supply Chains,239
812,5133.0,Practical and Effective Heuristics for the Backhaul Profit Maximization Problem,Practitioner,"The backhaul profit maximization problem (BPMP), which is known to be NP-hard, requires simultaneously solving two problems: (1) determining how to route an empty delivery vehicle back from its current location to its depot by a scheduled arrival time, and (2) selecting a profit-maximizing subset of spot-market delivery requests along the route subject to the vehicle’s capacity. The ability to quickly find high-quality solutions to BPMP and related problems gives logistics providers a competitive edge by allowing them to reduce costly deadhead miles (distances traveled with an empty vehicle). Implemented in our computing environment, the fastest known exact algorithm for BPMP requires approximately 11 hours and 44 minutes on average to solve the largest instances in the literature, which have 70 to 80 potential pick-up/drop-off locations. The fastest available heuristic from the literature is considerably faster, and finds high quality solutions, but requires a state-of-the-art mixed-integer programming solver. We present a heuristic framework for the BPMP based on greedy construction, iterative local search, and randomization. Algorithms developed with the framework are implemented in the freely and widely available C++ language and their effectiveness is demonstrated through an extensive computational experiment on both benchmark and randomly generated problem instances. We find that our approach is competitive with approaches from the literature in solution quality as well as running time.",1,Tyndall | Renaissance Waverly Hotel,22. Supply Chain Optimization,240
813,6088.0,Optimizing Last-Mile Delivery for Nonprofits: A Deterministic VRPTW Model with Multi-Driver Allocation,Academician,"The purpose of this research is to develop a robust operational framework tailored to nonprofit and community-based organizations, supporting their unique distribution and logistical needs. The challenges of last-mile delivery intensify for these organizations, especially given their limited resources. ""Meals on Wheels Central Texas,"" a nonprofit focused on serving elderly populations, exemplifies this challenge, as it relies on three types of drivers: volunteers, paid drivers, and staff. The organization prioritizes volunteer drivers to minimize costs, but the unpredictable nature of volunteer availability complicates the delivery process. When volunteers are unable to fulfill their routes, a rapid reassignment to other available drivers becomes essential to ensure continuous service. Our research addresses the vehicle routing problem with time windows (VRPTW), specifically adapted for nonprofits, by creating an optimization model that accommodates different driver types and emphasizes cost-effective route allocation. The model seeks to maximize volunteer participation while maintaining operational flexibility. The model assigns drivers to routes with an emphasis on volunteer allocation to optimize cost savings and considers paid or staff drivers as necessary to maintain coverage. This adaptive model supports the dual objectives of enhancing volunteer engagement and ensuring reliable service. By implementing this VRPTW framework, nonprofits can achieve a sustainable balance between cost management and responsiveness, ultimately strengthening their capacity to serve communities effectively amidst resource constraints.",2,Tyndall | Renaissance Waverly Hotel,22. Supply Chain Optimization,240
814,6637.0,Stochastic Fleet Mix Optimization for Robot-Aided Intralogistics,Academician,"Intralogistics operations, a critical component of the supply chain, are among the most labor-intensive and time-consuming processes that directly impact overall efficiency and responsiveness. In recent years, the growing supply chain challenges, including labor shortages, cost constraints, rising turnover, and increased customer demands, have intensified the need for resilient solutions. The effective integration of autonomous mobile robots (AMRs) has the potential to address these challenges through collaborative automation, where AMRs handle labor-intensive order transport tasks, allowing human workers, who are adept at item retrieval, to focus on this more specialized task. While existing studies have investigated the operational impacts of AMRs in intralogistics, strategic decisions concerning optimal AMR fleet composition remain unaddressed. This research proposes a two-stage stochastic model to minimize both operational and acquisition costs. The first stage addresses the strategic decisions regarding fleet mixing and sizing, while the second stage optimizes operational decisions, specifically order batching, batch assignment to AMRs, and sequencing. A sample average approximation approach is used to solve the model and estimate the expected total cost. Due to the NP-hard nature of the problem, a hybrid metaheuristic approach is developed to efficiently solve realistic instances. The applicability and effectiveness of the proposed method are demonstrated through extensive numerical analysis. In addition, several practical implications on AMR fleet sizing and mixing are also established based on the experiments.",3,Tyndall | Renaissance Waverly Hotel,22. Supply Chain Optimization,240
815,8520.0,"Optimizing Fleet Maintenance in Coal Mining Supply Chains: A Case Study of Samleswari Mines, Odisha, India",Academician,"Coal mining is a crucial part of India’s energy production, with coal as the primary power source. The 24/7 operations at sites like Samleswari Mines in Odisha are prone to disruptions from fluctuating demand, equipment breakdowns, and fuel delivery delays. These disruptions often lead to transportation delays, with significant challenges in managing the vehicle fleet—particularly trucks and payloaders—responsible for transporting coal from the mine face to railway sidings. Fleet uptime is essential, requiring efficient maintenance schedules to prevent breakdowns that cause operational bottlenecks, increased downtime, and missed production targets, potentially resulting in financial penalties. This research optimizes maintenance strategies to reduce downtime and improve fleet utilization at Samleswari Mines. An Agent-Based Model (ABM) is used to simulate the coal supply chain, focusing on fleet maintenance, and integrates variables like vehicle usage, repair schedules, and spare parts inventory. By testing different maintenance strategies within the model, the study identifies the most effective approaches to minimizing downtime and ensuring a steady coal flow. The model also emphasizes inventory management for spare parts to minimize delays caused by component shortages, shortening repair cycles and improving maintenance efficiency. The ultimate goal is to achieve maximum fleet uptime with minimal costs, enabling consistent coal supply and production target fulfillment. This study provides a decision-making tool for enhancing the operational efficiency of coal mining supply chains, offering data-driven insights for implementing effective fleet management practices, ensuring resilience in real-world mining environments.",4,Tyndall | Renaissance Waverly Hotel,22. Supply Chain Optimization,240
816,6861.0,Comparison of Mixed-Integer Linear Programming Models for the Pickup and Delivery Problem with Transshipments and Charging for Delivery Robots,Academician,"The classical pickup and delivery problem (PDP) can be extended to include transshipments, resulting in the pickup and delivery problem with transshipments (PDP-T), where goods can be transferred between vehicles during transit. Adding time window constraints further generalizes this to the pickup and delivery problem with time windows and transshipments (PDPTW-T). With the increasing use of advanced technologies such as drones and delivery robots in last-mile delivery, new operational challenges, such as charging requirements, have emerged. This study aims to develop several mixed-integer linear programming (MILP) formulations for the PDPTW-T, incorporating charging needs for delivery robots at transshipment points. We compare these formulations to assess their efficiency, introducing valid inequalities and applying a branch-and-cut algorithm to solve the models. The performance of each formulation is evaluated through computational experiments, providing insights into their effectiveness for modern delivery systems.",1,Tyndall | Renaissance Waverly Hotel,21. Transportation Optimization,241
817,8597.0,Solving the Strategic Locomotive Assignment Problem: Service Network Design,Academician,"This study develops a strategic Locomotive Assignment Plan (LAP) for one of North America's largest Class I Freight Railroads. Locomotives, as critical assets in rail operations, must be efficiently distributed across the network to meet train schedules and ensure operational reliability. The modeling framework employs a space-time network and integrates integer programming formulations. The strategic LAP identifies optimal weekly locomotive assignments to trains and power-change locations, providing a strong foundation for enhanced decision-making in downstream planning processes. The result is a repeatable weekly assignment plan that achieves network balance by strategically repositioning locomotives through deadheading and light traveling. Light traveling involves a group of locomotives traveling without carrying railcars, solely for repositioning—a costly yet often necessary strategy for achieving network balance. While the number of possible light travel operations is vast, methods are proposed to reduce this set to a small subset that can be incorporated into the space-time network as decision variables. A set of reduction rules is introduced to identify a small, effective subset that guarantees optimality. Additionally, novel heuristic approaches are presented, leveraging the physical structure of the rail network—insights not previously explored in the literature—that improve solution quality.",2,Tyndall | Renaissance Waverly Hotel,21. Transportation Optimization,241
818,9171.0,Trailer Scheduling at Multi-Door Cross-Docks,Academician,"Cross-docks are transshipment facilities used in logistics to consolidate freight by destination. This research addresses the cross-dock scheduling problem for a multi-door facility where inbound trailer arrival times are variable, products are interchangeable, and the objective is to minimize the makespan and the tardiness of outbound trailers that are impacted by a nonlinear penalty function. A mixed-integer programming model is developed to include these features and others such as soft departure deadlines. This problem is well-known to be NP-hard, so heuristics are needed to solve larger problems. A new solution approach is proposed that uses a construction heuristic to produce good-quality starting solutions for a population-based simulated annealing metaheuristic. The results of this model have been animated so that practitioners with no background in the technical aspects of the model or algorithm can vary parameters and watch the impact of the optimal solution on truck arrival and departures.",3,Tyndall | Renaissance Waverly Hotel,21. Transportation Optimization,241
819,5928.0,Optimal End-of-Life Ordering Policies with Multiple Suppliers,Practitioner,"Title: Optimal End-of-Life Ordering Policies with Multiple Suppliers Authors: Ishtiak Sikder, Russell E. King, Robert Locke Institution: Edward P. Fitts Department of Industrial and Systems Engineering, N.C. State University Abstract: The typical life cycle of a given product can generally be divided into three distinct phases based on market demand. The initial “ramp-up” phase would see an upward increase in product demand upon market launch. A steady-state phase then ensues which covers the majority of the product life when demand is stable. The End-of-Life (EOL) phase comes last when demand starts to decline and eventually reaches a point where the product is discontinued. Considering this, we propose an ordering policy which involves potentially switching from a large-scale to a small-scale supplier during the EOL phase to leverage economies of scale and keep total cost at a minimum. As such, we solve for the optimal number of order cycles over a finite EOL horizon and determine a distinct point in time when switching suppliers leads to minimal total cost. Given the switching time, the EOL phase is divided into two portions. For each portion the optimal number of order cycles and order quantities are determined. Costs for each portion are summed to calculate total cost. An algorithm is presented to find the switching point that minimizes total cost.",1,Tyndall | Renaissance Waverly Hotel,5. Supply Chain Economic Planning,242
820,5946.0,Supply Chain Coordination with Profit-sharing,,"Supplier selection, order allocation, and pricing are some of the most important decisions in a supply chain. In this research, we explore the centralized decision-making process in a decentralized three-stage supply chain considering price responsive demand, profit-sharing agreements, and finite production rates to create a realistic schedule for the manufacturing processes in the first two stages. A mixed integer nonlinear programming model is proposed to maximize the overall supply chain profit as well as satisfying the contractual profit-sharing agreement between all participating partners.",2,Tyndall | Renaissance Waverly Hotel,5. Supply Chain Economic Planning,242
821,6717.0,An Economic Equilibrium Model for Reverse Logistics System for  Electric Vehicle Batteries,Academician,"Abstract: The rapid expansion of the electric vehicle (EV) market, along with COVID-19-related supply chain disruptions and environmental concerns from mining, has raised critical questions about the sustainability of raw material supplies for EV batteries (EVBs). As a potential solution, end-of-life (EOL) EVBs offer significant economic and environmental benefits, as they still retain 70-80% of their initial capacity and can be reused towards making new EVBs through re-manufacturing or recycling. To explore the state-of-the-art research on circular economy (CE) and reverse logistics (RL) for EVBs, we reviewed 89 journal publications from January 2012 to May 2023, identifying key research gaps. Addressing these gaps, we develop an economic equilibrium model in RL system for EVBs, investigating the interrelationships among stakeholders, such as EOL EVB collectors, third-party logistics (3PL) providers, recyclers, and EVB manufacturers. First, we formulate an optimization model for each partner, integrating these models through market-clearing conditions. Second, the equilibrium model is constructed as a mixed complementarity problem (MCP) using Karush-Kuhn-Tucker (KKT) conditions and Lagrangian equations. Numerical examples will be presented. Future research directions include spatial analysis and consideration of various EVB types and the recycling technologies (e.g., hydrometallurgy, pyrometallurgy).",3,Tyndall | Renaissance Waverly Hotel,5. Supply Chain Economic Planning,242
822,6752.0,Simulation Model Design for Armored Plate Logistics in Support of the Air Force Adaptive Basing Concept-of-Operations,,"Modern Air Force doctrine is migrating towards an Adaptive Basing (AB) concept of operations. To support AB, equipment, supplies and personnel must be transported rapidly and reliably from a central depot (a Hub) to the Bases. This rapid deployment requires a sophisticated logistics planning tool to develop Course of Actions to satisfy the AB requirements. This study addresses the logistics of moving armor plates for warfighters between overseas theaters and stateside locations. We develop an inventory control and supply chain simulation model to track the plates’ locations throughout their lifecycle, from deployment to retirement. The model incorporates six main inventory stages: plates needing transport from in-theater bases to stateside, plates at the stateside entry point, plates awaiting testing, plates that have completed testing, plates at the stateside exit point, and plates currently in use. These stages help us identify potential bottlenecks in the logistics process. After identifying these bottlenecks, we analyzed the impact of modifying airlift schedules on inventory flow, assessing how changes in movement rates affected supply levels and bottleneck reduction.",4,Tyndall | Renaissance Waverly Hotel,5. Supply Chain Economic Planning,242
823,6168.0,Application of Circular Economy Processes for Sustainable Online Grocery Delivery Services,,"Driven by the increasing environmental concerns and resource scarcity challenges, this paper proposes a circular supply chain methodology to promote sustainability, resource efficiency, and economic resilience. The methodology adapts the established Supply Chain Operations Reference (SCOR) framework, integrating circular economy principles that emphasize resource reutilization, waste reduction, and value retention. The proposed Circular-SCOR methodology provides a standardized approach for organizations to design and optimize their supply chains for the circular economy. The methodology is demonstrated through a case study in the online grocery business. For grocery delivery services, circular economy practices can potentially reduce waste through early planning on inventory, packaging, delivery process, and recycling. As consumer demand for sustainable practices continues to grow, adopting circular models can provide a significant competitive advantage for food delivery and online grocery services.",1,Tyndall | Renaissance Waverly Hotel,27. Sustainable / Circular Supply Chains,243
824,6336.0,Cost-Effective Routing for Perishable and Non-Perishable Grocery Distribution,,"Logistical challenges impose a significant financial burden on global distributors, especially with rising oil prices and evolving consumer demands. This study presents a novel routing scenario aimed at reducing distribution costs for a distributor in Turkiye delivering both perishable and non-perishable grocery items. The proposed scenario incorporates a cost function that accounts for road tariffs, the number of drop-offs at customer sites, and fuel consumption costs. We introduce a novel mathematical model formulated as a Mixed-Integer Linear Program (MILP) to address these interconnected challenges. The research employs a combination of exact and non-exact solution methods, focusing on a two-stage metaheuristic approach that substantially reduces overall costs. This strategy involves breaking down the larger problem into manageable sub-problems, solving them individually, and integrating the solutions into a comprehensive routing plan. Our methodology led to a 4.93% reduction in operational costs compared to the distributor's existing strategy in a real case study from Turkiye, demonstrating its significant potential to enhance profitability and operational efficiency. This study illustrates how advanced routing techniques can reduce costs and improve logistics operations, offering practical solutions for increasing profitability in the industry.",2,Tyndall | Renaissance Waverly Hotel,27. Sustainable / Circular Supply Chains,243
825,8609.0,Circular Economy Models for Urban Logistics: The Role of Bio-Based Packaging in Sustainable Transportation Networks,,"Managing waste, minimizing emissions, and maintaining efficiency in urban logistics is becoming more challenging due to continuous urbanization. Especially with the expansion of e-commerce, logistics models seem to be ineffective in terms of sustainability. The circular economy is such a framework that promotes sustainability by reducing waste by encouraging reuse and recycling across the entire supply chain. Bio-based packaging is usually made from renewable resources and designed to be recyclable again and biodegradable. This article tries to explore the current sustainability scenario in urban logistics the role of suitable bio-based packaging material and its role in circular economy. The main objective of this paper is to explore the feasibility and impact of adopting a circular economy model with a bio-based packaging solution. Here, the authors tried to explore the circular economy principle in practice in urban logistics emphasizing bio-based packaging in the urban logistics sphere by evaluating its impact on the transportation system, cost reduction, ecological benefits, and logistical difficulties. The finding implies that bio-based packaging brings significant environmental, but it requires significant operational adjustment related to infrastructure and reverse logistics. This paper further tries to enhance the understanding of revolutionizing urban logistics with sustainable packaging solutions while promoting towards more towards more efficient and sustainable supply chain.",3,Tyndall | Renaissance Waverly Hotel,27. Sustainable / Circular Supply Chains,243
826,8842.0,Designing a Sustainable Multi-Objective Mixed-Integer Linear Programming (MILP) Model for Shrimp Supply Chains,Academician,"As the world moves toward more sustainable food production, shrimp supply chains face challenges related to balancing cost efficiency and environmental impact. This study develops and optimizes a multi-echelon shrimp closed-loop supply chain (SSC) network using a multi-objective mixed-integer linear programming (MILP) model. The SSC network encompasses shrimp fishers, farms, distribution centers, wholesalers, factories, waste powder factories, customers, and the poultry and livestock market. The model addresses both location-allocation decisions and flow optimization, aiming to minimize total operational costs while integrating an innovative environmental objective to reduce greenhouse gas (GHG) emissions. Key constraints ensure sufficient vehicle availability by determining the optimal number of vehicles required to meet demand within emission limits. The proposed model offers valuable insights for decision-makers in the seafood industry, addressing facility selection, optimal product flows, and waste management. Initial results demonstrate significant cost savings and environmental benefits. The findings highlight the potential for achieving a balance between economic goals and sustainability, contributing to greener and more efficient supply chain practices in the seafood sector. Keywords: Shrimp Supply Chain, Optimization, Sustainability, Mixed-Integer Linear Programming, Location-Allocation",4,Tyndall | Renaissance Waverly Hotel,27. Sustainable / Circular Supply Chains,243
827,6610.0,"Managing Multi-Item, One-Warehouse, Multi-Retailer Systems under Supply Chain Disruptions Using Deep Reinforcement Learning for Dispatch Decisions",,"In complex supply chains, managing inventory and dispatch decisions for multiple items from a single warehouse to multiple retailers becomes incredibly challenging during supply chain disruptions. Traditional optimization methods lack closed-form solutions and cannot capture these environments' dynamic and uncertain nature, making them impractical for real-time decision-making. Additionally, heuristic methods have poor scalability as problem complexity increases. Supply disruptions can occur at both the warehouse and the retailers, further complicating decision-making. There are limited studies on one-warehouse multi-retailer (OWMR) systems with disruptions, which show the need for adaptable solutions. The generality and adaptability of deep reinforcement learning (DRL) are beneficial for managing such supply chains to mitigate the consequences of supply uncertainties. This paper proposes a DRL framework to optimize dispatch decisions in a multi-item OWMR system that faces disruptions. For each item and retailer, the framework uses real-time inventory levels and holding and dispatch costs to model the state of the system. By formulating the dispatch decision problem as a Markov Decision Process, the DRL agent learns policies that minimize total costs while adapting to disruptions. The experimental results will be presented to demonstrate that the DRL approach outperforms conventional heuristics and optimization methods in achieving lower total costs and higher service levels during disruptions. The framework's scalability and adaptability enable it to handle various disruption patterns and demand fluctuations, hence improving resilience and efficiency in supply chain operations.",1,Lenox | Renaissance Waverly Hotel,14. Machine Learning Applications,244
828,6788.0,Adaptive Negotiation Strategy Using Deep Reinforcement learning for Supply Chain Optimization,Academician,"In today’s constantly changing market conditions, effective procurement negotiation is crucial to maintain profitability, supplier relationships, and overall supply chain resilience. The paper aims to extend the existing work on procurement negotiation by introducing an adaptive framework by utilizing deep reinforcement learning to automate and optimize negotiation process in a simulated environment. The proposed approach models the complex nature of supplier-buyer negotiations, incorporating real-time data and dynamic market conditions. A policy-learning mechanism is used to enable more efficient exploration and exploitation which helps in refining actions based on cumulative feedback. The framework is implemented within a simulation environment designed to emulate real-world procurement scenarios, including evolving market demands, supplier constraints, and strategic buyer goals. It dynamically adapts to changes in inventory, demand, market conditions, supplier constraints and delivery times, personalizes negotiation tactics, and simulates multi-agent interactions. Ultimately, the model identifies strategies that drive cost savings, enhance supplier collaboration and align with long term organizational objectives.",2,Lenox | Renaissance Waverly Hotel,14. Machine Learning Applications,244
829,8858.0,Order Release Control for Congestion Mitigation in Material Handling with Autonomous Mobile Robots,Academician,"The rapid adoption of autonomous mobile robots (AMRs) in material handling has significantly improved operational efficiency. However, the increasing deployment of AMRs in warehouses presents a critical risk of congestion, which can substantially degrade overall system performance. This study addresses the challenge of congestion in material handling systems resulting from an excessive number of AMRs. We propose a novel order release control framework that incorporates newly developed congestion metrics, derived using an unsupervised learning method, to dynamically estimate and manage congestion levels in real time. By regulating order releases, the framework effectively controls the number of AMRs operating on the shop floor, mitigating congestion and enhancing system performance. Computational experiments demonstrate that the proposed framework significantly reduces congestion and outperforms systems without order release control, as well as those that neglect congestion considerations. These findings highlight the importance of congestion-aware strategies in optimizing production control systems for material handling operations.",3,Lenox | Renaissance Waverly Hotel,14. Machine Learning Applications,244
830,6430.0,Feasibility of Artificial Intelligence in Basic Warehouse Operations,Practitioner,"Current literature indicates that initial applications of Artificial Intelligence (AI) have improved aspects of operational efficiency and productivity in warehousing. However, these are limited to facilities with the resources to build independent large language models. This study investigates the feasibility of addressing warehousing operational questions through lower-cost AI models. Specifically, it compares prompts in models developed in-house through an open-source chat bot structure versus common publicly available chatbots.",1,Lenox | Renaissance Waverly Hotel,19. Warehousing Applications,245
831,6870.0,Uncovering Key Contributors to Productivity in Warehouse Picking Using Machine Learning Models,Practitioner,"Improving labor productivity in warehouse operations is central to optimizing logistics operations, requiring targeted strategies that address multiple operational variables. This study highlights the impact of key factors, including optimized zoning and slotting, adjustments to order profiles, labor skill improvement, and modifications to picking processes, in enhancing productivity in warehouse operations. To quantify the impact of these variables, we employ various machine learning (ML) models to assess each variable's relative contribution to overall productivity improvements. Comparative analysis of ML model outputs reveals considerate variance, underscoring the complex interactions among these variables and the importance of selecting models tailored to specific operational settings. Through detailed examination, we clarify the functional significance of each factor, linking its role directly to observed productivity gains. The findings highlight opportunities for future advancements, including refining worker skill training, enhancing order profiling techniques, and further optimizing slotting and zoning strategies. This research presents a scalable, data-driven framework for sustained productivity improvements in warehouse operations, enabling warehouses to enhance labor productivity and support the development of more efficient and adaptable warehousing processes.",2,Lenox | Renaissance Waverly Hotel,19. Warehousing Applications,245
832,6926.0,Comparative Analysis of Analytical Models for Labor Management Standards in Warehousing: A Machine Learning Approach,Practitioner,"This study aims to establish new labor management (LM) standards for specific warehouse processes to enhance workforce productivity and operational efficiency. Through detailed LM observations and the use of a range of analytical models including regression analysis and various machine learning (ML) algorithms the study evaluates each model’s effectiveness in determining accurate LM standards and compares these results to traditional manual benchmarks. Comparative analysis is conducted to assess the accuracy and reliability of model-generated values and to examine the confidence levels associated with each model. This approach reveals key factors driving variability, providing insights into the limitations and strengths of each analytical method. The study concludes by identifying the most reliable method, based on its alignment with observed standards and other operational criteria, and proposes it as a practical alternative to manual standard-setting. This research offers a robust, data-driven framework for scalable LM standard creation, supporting increased labor productivity and greater consistency in warehouse operations.",3,Lenox | Renaissance Waverly Hotel,19. Warehousing Applications,245
833,6832.0,A Synergistic Truck-Drone-Robot System for Last-Mile Delivery with Hand-Off Points and Microstations for Dynamic Repositioning,Academician,"Recent advancements in last-mile delivery have accelerated the adoption of drones and autonomous delivery robots (ADRs), leading to significant operational and technological improvements. While numerous studies have explored collaborative truck-drone or truck-robot systems, research seldom considers both aerial and ground drones within a unified collaborative framework. This study proposes a novel collaborative delivery system incorporating a truck with multiple aerial drones and ground robots within a network of microstations. These microstations serve as hubs for drones and robots, enabling flexible last-mile delivery support. The drones and robots autonomously reposition to various hand-off points to retrieve packages from the truck and deliver them to customers. Unlike previous approaches, the proposed system eliminates truck waiting times for retrieving drones or ADRs after parallel deliveries. A novel optimization-enabled adaptive simulated annealing algorithm is proposed to minimize the delivery completion time of the truck-drone-robot delivery system. Extensive computational experiments demonstrate substantial time savings achieved through this collaborative framework. Furthermore, we benchmark the proposed system against a setting in which the drone and robots are transported by the truck. Our findings demonstrate the potential for significant efficiency gains in last-mile delivery through the integrated use of aerial and ground drones alongside traditional trucks.",1,Tyndall | Renaissance Waverly Hotel,4. Hybrid Fleet Deliveries,246
834,8781.0,A Hybrid Last-Mile Delivery System Using a Truck and Diverse Robotic Fleet,Academician,"The integration of autonomous robots into last-mile delivery systems offers a promising solution to the limitations of conventional truck-based logistics. This study investigates a hybrid last-mile delivery system utilizing both homogeneous and heterogeneous robots, addressing scenarios in urban areas. To manage battery constraints for robots returning after rendezvousing with a truck, we propose mixed-integer linear programming models that incorporate battery recharging and swapping strategies. Our work explores optimal solutions for truck and robot routes and evaluates different robot combinations to minimize total delivery time. We also develop a three-phased heuristic algorithm to efficiently solve large-scale problems. The models are validated using case studies in Ulsan Metropolitan City, South Korea. It demonstrates that the battery-swapping strategy can reduce total delivery time by up to 50% compared to truck-only systems. The findings highlight the significance of extended robot driving ranges in urban areas to enhance delivery performance. A sensitivity analysis further examines the effects of factors such as customer node size, robot speed, number of robots, and robot ratios. This study addresses key gaps in the literature on hybrid truck-robot delivery systems and provides practical insights to enhance efficiency and reduce delivery times in urban areas.",2,Tyndall | Renaissance Waverly Hotel,4. Hybrid Fleet Deliveries,246
835,5231.0,Resilient Logistics: Strategic Planning for UAV Deliveries with Interdependent Networks,,"The utilization of unmanned aerial vehicles (UAVs) is a rapidly growing trend in developed countries due to their efficiency. However, compared to ground vehicles, UAVs are more vulnerable to cyberattacks since they rely on communication networks to move from one point to another. On the other hand, logistics distribution uses UAVs to transport goods from distribution centers of multiple suppliers to consumer locations. Hence, the delivery strategy relies on the state of the distribution system, which connects suppliers, distribution centers, and customers through interdependent road infrastructure and UAV communication network. This study aims to provide the optimal delivery strategy when the communication network controlling the UAVs malfunctions. Due to its interdependent nature, the strategy for selecting suppliers, distribution centers, and distribution paths to consumer locations could be different before and after the malfunction. We develop a mathematical model to find the optimal logistics strategy and solve it with multiple, randomly generated, datasets. The results indicate that the obtained strategic approach to goods allocation and distribution planning can significantly mitigate the impacts of communication failures.",3,Tyndall | Renaissance Waverly Hotel,4. Hybrid Fleet Deliveries,246
836,6834.0,Human-Machine Teaming Function Allocation: A Systematic Review,,"With the rapid development of intelligent technology, humans and machines are working collaboratively as a team to accomplish their work. Research on human-machine teaming (HMT) has emerged and received scholars’ attention in recent years. Most research studies in this field focus on computers and control engineering, where scholars explore the application of HMT from a technical perspective. This study aims to synthesize the existing empirical research on HMT with a specific focus on function allocation. Both bibliometric and thematic review approaches were adopted. Keyword search was performed, and 149 papers were selected for the bibliometric analysis. Network analysis was performed to identify co-authorship scenarios in the literature. Thematic analysis was performed for 17 papers. The results inform that the research on HMT is targeted on five aspects of functional allocation: integration and coordination, trust and cooperation, approach to functional allocation, functional allocation design framework, and human safety perspective. This review offers insights into the current state-of-the-art research on functional allocation in HMT from the findings of both theoretical and empirical research which will help managers in taking engineered decisions regarding task assignment in a smart system where human and machines work as teams. Important future research includes identifying mechanisms linking human-machine team input to team output variables on a specific function allocation basis.",1,111 | Cobb Galleria Centre,SEMS Best Student Paper Competition,247
837,6714.0,Driving Change Success: The Organizational Change Capability Measurement Scale in Practice,Practitioner,"To predict the success rate of change programs in public organizations, our previous research developed a scientifically validated scale for measuring the organizational change capability of public organizations. The scale comprises 77 items across fifteen components. This study applies the scale within a military context to examine and compare the fifteen components, focusing on identifying which components have shown improvement and which have remained stable over time. For this purpose, data was collected twice over a two-year period from a single military organization, which is about to undergo the amalgamation of five entities across different regional locations. The longitudinal data enabled the analysis of the evolution of the fifteen components, leading to the identification of both promising shifts and surprising stabilities with respect to the investigated organizational change capabilities. Additionally, this study enables an assessment of changes in effectiveness by comparing the levels of change capabilities between the two measurement points. Key findings underscore the importance of strategic planning, stakeholder management, and organizational culture, as well as the challenges posed by bureaucratic inertia, all of which appear instrumental to the success of the organization’s amalgamation project. As the longitudinal data enabled to identify actionable insights for enhancing change success within a military unit, the key findings are finally framed within the broader context of organizational change in public organizations, calling for additional case studies across public organizations to further validate and generalize our findings.",2,111 | Cobb Galleria Centre,SEMS Best Student Paper Competition,247
838,8973.0,Systemic Interactions of Employees and Organizations: The Management and Career Agency Mismatch,Academician,"The successful execution of work relies on many factors that engineers have studied for decades. However, alongside the execution of work, employees face daily workplace challenges such as burnout, cognitive dissonance, and job fatigue, which, if left unaddressed, can hinder effective work execution and lead to increased employee turnover. One underlying cause of these challenges may be the mismatch between the successful completion of work tasks and the sense of agency employees feel in the workplace. Career agency, a crucial workplace factor, is often understood to be not applicable to the general workforce, as practitioners typically focus on the individual circumstances of each employee. These individual factors do not fully capture the holistic workplace dynamics that may inhibit the execution of career agency. To better understand these dynamics, this research employs systemic principles and practices to: 1) Identify the mechanisms that hinder the successful execution of career agency at the intersection of employees and organizations, 2) Examine the relationship between employee management level and career agency that these mechanisms highlight and 3) Investigate how these interacting mechanisms may be intensified by organizational pressures at higher levels. This research has the potential to inform effective work design and shed light on the often-overlooked organizational dynamics that may impede the career development and execution of both managers and individual contributors.",3,111 | Cobb Galleria Centre,SEMS Best Student Paper Competition,247
839,8522.0,"""Human-Machine Collaboration in Mining: A Critical Review of Emerging Frontiers of Intelligence Systems in the Mining Industry”",Practitioner,"Mining Industry 4.0 is creating a new paradigm of remote work, threatening the potential of lost jobs. As automation is being introduced in mining, it is creating a shift where more and more skilled operators are operating remotely conjuring up the specter of the potential of lost jobs and upheaval for the majority of workers. However, the future looks more promising than many have predicted. Our research focus on change management strategies to re-orient the thinking and practice by current and future professionals, including top and middle-level management to manage and execute this new paradigm. Mining industry engagement to identify barriers to technology integration and design success strategies including use-inspired learning modules. Using a preliminary literature review approach, we discuss enabling technologies facilitating human-machine collaboration along the mining life cycle, their impacts and synthesize the future of an industry where human and machine collaborate successfully to the benefit of humans. More than 70% of organizational change initiative efforts fail because of poor management of the change process. Contribute to guidelines for human-machine collaboration and offer insights for policymakers, industry stakeholders, and researchers regarding future directions.",4,111 | Cobb Galleria Centre,SEMS Best Student Paper Competition,247
840,4887.0,Sustainability of Industry 4.0/Smart Manufacturing: A New Concept of Psychology in Manufacturing,Academician,"Although sustainability is still considered the cornerstone and backbone of manufacturing industries, the sustainability of Industry 4.0 (I4.0) has become a buzzword in manufacturing environments, particularly after adopting and implementing emerging manufacturing technologies (EMT). The EMT is characterized by advanced manufacturing technologies (AMT) and advanced digital technologies (ADT). Sustainability for Industry 4.0 ( in manufacturing industries, in common, is separated into three primary streams or layers through a new concept of psychology in manufacturing. The major goal of this paper is to discover these layers and the psychological interface between them. These layers are represented by the adoption of emerging technologies as the first layer which is called a core layer and is dependable on the adoption of advanced manufacturing and digital technologies. The second layer is the chosen layer which is based on feasible emerging technologies selection for Industry 4.0. The third layer which is known as a given layer is implementing and site readiness of Industry 4.0. All these layers constitute manufacturing industries. Although these layers coincide or overlap at one point, there's a psychological link between them. A novel assessment for measuring the sustainability index of each layer individually and aggregated later will be analyzed and presented considering a proposed psychological perspective.",1,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
841,4929.0,A Comprehensive Guide to Developing a Lean Six Sigma Yellow Belt Training Program,Practitioner,"A Lean Six Sigma Yellow Belt training program is foundational to creating a robust certification program that allows for increased exposure to Lean and 6-sigma principles throughout an integrated healthcare system. At Cook Children’s four certification programs were developed – Lean Practitioner (LP), Lean Six Sigma Yellow Belt (LSSYB), LSSYB for leaders (LSSYB-L), and Lean Six Sigma Green Belt (LSSGB). A master black belt oversees all programs. The Council for Six Sigma Certification accredits two programs (LSSYB, LSSGB). Participants complete a combination of theory, assessment, and practical application of the LSS methodology with support from assigned mentors. All programs incorporate simulation, case studies, and/or Gemba walks of actual healthcare processes to integrate learning experiences into practice. Participants in all programs participate in projects or project-based deliverables. This presentation will focus on giving a deep dive into the structure of the LSSYB program. Since certification programs started in 2017, 35 employees completed their LP, 162 completed LSSYB, 22 completed LSSYB-L, and 40 completed LSSGB. Participants represented 105 departments and 5 companies. PI education touchpoints have grown 367% - from 330 to 1544. Certification participants achieved over $2.4 million in financial impact plus over $3.6 million in estimated impact. Certification project outcomes include improvement in inventory waste, time savings, staff wellness, and patient satisfaction, throughput, and outcomes. Through a certification program deployment, staff have pursued certification, resulting in certified staff members who lead and participate in engagements within their own functional area, resulting in a more streamlined and comprehensive continuous improvement culture.",2,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
842,5126.0,Towards an introduction of a lumbar-assist exoskeleton in the agriculture industry: A pilot test,Academician,"Lumbar-assist exoskeleton is gaining attention as an ergonomic intervention to reduce low back stress. Previous studies have shown the effectiveness of lumbar-assist exoskeletons in various industries, but their impact on the agriculture industry remains unclear. Our laboratory has demonstrated that exoskeleton use can reduce low back muscle fatigue during a 3-minute bush-crop harvesting task, but there is no published data on the effectiveness of exoskeleton under uneven ground surfaces (often seen in the agriculture industry). The focus of this pilot test was to explore the effect of a passive lumbar-assist exoskeleton on low back biomechanical responses under different harvesting trunk postures and sloped ground surfaces. Noting the active and passive tissue contributions to trunk extensor moments vary with the combination of those two variables, it was necessary to investigate the effectiveness of the exoskeleton on both active and passive tissue loads. Six human subjects were asked to maintain simulated static harvesting trunk postures (stooping, squatting, kneeling) on sloped ground surfaces (20° incline, -20° decline, 0° flat), with and without exoskeleton. Lumbar muscle activity and passive extensor moment were employed as dependent variables. Results showed the exoskeleton was effective in reducing the passive extensor moment—the magnitude of reduction increased in the order of kneeling, squatting, and stooping—and in reducing the lumbar muscle activity in kneeling and squatting. The exoskeleton consistently reduced the passive extensor moment regardless of the sloped ground surfaces but further studies are needed for the lumbar muscle activity due to inter-subject variability.",3,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
843,5592.0,Spatial-Temporal Dynamics of Guinea Worm Disease: Risk and Intervention Simulations in Chad,Practitioner,"Guinea worm disease (GWD) continues to pose significant public health challenges, especially in Chad, where unique environmental and epidemiological factors complicate eradication efforts. Our research investigates the interplay of environmental variables—precipitation, temperature, water coverage, and population density—and their roles in facilitating GWD transmission. By integrating Guinea worm case reports and genetic profiles of the parasite, we develop spatial-temporal models that assess the underlying risk factors contributing to GWD spread. These models are designed to capture complex interactions across multiple hosts, including humans and domestic animals, and to quantify the influence of environmental determinants on disease dynamics. Through rigorous analysis, we examine how environmental conditions and human-animal interactions impact transmission, contributing to a nuanced understanding of GWD epidemiology. Additionally, we employ model simulations to evaluate the efficacy of various intervention strategies under differing environmental and interaction scenarios. This simulation-based approach provides critical insights, enabling us to assess potential outcomes and refine public health responses. Ultimately, our work aims to inform and support policy development, offering a data-driven foundation for interventions that are tailored to the environmental and epidemiological contexts of Chad. By addressing these complex interactions, we strive to advance efforts toward the eradication of GWD, paving the way for a healthier future in Chad and beyond.",4,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
844,5701.0,A Two-Stage Deep Learning Framework for Multivariate Air Quality Time Series Forecasting,Academician,"Accurate analysis of air quality time series data is crucial for formulating and implementing effective air pollution control policies. However, in real-world applications, temporal air quality measurements often face the issues of missing values due to environmental conditions, sensor limitations, and maintenance interruptions. Such issues can hinder the accuracy of air quality predictions, as missing values disrupt the continuity and reliability of time series data. In this study, we propose a two-stage air quality time series forecasting framework based on deep learning approaches to enhance both data usability and model performance. In the first stage, our framework addresses the irregular intervals in air quality data by applying various time series imputation models to effectively impute missing values. In the second stage, we train forecasting models on the imputed data, systematically optimizing the combinations of imputation and forecasting models to determine the best-performing pairs. We used data including hourly collected air quality measurements such as fine particulate matters (e.g., PM10 and PM2.5) and meteorological factors like temperature and wind direction from 2021 to 2023 at Daegu and Kyungpook regions in Korea. Our experimental results demonstrated that the combination of transformer-based imputation and TSMixer significantly outperforms other baseline methods in terms of predictive accuracy. Our framework can identify optimal imputation-forecasting configurations that maintain high predictive accuracy and robustness, even in the presence of missing information in time series air quality data, making it a valuable tool for data-driven air quality management.",5,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
845,5751.0,Activity Tracking and Perceived Loneliness in People with Visual Disabilities,Academician,"Loneliness is prevalent among individuals with visual disabilities due to social and mobility barriers, compounded by inaccessible traditional assessment methods. This study introduces a Kinect-based in-home gait tracking system to monitor loneliness in individuals with visual disabilities, drawing on James-Lange Theory linking physical activity to emotions and Bandura’s Social Cognitive Theory connecting self-awareness to improved self-care. The study involved 13 sighted individuals and 14 individuals with visual disabilities. At each participant’s home, a Kinect device connected to a desktop computer tracked gait. Participants accessed their walking data via the NVDA screen reader over approximately 14 days. Participants completed the UCLA Loneliness Scale before and after the study. Kruskal-Wallis tests indicated a significant difference in perceived loneliness among participants after using the Kinect system, H(3) = 10.98, p = 0.01. According to post-hoc Mann-Whitney U tests, among participants who used the Kinect system, those with visual disabilities reported significantly lower perceived loneliness than their sighted counterparts, U = 8.5, z = -1.948, p = 0.05. The Kinect system is likely more beneficial for individuals with visual disabilities than for sighted individuals. Additionally, among participants with visual disabilities, those who used the Kinect showed significantly lower loneliness than those who did not use the Kinect, U = 2.50, z = -2.798, p = 0.004. The Kinect system serves as an effective intervention for reducing loneliness in individuals with visual disabilities. In conclusion, in-home gait tracking potentially contributes to effectively managing loneliness in individuals with visual disabilities, thereby positively impacting their emotional well-being.",6,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
846,5931.0,"Video-Based Task Analyses of User Actions When Assembling, Donning, Doffing and Disassembling Occupational Exoskeletons",Academician,"Exoskeletons have emerged as a promising solution to alleviate musculoskeletal injuries in industrial workplaces. But, before successfully using an exoskeleton for work, the device must be assembled, and the worker must don it. After use, the worker must doff, disassemble and store it. If exoskeleton designs do not enable workers to easily, and correctly assemble, don, doff and disassemble and store the devices, even if exoskeletons are scientifically proven to be effective in reducing injuries, workers and industry may not adopt and use them. Assembling, donning, doffing, and disassembling and storing the devices must not take too long, or be confusing, or be error prone for workers to perform, or require extensive help and assistance. To understand how exoskeleton designs impact user difficulty when assembling, donning, doffing and disassembling exoskeletons, we qualitatively analyzed and coded visual indicators of user confusion from videos of 29 participants assembling, donning, doffing and disassembling a back support, a shoulder support, a sit-stand support, and a handgrip strength support exoskeleton. Results indicate that the common sources of confusion among participants include locating, orienting and placing parts when assembling the exoskeletons, interpreting labels when assembling and donning, connecting parts during assembly, and sizing for fit when assembling and donning. The assembly task results in the most instances of confusion among users across exoskeletons. The back and the shoulder support devices with many component parts result in most user confusion.",7,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
847,5932.0,"Quantitative Assessment of Assembly, Donning, Doffing and Disassembly Task Completion Performance Metrics in Occupational Exoskeletons",Academician,"Before successfully using an exoskeleton for work tasks, the device must be assembled, and donned. After use, the worker must doff and disassemble it. Exoskeleton design elements including the number of parts, and how intuitive the design is, among others, can impact the speed and ease of these tasks. The time needed for these non-work tasks, can turn off and discourage industries from adopting these devices. We evaluated the time taken for assembly, donning, doffing and disassembling 4 exoskeletons – a back, a shoulder, a sit-stand, and a grip strength support device. 29 participants assembled, donned, doffed and disassembled the 4 devices. Task completion times and rates were quantified. Two-way ANOVA results reveal significant main effects for both exoskeleton type (F = 178.92, p < 0.001) and task type (F = 245.67, p < 0.001), with a significant interaction effects (F = 34.56, p < 0.001). The grip strength device took the least time to assemble (mean = 6.8 minutes, 95% CI [5.9, 7.7]), significantly faster than both back support (23.5 minutes [20.8, 26.2]) and shoulder support devices (18.7 minutes [16.4, 21.0]). Participants achieved more than 90% completion rates across all tasks with the grip support and sit stand support devices, with lower completion rates for the back support and shoulder support devices (only ~70-75%) for assembly tasks. Effect size analysis revealed large differences between exoskeletons in assembly tasks (Cohen's d = 1.82 for grip support vs. back support), but smaller differences in doffing tasks (d < 0.3).",8,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
848,5978.0,Leveraging Synthetic Data for Efficient Training of AI Models for Real-World Object Detection,Academician,"Modern computer vision systems largely depend on real-world data for training, which is costly in terms of time, materials, and resources. As industries push toward automation and AI-driven solutions, the need for more efficient training methods is growing. The primary aim of this work is to use synthetic images generated from 3D models to train a computer vision model capable of real-world object detection. This approach seeks to reduce the time, cost, and resources typically required for training AI models with real-world data. This proposal presents a method where Blender will be used to create 3D environments populated with CAD models of prismatic geometries, generating synthetic images with color and depth information. These images will then be used to train a model using Detectron2 to recognize a single object and extract features, focusing on key point localization, and object segmentation. The expected outcome is a robust AI model that can detect and identify real-world prismatic objects using training data derived from synthetic 3D models. This model aims to demonstrate that synthetic data can effectively replace real-world data in certain applications. This approach holds potential to enhance the efficiency of computer vision systems, particularly in automation and quality inspection processes.",9,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
849,6005.0,Key Factors for an Effective Experiential Learning in Industrial and Systems Engineering Capstone Courses,Academician,"Experiential learning can be an invaluable part of an undergraduate student’s education, providing students with hands-on experience that complements classroom learning. In addition to internships, co-ops, and group projects, another way to obtain experiential learning is through a Senior Design Capstone course. Recent studies have explored the benefits of senior design capstone courses; however, a gap exists in identifying the different types of projects in which students engage, and the benefits derived from an Industrial and Systems Engineering Senior Design Capstone course. This study aims to: 1) categorize the different types of projects in which students choose to engage, 2) map course subjects and skillsets utilized in these senior design capstone projects, 3) contrast best practices for teaching a senior design capstone project, and 4) provide insights on achievements gained through the senior design capstone course. The research team analyzed five years of our Industrial and Systems Engineering Senior Design Capstone projects using statistical analysis techniques such as descriptive statistics and Chi-square analysis to identify patterns, trends, and relationships with respect to achieving the five objectives. Our results show that teams who select industry projects, start planning early, leveraging team members’ strengths, and work cohesively with clear roles tend to perform better in oral presentations, reports, and video deliverables – often leading to recognition or awards.",10,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
850,6071.0,Bridging System Methodologies and Innovation to Drive Operational Savings in Automotive Supply,Practitioner,"In collaboration with a local developing automotive packaging manufacturer in a high-demand market, our initiative focused on operational efficiency through the application of systemic methodologies and statistical process evaluation. Utilizing a soft systems approach, we identified key bottlenecks and needs by integrating perspectives from operators and management. Statistical analysis then quantified critical process variables, creating a strong base for targeted innovation. Two distinct approaches were employed: ""Streamlined Communication System"" under Performance Excellence, and ""Productivity-Aimed Layout Redesign"" within Facilities Design and Planning. The communication initiative enables real-time information flow across MELA’s three industrial units, minimizing idle production time and ensuring a seamless workflow. The layout redesign initiative, focused on reducing transportation distances within the facility, simplifying material routing; reducing total cycle time for all processes. The outcomes of this project included the successful implementation of these two innovative strategies, resulting in measurable improvements and saving estimated at $6,000 USD per year. These findings demonstrate the power of combining structured diagnostic tools with data-driven methods to inform sustainable and effective innovation in industrial contexts. This study offers insights aligned with current megatrends, emphasizing efficient resource utilization in environments where supply and demand are imbalanced. It showcases the application of academic theory to real-world challenges, illustrating the potential of early-stage engineering intervention in advancing operational excellence. The project offers a model for impactful, scalable improvements in manufacturing.",11,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
851,6153.0,A systematic literature review of circular economy in the construction industry’s supply chain -A contingency analysis,Academician,"The construction industry is one of the most polluting and resource-intensive industries. The industry’s continuous adoption of unsustainable consumption and production practices has led to an increase in carbon dioxide emissions, excessive waste generation, and other sustainability issues. The integration of circular economy into the industry's supply chain has been identified as the most effective approach to address the unstainable consumption and production in the industry. Hence research on the intersections of circular economy, construction industry, and supply chain management has been receiving increasing attention in recent years. However, only a few of the existing research are literature reviews. Additionally, these existing literature reviews lack a holistic overview of the research area and are mostly descriptive. Therefore, to address this gap, this research employs the Scientific Procedures and Rationales for Systematic Literature Reviews protocol to provide a holistic overview of research on the intersection of circular economy, construction industry, and supply chain management. A contingency analysis was conducted in Statistical Package for the Social Sciences (SPSS) using findings from the content analysis of the 21 selected articles. Based on the results of the contingency analysis, this research proposes a conceptual model for circular supply chain research within the construction industry and identifies key propositions for future research.",12,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
852,6381.0,"Optimizing Economy Class Seating on the Boeing 787: Integrating Ergonomics, Comfort, and Sustainability",Practitioner,"The conducted study will explore the integration of ergonomics, human factors, and sustainable manufacturing practices in designing economy-class seating for American Airlines’ Boeing 787. Focused on improving passenger comfort, health, and sustainability, the research will investigate how ergonomic principles can enhance long-haul flight experiences while adhering to industry standards and minimizing environmental impact. Key design elements such as seat width, pitch, lumbar support, recline mechanisms, and cushioning are analyzed from an ergonomic perspective. The study highlights how these factors reduce passengers' discomfort, fatigue, and physical strain during extended flights. Additionally, the research considers the importance of accommodating a range of body types and ensuring accessibility for passengers with disabilities. Sustainability is a central theme, with the research examining the use of lightweight, recyclable materials that reduce aircraft weight, improving fuel efficiency. Modular seat design is also explored for its potential to simplify maintenance and recycling, extending product life and minimizing waste. The study suggests the use of renewable materials to reduce the carbon footprint of seat production, without compromising durability or comfort. The findings provide actionable recommendations for improving economy-class seating design on the Boeing 787, combining ergonomic enhancements with sustainable manufacturing techniques. This research offers a pathway to creating a more comfortable, efficient, and environmentally responsible air travel experience, benefiting both passengers and airlines while supporting broader environmental goals.",13,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
853,6393.0,Data-Driven Optimization for Clinical Treatment Guidelines,Academician,"Evidence-based guidelines play an important role in informing how chronic diseases should be managed, as these recommendations are widely disseminated and widely implemented. However, they are one-size-fits-most, failing to account for patient-to-patient differences. Personalized medicine has shown significant potential to improve health outcomes over clinical practice guidelines. However, the implementation of personalized medicine may be challenging, resulting in unwanted practice variation and suboptimal patient care. To find the right balance between personalized medicine and clinical guidelines, we propose the treatment guideline design problem. We develop a framework to design optimal treatment guidelines for a population of patients which are each modeled according to their own Markov Decision Process parameters. We propose several exact and heuristic methods to solve the problem, including a mixed-integer linear program formulation and a branch-and-bound clustering algorithm. Our results on a hypertension treatment planning case study demonstrate that with only a small number of subgroups, we can develop clinical practice guidelines which perform almost as well as personalized treatment policies.",14,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
854,6428.0,"Optimizing  workflows, minimizing inefficiencies, and improving productivity and job satisfaction in BD Humacao",Practitioner,"This study at Becton Dickinson (BD) Humacao focuses on optimizing workflows and improving productivity through a three-phase Job Design and Work Measurement Project. Phase I analyze a repetitive assembly process using stopwatch techniques. Phase II introduces process improvements via Methods-Time Measurement (MTM). Phase III evaluates non-repetitive tasks in the Document Control department using work sampling. The overarching goal is to optimize workflows, minimize inefficiencies, and improve productivity and job satisfaction. Phase I used the stopwatch technique to analyze the assembly process for the CapSure Permanent Fixation System, identifying workflow bottlenecks and calculating standard times for key elements. Phase II applied MTM analysis to propose improvements, including a semi-automatic hydraulic system and an ergonomic workstation layout to reduce fatigue. In Phase III, a work sampling study in the Document Control department evaluated task efficiency, analyzing productive and non-productive activities with P-charts, Pareto analysis, and ILO/Westinghouse methodologies. In Phase I, process diagrams identified improvements, especially in the manual spinning wheel operation. Phase II's MTM analysis and ergonomic changes led to a 48.5% reduction in standard time and a 94.07% efficiency increase, raising production capacity from 64.39 to 124.96 units. The ergonomic improvements also enhanced operator comfort. Phase III's work sampling showed technicians spent 82.5% of their time on productive tasks, with 17.5% non-productive. Recommendations were made for time management tools and ergonomic enhancements to reduce inefficiencies. The study demonstrates how combining time and motion analysis with ergonomic improvements boosts productivity and job satisfaction.",15,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
855,6432.0,Boosting Bakery Efficiency: A Time Study and Operational Improvement at Pannera Bakery,Practitioner,"This study, conducted at Pannera Bakery on Las Cumbres Avenue in San Juan, Puerto Rico, involved a two-phase field project to assess and improve workflow efficiency in key customer service operations. In the first two phases, 100 time samples were collected to analyze tasks such as order intake, food preparation, and order dispatch. After identifying and discarding a single outlier, the observed time for the primary operation was recorded at 2.17 minutes. Using the Westinghouse method with a leveling factor of 112%, the adjusted normal time was calculated as 2.43 minutes, and the standard time, with a compensation factor of 0.187, was 2.88 minutes. To enhance performance, several recommendations were proposed, including adding an additional cash register, implementing Kanban and 5S methodologies, and incorporating ergonomic tools (e.g., an extractor, anti-fatigue mat, and fan) to minimize employee fatigue. These enhancements are projected to require an investment of $3,606.84. The project also aligns with the United Nations Sustainable Development Goal 8, promoting decent work and economic growth through improved operational practices and employee well-being. In Phase 2, a comparison of observed and Methods-Time Measurement (MTM) analysis revealed an improvement in the operation's standard time from 0.82 minutes to 0.75 minutes—a 4% gain. The findings illustrate the impact of structured time-study methods and targeted improvements, providing a model for sustainable, efficiency-focused advancements within bakery operations.",16,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
856,6435.0,Optimizing Sterilization: Enhancing Efficiency and Sustainability in Industrial Operations at Baxter Aibonito,Practitioner,"This study examines operational efficiency for the sterilization engineer role at Baxter Aibonito, employing the International Labour Organization (ILO) methodology to assess and enhance work conditions. By evaluating environmental factors, task repetitiveness, physical effort, and posture, the research aims to improve both the working conditions and overall productivity of the sterilization engineer. This assessment addresses the need for a structured system that ensures high efficiency while supporting sustainable performance. Findings indicate that the total score for evaluated factors reached 155 points, accounting for a 28.5% adjustment in standard operational time to address fatigue and potential delays. Recommendations include integrating office hours into the workday to reduce downtime and optimize operational efficiency, leading to improvements in task planning and execution. These interventions underscore the potential for productivity gains through targeted adjustments. The study concludes that applying the ILO methodology is effective in enhancing productivity and workplace quality for the sterilization engineer role at Baxter Aibonito. This approach aligns with the United Nations Sustainable Development Goal 9, promoting innovation and resilient industrial infrastructure. The results demonstrate that an integrated framework for ergonomic assessment and operational adjustments significantly supports employee efficiency and workplace sustainability. Further, it emphasizes the importance of continual evaluation and iterative adjustments to maintain effectiveness and ensure long-term sustainability in industrial roles.",17,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
857,6436.0,CareRegTemp: Precision Temperature Control for Accurate Sample Preservation in Healthcare,Practitioner,"This study proposes the development of ""CareRegTemp,"" an instrument designed to enhance temperature control for preserving medical and laboratory samples, thereby improving accuracy in test results. Effective preservation is dependent on quality data collection, careful handling, and precise temperature management. To gauge market demand and product viability, a survey of 52 healthcare professionals assessed key factors such as preferred price, anticipated volume, and demographics. Findings show demand predominantly from age groups 18-35 and 46-55, with 59.6% of respondents emphasizing the importance of proper sample temperature maintenance. While the majority of participants preferred a price range between $65 and $80, resulting in a set price of $75, January emerged as the ideal purchase month. Based on Puerto Rico’s 2020 census and healthcare employment statistics, the annual demand for CareRegTemp in 2025 was estimated at 216,999 units. The study further analyzed materials requirements, calculating Economic Order Quantity (EOQ) and Safety Stock levels. Key components include caps, glass and stainless-steel test tubes, and silicone grips, each with a 6-week lead time and a 95% service level to maintain a Safety Stock of 42,966 units. The Master Production Schedule (MPS) indicates a distribution strategy aligning inventory availability at month-start, and the Materials Requirement Planning (MRP) system schedules production over 63 weeks to meet demand. To initiate production for 2025, materials ordering should commence in October 2024, ensuring inventory availability by January 2025. This structured approach supports reliable delivery and customer satisfaction.",18,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
858,6443.0,Evaluating Healthcare Service Quality through Patient Satisfaction Metrics,Practitioner,"This study investigates the connection between service quality and patient satisfaction in the healthcare sector, focusing on a public hospital. To effectively assess service quality, the research employs the SERVQUAL scale, a recognized tool for evaluating patient perceptions in healthcare. Collaboration with hospital administration was integral to creating reliable methods to capture patient expectations and experiences, offering valuable insights for hospital management. The study underscores the importance of high service quality in healthcare, noting its parallels with increased market share, return on investment, and cost efficiency seen in other industries. A primary focus of this research is understanding how service quality affects patient satisfaction, particularly in a public hospital setting. A structured 22-item questionnaire based on a five-point Likert scale was administered to 400 patients to gather comprehensive data on their experiences and satisfaction levels. Findings from this research provide essential insights that can help improve service quality, benefiting patient care and operational efficiency in the healthcare industry. These insights not only enhance understanding of patient satisfaction but also support healthcare providers in making informed management decisions for quality improvement.",19,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
859,6488.0,Gesture Recognition through Object Detection for Efficient Human-Robot Collaboration,Academician,"With the ever-changing and innovative manufacturing and industrial revolution, there has been a major focus on harnessing technology to improve productivity, preserve the environment, and tackle socio-economic issues. Manufacturing processes with Human Robot Collaboration (HRC) systems and gestural interaction are becoming more advanced for better safety, and usability is also becoming a critical point within these processes. This study proposes the integration of a high-level object detection model into the Digital Twin space. The object detection model determines and pinpoints the position of humans and various objects within the same realm, which is very important for the effective and safe interaction between the human and the computer. Additionally, for the purpose of improving the model's accuracy, a technique of transfer learning was adopted, which employs the vast obtainable datasets that came from multiple use cases. The development and application of this Digital Twin allow various conditions to be created for the simulation of the environment in which the robot will operate. With the introduction of 3D space modeling, the Digital Twin can provide simulation of various environmental factors during training. By integrating the Digital Twin with the physical robot system, this simulation framework makes it easy to conduct realistic and precise robot operation and gesture control simulations. The integration of these methods guarantee the efficacy and flexibility of the system, thus enhancing the use and application of Human Robot Collaboration (HRC) and gestural interaction.",20,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
860,6512.0,Streamlining Fermentation Operations: Reducing Distance while Enhancing Operator Task Execution​,Academician,"Introduction In the competitive beverage industry, optimizing production efficiency is crucial for maintaining quality and profitability, a leader in the spirits market, is focusing on its fermentation operations. This involves a thorough examination of the current layout and workflow surrounding 19 fermentation tanks, particularly the sampling and testing procedures. Observations have identified opportunities for improvement, particularly reducing unnecessary travel distances and enhancing operator efficiency within 18,000-square-foot facility. The anticipated outcomes include actionable recommendations for minimizing movement, consolidating tasks, and ultimately enhancing both productivity and fermentation quality, reflecting it’s commitment to continuous improvement and excellence in spirit production. Methods To achieve this, the Systematic Layout was employed to track operators' movements and time spent on tasks, revealing insights into workflow inefficiencies. Results and Discussion Current Layout Distance (ft) 32,504 Time (min) 282 Document Time (min) 48 Proposed Layout Distance (ft) 29,903 Time (min) 211 Document Time (min) 0 Savings Distance 8% Time 56% Conclusion The proposed solutions for streamlining the spirit’s manufacturer fermentation operations focus on optimizing facility layout and addressing workflow inefficiencies. Analysis revealed that 17% of the time spent on 24-hour operations was spent on non-value-added activities during sampling and testing, prompting significant improvements. The recommendations reduced travel by 224 feet (8%), enhanced operator efficiency, and replaced manual documentation with AI-powered digital solutions, eliminating 100% of the time spent on record-keeping. With a development cost of $59,920, including construction, equipment, testing, and training, these enhancements improve productivity, reduce costs, and support the manufacturer’s commitment to operational excellence.",21,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
861,6688.0,Deep Learning for Diabetic Retinopathy with Color Fundus Image Analysis in AI for Medical Imaging,Academician,"This work analyzes clinical photos of diabetic retinopathy (DR), a common and preventable cause of vision impairment, using deep-learning methods. We have analyzed the computational efficiency of several concatenated CNN models, including DenseNet201, ResNet50, VGG19, and MobileNet V2. ResNet50 has a training time of 15.37 seconds, but MobileNet V2 has 78.22% validation accuracy. This shows a trade-off between accuracy and efficiency. MobileNet V2 completes a 5-fold cross-validation process with 77.4% accuracy and predicts DR based on AUC. The IRI clustering-based technique can locate damaged retinal areas almost 100% reliably. These results provide several relevant insights for healthcare professionals and AI researchers in DR detection. The future research, known as OmniVec, offers a highly accurate multimodal deep-learning model for image processing. OmniVec sets new records on the ImageNet benchmark and outperforms existing bests in visual, audio, and text domains and 3D data. During pre-training, OmniVec enables existing modalities to jointly exchange information, which in turn boosts performance on a wide range of benchmarks, and therefore it may be a beneficial model for other multimodal works in the future.",22,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
862,6696.0,Scaling-up Analysis of Text-Data in Hospitals: An LLM Approach,Practitioner,"Hospital patient feedback reports offer critical insights for improving care quality and patient satisfaction, yet challenges in interpreting complex narratives and linking insights to outcomes limit their impact. This study utilizes Large Language Models (LLMs) to analyze a dataset of over 7,583 feedback reports from patients based on their emergency department (ED) visit at a large hospital system in southeastern United States. The analysis workflow included de-identification of the reports, followed by regression-based analysis to predict the likelihood-to-recommend score based on metadata, such as arrival-to-room time, patient acuity, and patient demographics. Additionally, an LLM was prompt-tuned to identify the presence (or absence) of specific themes in the free-text comments: trust in staff, expectations being met, feeling of safety, and cleanliness; and if the experience was positive. Results thus far show that arrival-to-room time, ED overcrowding and patient age are significant predictors of the likelihood-to-recommend score. LLM results showed 42.54% indicated trust in staff, 42.32% reported expectations being met, 42.63% noted a feeling of safety, 93.21% observed cleanliness, and 46.23% had a positive experience overall. The next steps in this ongoing study will explore further LLM-generated themes in the reports, and statistical relationships between the metadata and qualitative themes. By combining qualitative narratives with quantitative data, the study provides context for metrics based on the patient’s experience, enhancing the understanding of systemic factors affecting care. Findings suggest that LLMs can help scale analysis of patient feedback databases toward developing actionable insights to improve care quality.",23,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
863,6711.0,A hybrid multi-criteria decision-making and fuzzy stochastic chance-constrained optimization approach for determining the location of healthcare waste (HCW) disposal facilities,,"Effective management of healthcare waste (HCW) disposal is an ongoing challenge for healthcare providers and urban municipalities, particularly in densely populated areas. This study presents a novel framework for selecting optimal HCW disposal sites by integrating a fuzzy multi-criteria decision-making method with a fuzzy stochastic chance-constrained optimization approach. The framework provides a robust, data-driven methodology for determining the best disposal sites by balancing multiple criteria under uncertainty. To ensure a comprehensive and reliable selection process, 36 criteria were identified through a rigorous literature review and expert consultations. These criteria were then grouped into five primary categories: environmental, social, economic, technical, and geological. A case study was conducted in Chicago—a densely populated area with complex healthcare demands and varied environmental and logistical considerations - to demonstrate the applicability of this framework in an urban setting. Our results show that the proposed method outperforms traditional fuzzy AHP models in both robustness and simplicity, providing a more adaptable and efficient solution for HCW disposal site selection. This research contributes a practical and scalable tool for urban municipalities and healthcare institutions, supporting informed decision-making to improve public health and environmental outcomes.",24,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
864,6776.0,Heuristic Evaluation of the Electronic School Medication Administration Record (eSMAR) Application: Improving Medication Management in Schools,,"Approximately 27% of the 50.1 million K-12 school-age children in the US experience at least one chronic medical condition requiring them to receive medication during the school day. Findings indicate that 58.4% of school nurses reported missed doses as the most common medication error, followed by the wrong time (19.0%), and the wrong dose (18.3%). To reduce errors in medication administration in schools, we developed a technology-assisted system eSMAR to help school nurses safely and efficiently manage medication administration and documentation. To ensure the safety and usability of the eSMAR system within the school environment and identify design improvement opportunities, four evaluators used Nielsen's Usability Heuristics to assess usability problems in three medication administration tasks involving creating and modifying medication administration records and administering medications. The usability problems when performing the three tasks were organized and categorized by usability design principles and rated for severity (1 to 4). Task 1, “planning and creating student’s administration records” had the most usability problems. Example usability problems included ability to proceed to the next step in the medication administration flow and managing no-shows. This evaluation revealed key design opportunities including the option to modify a medication once entered and feedback regarding past due medications. These insights will be used to refine the design of the eSMAR application. Balancing safety and usability is a main consideration the design team will need to account for while addressing the identified usability issues.",25,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
865,6778.0,Network Reduction Based on Flow Performance,Academician,"Transportation networks in large cities, such as Bogotá, face significant challenges due to the dynamic nature of demand across various scenarios. Traditional approaches to network design typically focus on topological analyses, which may fail to capture the complex flow patterns that emerge under changing conditions. This research introduces a new approach to optimize urban transportation networks by integrating network flow theory with scenario based analysis. Initially, we solve the Minimum Cost Flow Problem (MCFP) for multiple demand scenarios within a fixed network. In a subsequent stage, we aim to redesign the network by maximizing the overall similarity to the flow solutions of each scenario. This is achieved by reconstructing the network using the original nodes and defining the edges and associated costs, as well as determining how much flow is sent through each edge to best match the MCFP for each scenario. By combining these methods, we seek to create a more adaptive and efficient network that responds to real-time fluctuations in demand, leading to improved service in densely populated urban areas",26,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
866,6790.0,Assessment and Classification based on Quality Degradation of Oranges using Machine Learning Model,Practitioner,"Quality assessment and classification of fruits on a large scale can be overwhelming, primarily when done by humans (manual means). This is because the outcome of humans doing repetitive work leads to cognitive adaption, mental fatigue, boredom, reduced productivity, and inconsistency over time. This research aims to use a machine learning model for the quality classification of orange fruits to ensure good product decision-making. This is a means of a sustainable practice to minimize fruit waste at grocery stores, such that fewer quality products can be used for other better purposes. Quality degradation (i.e., freshness) of oranges will be classified into three categories using a machine-learning model based on visual characteristics that define quality (color, bruise/cut on the surface, blemish, mold growth). This approach will reduce the waste of citrus fruits at retail stores where quality is determined based on individual perspective, classifying good quality products as lesser quality. The approach will ensure consistency, reduce human error, minimize time in decision-making, and minimize fruit waste to provide an enabling environment.",27,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
867,6952.0,Telemedicine Physician Staffing in Face of Reentrant Patients,Academician,"This manuscript considers the dynamic physician staffing problem for the telemedicine platform. Patients may abandon the queue when waiting for remote treatment, and those abandoning patients have the behavior of random retrial, e.g. patients with mild symptoms still need to seek treatment. On the other hand, patients who complete treatment services have random re-entry behavior, such as relapse ones who need to receive treatment again. Under the condition of satisfying patients, how to optimize physician allocation is a practical challenge faced by academia and telemedicine providers. However, few papers have been found on the operations management of telemedicine physician staffing with different reentrant patients. To fill the gap, we propose a fluid analytical approximation of dynamic physician staffing based on the time-varying queueing model, which can ensure the quality of service for patients by making full use of medical resources. The effectiveness of the proposed solution is validated by discrete event simulation in the numerical study. We also reveal some interesting insights, such as the lower and upper bound of staffing capacity, periodic steady state, and the nonstationary model's convergence rate.",28,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
868,8582.0,Development of a multivariate Gaussian distribution-based sarcopenia risk measure for patients with Parkinson’s disease,Academician,Sarcopenia is an age-related syndrome characterized by the progressive loss of muscle mass and strength. Early screening of the Parkinson’s disease (PD) patients likely to have latent sarcopenia risk is significant for the timely treatment and rehabilitation of PD patients. This study proposes a quantitative sarcopenia risk measure for PD patients to quantify their sarcopenia risk as a percentage value. The appendicular skeletal muscle index (ASMI) and handgrip strength of 108 PD patients were collected to model multivariate gaussian distribution-based sarcopenia distributions for female ( n = 45) and male ( n = 63) PD groups. The latent sarcopenia risk of each PD patient is measured by the ratio of two cumulative probability components in each sarcopenia distribution: 1) the cumulative probability that both the ASMI and handgrip strength in the sarcopenia distribution are greater than those of the particular PD patient and 2) the cumulative probability that both the ASMI and handgrip strength in the sarcopenia distribution are greater than those of the sarcopenia diagnosis thresholds. This study performed the regression analysis of the proposed sarcopenia risk measure on ASMI and handgrip strength to validate the proposed measure. The results showed that the proposed sarcopenia risk measure appropriately reflects the original sarcopenia evaluation criteria with statistical significance in the regression analysis. The proposed sarcopenia risk measure for PD patients would be usefully employed for various analyses to classify and predict the PD patients who are highly susceptible to sarcopenia.,29,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
869,8588.0,Effectiveness of a zonal product architecture for software-defined product design: A case study of robot vacuum cleaner,Academician,"Software-defined products (SDPs) enhance original functions and services through digitalized and automated software components that are decoupled from physical product components. In this regard, zonal architectures are receiving attention as new product architectures for SDPs to reorganize distributed electronic controllers into core modularized zones. This study compares a conventional product architecture and its transformation to a zonal architecture for a robot vacuum cleaner to identify potential benefits. First, structural and functional relationships in the current architecture of a robot vacuum cleaner are characterized using design structure matrices (DSMs). Based on DSMs, a zonal architecture is then constructed by clustering the structural and functional components of the original product architecture. Next, both the original and zonal architectures are represented as networks and evaluated using major network measures. They are further simulated by varying the number of wireless control functions and physical functions. The results show that the product architecture complexity observed from network dependency and density is less imposed in the zonal architecture than in the original product architecture as the number of additional control and product functions increases. The proposed approach demonstrates the importance of zonal product architectures for SDPs.",30,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
870,8735.0,Optimization Processes for Improving Clinician Schedules,,"Building work schedules for healthcare providers manually is a time-consuming process that puts strain on hospital staff. Scheduling residents and fellows is further complicated by numerous rules that must be followed, including detailed educational requirements and hospital coverage demands. Additionally, effective schedules should minimize issues like denied vacation requests and disparities in training opportunities. To address these challenges, the Provider Scheduling team of researchers at the Center for Healthcare Engineering and Patient Safety (CHEPS) have been developing mathematical models and software tools to assist departments and training programs at Michigan Medicine and other health centers in meeting their specific scheduling needs. As a recent example, the Pediatric Critical Care Fellowship program at Michigan Medicine sought a tool to streamline and optimize the scheduling process. The CHEPS team adapted an existing mixed integer programming tool, originally created to solve residency block scheduling problems, to meet the specific needs of the fellowship. The team collaborated closely with program leadership, particularly the Associate Program Director, to document requirements, develop new features focused on promoting educational equity, and implement these into a C++ program utilizing IBM’s CPLEX library. After thorough testing, the tool was demonstrated to collaborators, showing that it successfully minimized missed educational opportunities, thereby promoting a more equitable training experience for fellows. This project illustrates how standardized processes can be effectively applied to develop scheduling tools that enhance both efficiency and fairness in clinical education.",31,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
871,8754.0,Exoskeleton Evaluation for Emergency Responders,,"Emergency Medical Technicians (EMTs) are vulnerable to low back pain and injuries as they often have to perform forceful exertions and heavy lifting tasks (e.g. lifting patients, carrying patients, chest compressions). Passive low-back exoskeletons (LBEs), which reduce strain on the spine for lifting tasks, could reduce the risk of musculoskeletal disorders in EMTs. However, current commercially available exoskeletons have not been thoroughly tested for dynamic job demands that are typical to EMTs. Therefore, in order to test the effectiveness of LBEs for EMS related work, the proposed study measured the effect of an LBE on the muscle activity of lower back muscles while performing standard tasks typically performed by the EMS. In this study, 5 tasks were selected from commonly performed physical agility tests used to assess fitness of EMTs to perform on the field. Participants will perform the following tasks in exoskeleton and control conditions (i.e. while wearing and not wearing the exoskeleton): stair climbing with equipment, stair climbing with patients, chest compressions, stretcher lift, and patient rescue. Surface electromyography (EMG) of the participants’ erector spinae muscles will be collected. Root mean square EMG (RMS EMG) will be calculated from this data, and further used to calculate amplitude probability distribution function (APDF) which will be used to calculate static, average and maximum muscle activity which will be compared between the exoskeleton and control conditions. Preliminary results from our pilot study indicate a reduction in muscle activation for stair climbing and CPR tasks when wearing the exoskeleton.",32,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
872,8900.0,Impact of Urbanization on Land Ownership Patterns in Fringe Communities of Asaba Delta State.,Practitioner,"Asaba the capital of Delta State Nigeria, is remarkable for its quiet and relaxed atmosphere. As a result, the city is very much sought after by intending home owners and private investors. When Asaba became the capital of the newly created Delta State in 1991, it was expected that new job opportunities would spring up for its residents and immigrants. Also, it was expected that Asaba and its fringe communities would provide a suitable environment for private investors. Particularly, the process of land acquisition was supposed to be made simpler since land in the area became subject to the jurisdiction of the state government, under the Land Use Act 1978 as amended in 1990. However, the reality of the situation in Asaba is that the land acquisition process is unorganized and plagued with contention for the same land interests. This trend has raised a disturbing concern for intending home owners and investors who are hard pressed to trust the process of land acquisition for their well-intentioned residential and investment needs. Using data obtained through questionnaire survey, key informant interviews and focus group discussions, this study will unravel the factors that mitigate against sustainable land ownership and land acquisition in Asaba and its fringe communities. Recommendations from the study will guide real estate developers, industrialists and home owners to ensure that they have uncontested rights to land acquired for development purposes.",33,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
873,8960.0,"""Virtual Reality in Engineering Education: A Synthesis of Applications, Outcomes, and Opportunities""",Academician,"As the field of Virtual Reality (VR) continues to advance rapidly, it is evident that these technologies can facilitate transformative avenues in engineering education by enabling immersive, interactive, and experiential learning. This literature review investigates VR in engineering education across disciplines exploring pedagogical practices and outcomes. It synthesizes the results of studies on VR use to characterize how it is applied to improve the teaching and learning process in engineering education. The analysis shows how VR has been used to simulate complicated systems, visualize abstract engineering ideas, and intuitive experience in immersive, controlled surroundings. These applications show great promise in meeting educational needs such as limited access to physical resources for engaging, hands-on learning experiences. This literature review further discusses the effects of VR on student engagement, learning, and skill acquisition, considering key theoretical frameworks to contextualize these outcomes. Challenges in integrating VR into curricula, including scalability, cost, accessibility, and interdisciplinary collaboration are highlighted. Best practices and strategies for effective implementation are identified, emphasizing the importance of aligning VR tools with specific learning objectives and measurable outcomes. The identified gaps in knowledge are geared towards helping educators, curriculum designers, and researchers to make better pedagogical and technical decisions involving VR while considering the implications of these technologies in preparing engineering students for the demands of a rapidly evolving technological landscape.",34,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
874,9088.0,Self-modulation of Heart Rate Variability with Haptic Biofeedback to Support Recovery from Traumatic Brain Injury,,"Dysregulation of Autonomic Nervous System (ANS) in Traumatic Brain Injury (TBI) patients can not only impact their Emotional Regulation Skills (ERS) and social interaction but also increase the risk of cardiovascular diseases in these individuals. This ongoing study aims to examine the effects of a 12-week haptic biofeedback program on ANS function in TBI patients, with a focus on its potential to enhance emotional regulation and overall well-being. ANS function will be measured in terms of heart rate variability (HRV), which will be monitored using the Lief “smart patch”. The pre and post program MRI scans of the participants will also be recorded to study the associated changes in the DTI-MRI metrics of their brains. In addition, various surveys will be completed by the participants at different stages throughout the program such as BSI-18, Key Behaviors Change Inventory (KBCI), GAD-2 for anxiety, PHQ-2 for depression, and Telehealth Usability Questionnaire (TUQ). Statistical relationships between such subjective measures and HRV and MRI metrics will be analyzed, including whether increase in HRV predicts improvement in self-reported mental health, behavioral changes, quality of life, and how this relationship may be influenced by system usability. The participant will also be interviewed in order to identify barriers and facilitators of effective and sustained engagement with the program. Thematic analysis of interview and survey data, along with findings from the TUQ, will be used to identify qualitative patterns, that can inform design improvements to the HRV biofeedback program.",35,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
875,6196.0,Aggregation of Bilinear Bipartite Equality Constraints and its Application to Structural Model Updating Problem,,"In this paper, we study the strength of convex relaxations obtained by convexification of aggregation of constraints for a set S described by two bilinear bipartite equalities. Aggregation is the process of rescaling the original constraints by scalar weights and adding the scaled constraints together. It is natural to study the aggregation technique as it yields a single bilinear bipartite equality whose convex hull is already understood from previous literature. On the theoretical side, we present sufficient conditions when conv(S) can be described by the intersection of convex hulls of a finite number of aggregations, examples when conv(S) can only be obtained as the intersection of the convex hull of an infinite number of aggregations, and examples when conv(S) cannot be achieved exactly from the process of aggregation. Computationally, we explore different methods to derive aggregation weights in order to obtain tight convex relaxations. We show that even if an exact convex hull may not be achieved using aggregations, including the convex hull of an aggregation often significantly tightens the outer approximation of conv(S). Finally, we apply the aggregation method to obtain convex relaxation for the structural model updating problem and show that this yields better bounds within a branch-and-bound tree as compared to not using aggregations.",36,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
877,8565.0,Analyzing the Landscape of Organ Donation and Transplantation in the US: A Data-Driven Exploration of Contributing Factors,Academician,"Organ transplantation, particularly kidney transplants, is a critical process impacted by factors of medical utility and justice. In 2023, 27,332 kidney transplants were performed, yet nearly 90,000 candidates remained on the waitlist, reflecting the need for equitable and efficient organ allocation systems. This project aims to develop a machine learning-based predictive model to estimate kidney waitlist times and match probabilities using the National STAR dataset. The model will incorporate patient and transplant factors, such as age, blood type, geographic region, GFR, and EPTS score, to provide personalized predictions for candidates. By utilizing regression and classification techniques, the tool will not only predict wait times but also highlight which factors most influence a candidate’s likelihood of being matched. This insight can guide candidates to take proactive actions, such as enrolling in multi-listing programs in favorable regions. Additionally, analysis of what factors are most important across all candidates can help shape organ allocation policy. These findings, along with trends and patterns found using descriptive analytics, will be presented in a transparent way to inform policy makers. The final model will be deployed via a user-friendly web application, making it accessible to both patients and healthcare professionals.",38,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
878,6548.0,"Powering Progress, Protecting Nature: Harnessing Ocean Currents for Sustainable Energy",Practitioner,"This research investigates the potential of ocean resources for energy generation in the Caribbean Sea, with a focus on Puerto Rico’s Gilligan Island located in the south of the island. As part of an effort to explore sustainable alternatives for renewable energy, this study evaluates the ocean currents of the region as a viable source of energy while ensuring minimal environmental impact on marine life. The research synthesizes ongoing studies conducted by various institutions and aims to identify the most effective system for harnessing energy from the ocean currents to benefit the southern communities of Puerto Rico. The study involves a detailed analysis of the forces generated by ocean currents in the Caribbean Sea. Through this analysis, the researcher will calculate the potential energy that can be generated, taking into account fluctuations in current strength over time. A key component of the research will be the development of a comprehensive table that outlines the variation in current forces, identifying optimal periods for energy generation. Additionally, the research will explore alternative technologies and methods that can minimize the environmental impact on marine ecosystems, addressing concerns related to marine life disruption. By aligning this research with existing studies on ocean energy, the project aims to propose a sustainable solution for utilizing renewable energy sources from the ocean. The findings will provide valuable insights into the feasibility of ocean current-based energy systems, contributing to the broader conversation on clean energy alternatives in the Caribbean region.",39,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
879,8860.0,Accelerating Nuclear Fuel Qualification with AI: Predicting Irradiation Test Outcomes Using Machine Learning,Academician,"Reducing the qualification time for novel nuclear fuels is critical for advancing nuclear energy efficiency and cost-effectiveness. Over the past months, we have enhanced our AI-driven model to predict fuel behavior during irradiation tests, critical and time sensitive process in the qualification of novel nuclear fuels. By collaborating with the Oaks Ridge National Laboratory (ORNL) team, we obtained the data and refined our model with key variables derived from nuclear fuel behavior, focusing on heat generation rate (HGR) and burnup, two metrics vital for assessing reactor performance and fuel efficiency respectively. Our process involved comprehensive data cleaning, feature extraction, and up-sampling techniques to standardize input datasets collected under varying conditions. We trained several machine learning models, including RNN LSTM, TCN, and Random Forest, to predict HGR and burnup, achieving mean errors as low as 4% with TCN for HGR and 2% with LSTM for burnup. These models outperform traditional numerical methods, currently used to predict this behavior, which require significant computational resources, highlighting the efficiency and scalability of our approach. Despite these advancements, data scarcity remains a key challenge. To address this, we are developing a design of experiments to generate synthetic data, expediting the training process and reducing reliance on prolonged irradiation tests. By eliminating prototypes unlikely to meet test criteria early in the process, our methodology offers substantial savings in time and resources.",40,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
880,7033.0,Data-Driven Interpretable Policy Design for Electric Vehicle Charging: A Causal Learning Approach,Academician,"Adoption of Electric Vehicles (EV) has led to the demand for efficient charging solutions, especially in residential sector where most of the charging happens at home. Designing charging policies in these settings requires leveraging existing data on consumers' behavior while addressing challenges related to incomplete datasets. This work introduces data-driven approach to EV charging policy design, using existing charging data that captures consumers' preferences for charging times, durations, frequencies, and responsiveness to cost fluctuations to reduce charging costs. To tackle challenges of incomplete data, such as unseen actions in offline learning tasks, we employ Doubly Robust Estimation $\--$ causal inference technique to predict counterfactuals. These predictions fill gaps in state-action information, enabling development of robust, behaviorally informed policy. Through counterfactuals, our method projects how users might respond to various charging strategies not present in the static data. Given sensitivity of EV charging, impacting both consumer convenience and grid reliability, we prioritize interpretability through prescriptive policy trees. Policy trees provide a structured, rule-based decision framework that offers clear, hierarchical structure of decision pathways, unlike black-box methods like neural networks. This hierarchical structure facilitates systematic comparisons of charging decisions, helping optimize policies for diverse user behaviors. The proposed framework simultaneously predicts counterfactuals and trains an optimal policy tree using a greedy method. The method is designed to adapt as more data becomes available, improving accuracy and scalability over time. By bridging the gap between static data limitations and dynamic demands of residential EV charging, our approach offers an interpretable, and behaviorally informed solution.",41,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
881,6540.0,From Classroom to Production Line: Turning Innovative Ideas into Profitable Realities,Practitioner,"This project challenged students to address a practical problem by developing an innovative solution and designing a production line for its manufacture. The students conceptualized a material handling cart to transport heavy spools ergonomically, minimizing physical strain and potential hazards. To bring this idea to fruition, they engaged in a comprehensive development process, which included conducting a client survey to assess design preferences and performing market research to estimate demand. They created four projection models, selecting the most accurate for forecasting purposes. Based on these projections, the team planned material requirements and mapped out a detailed process flow diagram for the manufacturing line. The production plan involved determining equipment capacity, scheduling necessary resources, and calculating both direct and indirect labor costs, as well as overhead expenses. The students also formulated a distribution strategy tailored to the local market. Financial projections indicated an annual manufacturing cost of $4,030,248 and estimated sales revenue of $4,453,104, yielding a profit of $422,826 per year. With an initial investment of $28,768, the project offers a rapid payback period of just 0.07 years, validating the feasibility and economic viability of the proposal. This real-life application provided students with practical insights into finance, cost accounting, and production planning, bridging academic theory and industry practices.",42,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
882,6728.0,Enhancing Fleet Management Efficiency and Compliance: An Integrated System for Nonprofit Organizations,Academician,"This study develops an integrated fleet management system to streamline operations for Transitions Mental Health Association (TMHA), a nonprofit providing mental health services across San Luis Obispo and Santa Barbara counties. TMHA’s 62-vehicle fleet is currently managed using disconnected systems, including Excel spreadsheets, Outlook calendars, and paper-based records, which lead to administrative burdens and compliance risks. To address these issues, we designed a centralized, automated solution for vehicle tracking, maintenance scheduling, and reservation management. Key features include automated safety notifications, simplified collision reporting, and a unified reservation interface to reduce reliance on individual knowledge and improve organizational resilience. Effectiveness is measured through key performance indicators such as safety checklist completion rates, incident reporting accuracy, and reductions in administrative time, with initial findings indicating enhanced operational efficiency, reduced compliance risks, and a notable decrease in manual workload. This research contributes to fleet management literature by demonstrating a scalable model that can support similar nonprofit and small organizational contexts in achieving efficient, automated fleet operations that improve safety and resource management.",43,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
883,6851.0,Optimizing Emergency Room Patient Flow: A Simulaiton-Based Approach to Reducing Wait Times,Practitioner,"The COVID-19 pandemic caused abrupt changes in emergency care, highlighting the need for rapid response and efficient resource use. This study, conducted in the emergency department of a private hospital with over 112,000 arrivals in 2023, noted a post-pandemic increase in ER visits surpassing pre-pandemic levels. Employing the DMAIC (Define, Measure, Analyze, Improve, Control) methodology and the Critical Incident Approach, we identified key issues in patient experience. Due to the non-linear nature of patient flow, we developed an agent-based simulation, contrasting with the traditional discrete simulations commonly used in ER settings. Our simulation treated patients as semi-dependent entities and more accurately modeled the movements and actions of medical staff. The simulation predicted a 35% reduction in waiting times by implementing a low-complexity patient flow, which was then put into practice. Timeliness showed the largest gap between importance and satisfaction. Actual implementation exceeded predictions, achieving a 50% reduction in waiting times and a 33% reduction in total patient time. We focused on objective metrics over patient satisfaction surveys. This study demonstrates the value of agent-based simulations in modeling complex, variable processes like emergency rooms, despite potential discrepancies between models and reality. By accurately simulating patient and staff interactions, we could implement a differentiated flow for low-complexity patients—nearly half of ER visits—significantly improving efficiency. We plan to share this methodology with public hospitals and promote the Critical Incident Approach to enhance patient satisfaction. Future work will refine simulations to better mirror real conditions, suggesting potential applications in other healthcare settings.",44,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
884,6434.0,Sealing Success: Enhancing Efficiency and Sustainability in Medical Device Production,Practitioner,"This report presents an in-depth analysis and improvement plan for optimizing the medical device sealing process, addressing several United Nations Sustainable Development Goals (SDGs). The primary focus is on enhancing process efficiency and quality, with secondary benefits for worker safety and environmental sustainability. Key SDG alignments include: SDG 3 (Good Health and Well-being) , where process improvements contribute to safer work environments; SDG 8 (Decent Work and Economic Growth) , emphasizing increased productivity and cost reduction through optimized processes; SDG 9 (Industry, Innovation, and Infrastructure) , promoting sustainable industrial practices through innovative process adjustments; and SDG 12 (Responsible Consumption and Production) , which aims to minimize resource waste and environmental impact by improving production efficiency. Statistical analysis was conducted to refine time metrics and identify outliers, employing tools such as the Dixon test and Westinghouse method for precise adjustments. The cycle time post-adjustment was 43.48 seconds with a standard deviation of 3.01 seconds, providing a robust basis for accuracy in process evaluation. Element times were established as follows: Element 1 at 9.61 sec, Element 2 at 5.19 sec, Element 3 at 25.51 sec, and Element 4 at 19.56 sec. Recommendations include enhancing workstation flow, standardizing procedures to reduce errors, providing ergonomic training, and optimizing sample size to 28 readings for precision. Adjusted standard times offer a solid foundation for planning and resource management, as seen in Element 3’s reduced time of 1.25 seconds, highlighting efficiency gains. This study’s findings support a model for safer, sustainable production practices in industrial settings.",45,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
885,5166.0,Data-Driven Optimization of E-Commerce Inventory and Logistics Operations for a Small Board Game Retailer,Practitioner,"Small businesses in the premium board game e-commerce market relying on platforms like Amazon face unique challenges in maintaining profitability, particularly due to excessive storage fees and fluctuating sales during critical periods, such as the holiday season. At the bridge of supply chain and marketing concepts, the interconnectivity between inventory optimization and sales strategies is explored. This project aims to streamline inventory management and enhance sales performance to reduce operational costs while boosting brand visibility for “All You Need is Wine,” a board game that gamifies blind wine taste testing. To address these issues, historical sales data was analyzed, and various inventory management tools were used to identify trends, ultimately developing an optimized inventory management plan. Key analyses include tracking the inventory turnover ratio to balance stock levels and a break-even analysis to align sales goals with storage and advertising costs. High-level marketing strategies, such as exploring new sales channels and targeted advertising, will support efforts to increase brand awareness and drive top-line growth. The proposed improvements target both the bottom line by reducing storage costs and the top line through expanded sales efforts.",46,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
886,6439.0,VMR Clip: A Color-Coded Solution for Reducing Food Waste and Promoting Sustainability,Practitioner,"This study evaluates the demand, cost structure, material planning, and commercialization of the innovative ""VMR Clip,"" designed to reduce food waste through a color-coded system indicating the freshness of stored food. The clip uses green, yellow, and red colors to help consumers easily identify food quality, promoting sustainability, improving food safety, and fostering household savings. The methodology included a survey of 51 participants from diverse demographics to gauge interest and determine purchasing preferences. The survey revealed that 94.1% of respondents were interested in buying the ""VMR Clip,"" with the highest interest coming from young women aged 18-24, who showed a strong focus on food safety and sustainability. The survey also identified seasonal demand fluctuations, with May exhibiting the highest purchase interest due to the availability of fresh food. Material requirements, including plastic, wire, batteries, lights, buttons, and oxygen sensors, were outlined to ensure product functionality. Economic Order Quantity (EOQ), Safety Stock, and Reorder Point calculations were performed to optimize material purchasing and reduce operational costs. The estimated total annual budget for production was $368,096.40, with peak investments in May ($79,493.40) and June ($50,425.20), aligning with the identified demand. In conclusion, the ""VMR Clip"" demonstrates significant market potential, particularly among environmentally conscious young adults. The analysis shows that the device can be efficiently produced and marketed, supporting the United Nations' Sustainable Development Goal 12 on responsible consumption and production, while contributing to food waste reduction and sustainability efforts.",47,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
887,6927.0,An Economic Equilibrium Model for Reverse Logistics System for  Electric Vehicle Batteries,Academician,"Abstract: The rapid expansion of the electric vehicle (EV) market, along with COVID-19-related supply chain disruptions and environmental concerns from mining, has raised critical questions about the sustainability of raw material supplies for EV batteries (EVBs). As a potential solution, end-of-life (EOL) EVBs offer significant economic and environmental benefits, as they still retain 70-80% of their initial capacity and can be reused towards making new EVBs through re-manufacturing or recycling. To explore the state-of-the-art research on circular economy (CE) and reverse logistics (RL) for EVBs, we reviewed 89 journal publications from January 2012 to May 2023, identifying key research gaps. Addressing these gaps, we develop an economic equilibrium model in RL system for EVBs, investigating the interrelationships among stakeholders, such as EOL EVB collectors, third-party logistics (3PL) providers, recyclers, and EVB manufacturers. First, we formulate an optimization model for each partner, integrating these models through market-clearing conditions. Second, the equilibrium model is constructed as a mixed complementarity problem (MCP) using Karush-Kuhn-Tucker (KKT) conditions and Lagrangian equations. Numerical examples will be presented. Future research directions include spatial analysis and consideration of various EVB types and the recycling technologies (e.g., hydrometallurgy, pyrometallurgy).",48,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
888,6391.0,"Leveraging Digital Twins to Foster Human-Centered, Resilient Supply Chains",Academician,"A digital twin is a virtual replica of physical assets, processes, or systems that integrates real-time data and advanced simulation techniques. Aligned with Industry 5.0, digital twins in supply chains emphasize collaboration between human expertise and digital models, ensuring that human-centered values such as flexibility, resilience, and sustainability are embedded into operations. This approach allows digital twins to incorporate feedback from human decision-makers, enhancing their ability to adapt to evolving demands and disruptions while fostering a more socially responsible and efficient supply chain. The goal of this research is to enhance decision-making, improve operational efficiency, and increase resilience within supply chains by creating dynamic, real-time models of various supply chain components through digital twins. Additionally, the research aims to display an example use case, including a platform where different supply chain stakeholders can collaborate, share data, and test scenarios in a virtual environment. This platform will facilitate proactive planning by simulating various disruptions, such as supplier delays or market fluctuations, to assess their potential impact and develop mitigation strategies. Ultimately, the research seeks to transform traditional supply chain management by offering a more agile, efficient, and resilient system equipped to handle the complexities of global markets and the increasing demand for transparency and sustainability.",49,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
889,6737.0,Leveraging Digital Twins to Assess the Benefits and Challenges of Manufacturing Nearshoring,Academician,"The resurgence of manufacturing nearshoring in the United States has significant implications for supply chain resilience and sustainability. This research aims to develop a comprehensive framework using supply chain digital twins to assess the impact of manufacturing nearshoring on supply chain dynamics, performance, and sustainability. The research creates a virtual representation of the supply chain to simulate, analyze, and optimize nearshoring scenarios. The economic, environmental, and operational benefits and challenges of nearshoring are explored, providing valuable insights for policymakers, industry leaders, and supply chain professionals. Findings indicate that offshoring generates over a 10,000% increase in Carbon emissions from cargo ships alone and leads to a 30% rise in total costs compared to nearshoring. These results emphasize the potential of supply chain digital twins as a decision-making tool, enabling businesses and policymakers to holistically evaluate the feasibility of nearshoring to strengthen supply chain resilience and sustainability in the era of Industry 5.0. The findings corroborate that supply chain digital twins can be used by businesses and decision-makers to holistically evaluate the feasibility of nearshoring parts or the entirety of a supply chain near the United States.",50,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
890,6235.0,Development of a text-to-image reconstruction framework for wheel designs,Academician,"Recent advancements in text-to-image (T2I) models have facilitated generating novel engineering designs from text prompts in the early design stage. However, T2I models face challenges in maintaining the appropriate symmetry and periodic patterns that are essential for rotary-shaped parts such as bearings, gears, and wheels. To address these challenges, this study proposes a design reconstruction framework to ensure the structural balance and stability of the wheel designs generated by T2I models. First, magnitude spectrums for initial wheel images generated by T2I models were derived using the fast Fourier transform to identify the periodicity and patterns of spokes in the images. Then, a conditional diffusion model was trained using the greyscale wheel images and their magnitude spectrums to regenerate wheel designs that are similar to the geometrical characteristics of the initial wheel designs. In addition, a classifier guidance method was employed during diffusion model training to reflect feedback information regarding how closely the images generated by the diffusion model satisfy the desired periodic and symmetric characteristics. The results showed that the proposed framework enhanced the feasibility of the generated wheel designs by effectively adjusting asymmetric features and non-periodic elements. The findings of this study demonstrate that the proposed framework can effectively improve the feasibility of generative designs to aid a conceptual design process via T2I-based generative design tools.",51,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
891,6476.0,Characterizing the simulation of  system behaviors for requirement verification using SysML,Practitioner,"The development of executable SysML models is typically completed without understanding which SysML diagrams, elements, and ancillary aspects such as verification languages, etc. are needed to simulate system behaviors. This produces static models that need to be reworked to support requirement verification or other systems engineering tasking. Characterizing system behaviors in terms of executable methods organizes potential SysML simulation categories and relates the executable models to systems engineering tasking which will increase the complexity of modeling efforts and realize the simulation capabilities for verifying requirements in SysML. The methods that will be developed include the use of Association Blocks to complete informational exchange without behavioral elements and diagrams, Signals and Activities for one-way informational transfer, as well as specialized methods for nested points of interface and self-reinforcing or multi-layered structures, Messages and Interactions for varied informational exchange, State Machines for state-based behaviors, and combinations of methods for simple, medium, and complex system dynamics. Examples of the applications of these methods include a cable system verifying electrical current, a computing system sending data from a processor to a reader, as well as variations for nested ports and multi-layered readers, a computing system sending and modifying data to containers via a state’s entering, ongoing, and exiting activities, a computing system sending and modifying data to containers by transitioning between states, and a user withdrawing money from an ATM. Each method demonstrates verification of requirements with differing levels of modeling complexity.",52,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
892,5663.0,"Value-based pricing of Tislelizumab in combination with chemotherapy as first-line treatment for esophageal squamous cell carcinoma, assessed through economic modeling.",Practitioner,"This study aims to determine the price range at which Tislelizumab, combined with chemotherapy, would be considered cost-effective compared to chemotherapy alone for treating advanced esophageal squamous cell carcinoma (ESCC) in the U.S. healthcare system. A three-state Markov model was constructed to evaluate these treatment regimens over a lifetime horizon using survival data from the RATIONALE-306 trial, with cost and utility estimates drawn from public sources and literature. The analysis found that adding Tislelizumab provided an incremental gain of 0.545 QALYs over chemotherapy alone. The value-based price for Tislelizumab was estimated to be $1,575 per cycle at a willingness-to-pay (WTP) threshold of $50,000 per QALY gained, and $7,130 per cycle at a WTP threshold of $200,000 per QALY gained. Sensitivity analyses showed a high probability of cost-effectiveness at these price points, confirming the robustness of the findings across various model parameters, including survival hazard ratios and adherence levels. These results establish a value-based price range for Tislelizumab from $1,575 to $7,130 per treatment cycle, offering critical insights for healthcare policy and drug coverage decisions amid the rising costs of immunotherapy.",53,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
893,6189.0,Optimizing Product Development and Supply Chain for a Beer Tasting Board Game,Practitioner,"The global board games market, valued at over $11 billion as of 2022, is growing and is currently seeing a rise in demand for interactive social games. ""All You Need is Beer"" expands the ""All You Need is Wine"" brand by introducing a beer-tasting board game that invites players to blind-taste and identify beer styles by exploring the five S’s of beer tasting: Sight, Swirl, Smell, Sip, and Savor. To ensure market viability, this work leverages lean product design principles and an optimized supply chain strategy to reduce production costs, focusing on material choice, packaging revisions, and efficient manufacturing. We propose a comprehensive business plan that includes a market analysis, financial projections, and a supply chain strategy focusing on e-commerce distribution. The game design incorporates essential components like tasting sheets and note sheets in a streamlined format to simplify manufacturing and logistics as well. Currently, in the design and planning stages, the game is projected to launch with a flexible production process and a market strategy that includes online sales channels such as Amazon and more.",54,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
894,6093.0,Exploitation of Effective Emission Trading to Aim Carbon Neutrality,Academician,"“A livable climate, carbon neutrality commitments must be backed by credible action” which is delivered by the UN as an emergency goal. To pursuit the carbon neutrality, the emission trading (ET) problem, which concerns a matching solution between a polluter and a green entity with multiple objectives is exploited. Particularly, the solution approach should provide a traceable reputation score to market users with, ESG (Environmental, Social and Governance) data. In such way, participants and regulators can trustfully ensure that carbon reduction goals can be met. The ET problem does not only concern a matching solution between buyers and sellers, but also with multiple objectives, such as buyer’s low cost, seller’s high profit, and so on. In this study, an effective, open, and fair ET to match the buyers and sellers is proposed. To solve this ET problem, the Evolutionary Multi-objective Optimization (EMO) Based ET (EMOBET) approach is proposed to uncover candidate solutions based on the simultaneous consideration of multiple, potentially conflicting objectives. The deliverables of this study are composed of the matching solution of buyers and sellers and the results of multiple objectives, including minimizing cost of buyers, maximizing profit of sellers, maximizing the total buyers’ reputation from the perspectives of SD, maximizing the number of matching. This research results are expected to contribute: (1) Reference for Government carbon neutrality strategy, (2) Networking for Business carbon trading, and (3) Enhancement for Organization sustainability.",55,Habersham Ballroom | Renaissance Waverly Hotel,Poster Session,248
895,5555.0,Evaluating Attention Allocation in Human-Robot Collaboration in Construction Tasks,Academician,"The integration of robots in construction has the potential to enhance work efficiency and safety. As the level of interaction between workers and robots keeps increasing, it becomes crucial to evaluate whether human-robot collaboration (HRC) introduces additional cognitive load, leading to worker distraction during construction tasks. While existing research largely addresses human factors at lower interaction levels (e.g., cooperation, coexistence), it remains unclear how robotic collaboration impacts workers’ attention, possibly causing distractions and decreased safety performance or work efficiency. This study investigated worker’s attention allocation by comparing solo and collaborative tasks in an immersive virtual reality (VR) construction environment. Eye-tracking data were analyzed across the two task modes to assess changes in workers’ attention based on the areas of interest (AOIs), allowing identification of distraction events. Preliminary findings suggested that HRC affected the attention allocation of workers at different task stages, potentially impacting productivity and overall task performance in construction environments. This study contributes to evaluating the feasibility and safety of HRC in construction, facilitating its effective adoption in the construction industry.",1,Waverly | Renaissance Waverly Hotel,Construction Engineering & Management: Addressing Risk Factors with Humans in Construction,249
896,5262.0,Supply Chain Challenges in the Construction of Resilient Manufactured Homes,Academician,"The US is facing a critical shortage of affordable housing, with an estimated need for over 1.5 million additional homes within the next three years, particularly in disaster-prone areas. Increasing housing supply through manufactured homes, built to the US Department of Housing and Urban Development's Manufactured Homes Construction and Safety Standards (MHCSS), also known as HUD Code, in controlled factory settings, presents a viable solution. This study explores the integration of advanced resilient features, specifically FORTIFIED® standards, into these homes to address the needs of residents in areas subject to extreme weather events. This paper presents the development of evaluation procedures for analyzing supply chain needs in this type of manufacturing. We investigate supply chain challenges by conducting semi-structured interviews with key stakeholders, including home builders, material suppliers, installers, regulatory bodies, and insurance representatives. Industrial engineering techniques such as affinity diagrams are used to identify and analyze supply chain issues, including material sourcing, training gaps, and regulatory hurdles. An advisory board of industry experts and academic researchers provides guidance and validation, ensuring the study's practical relevance. The paper will highlight the methodological approach and framework established for assessing supply chain challenges in manufacturing disaster-resilient homes.",2,Waverly | Renaissance Waverly Hotel,Construction Engineering & Management: Addressing Risk Factors with Humans in Construction,249
897,9096.0,Title: Exploring the Impact of Construction Sign Design on Memory Recall and Decision Confidence: A Cognitive Approach,Academician,"This research investigates the effectiveness of construction signs by examining participants' memory recall and decision-making processes. The primary objective of this study is to understand how individuals perceive and remember construction signs, which are crucial for ensuring safety and compliance in construction sites. To achieve this goal, the relationship between participants' confidence levels in recognizing construction signs and the time it takes for them to express this confidence provides insights into the cognitive processes involved in sign recognition and decision-making. 30 construction workers were recruited to participate in the experiments while looking at different signs while their physiological responses were recorded using an EEG and a mobile eye tracker. The research team also collected the confidence level in identifying the correct construction sign from a set of alternatives and the duration required to reach a level of confidence in their selection. The collected data was analyzed using repeated measure ANOVA to determine the effectiveness of construction sign designs, potentially revealing how different elements such as color, shape, and symbol clarity impact memory recall and confidence in sign recognition. This research contributes to the knowledge of visual communication and safety signage, aiming to improve public safety and compliance through evidence-based design strategies. By understanding the cognitive mechanisms that underlie sign recognition, designers and policymakers can develop more effective signage that enhances safety and minimizes confusion in construction areas.",3,Waverly | Renaissance Waverly Hotel,Construction Engineering & Management: Addressing Risk Factors with Humans in Construction,249
898,6979.0,Exploring Women’s Acceptance of Using Exoskeletons in Labor-Intensive Construction Tasks,Academician,"The construction industry is among the most physically demanding sectors, frequently subjecting workers to fatigue and musculoskeletal strain. Exoskeletons, wearable assistive devices designed to augment physical capabilities, have shown significant potential for alleviating the physical demands of such tasks. Widely adopted in industrial settings, exoskeletons are valued for their ability to improve task efficiency, reduce muscle strain, and enhance overall worker well-being. However, the majority of existing research on exoskeleton benefits has primarily focused on male subjects. This focus leaves a critical gap in understanding female workers’ experiences and acceptance of this technology. Therefore, this study aims to address this gap by exploring factors influencing women’s acceptance of exoskeletons in construction tasks. It centers specifically on repetitive, labor-intensive activities such as bricklaying, which require frequent bending and lifting. This research investigates user-reported factors that contribute to overall comfort and usability, including pressure distribution, weight balance, heat buildup, and perceived ease of movement during extended wear periods. By focusing on female participants, this study seeks to identify specific design and usability factors that could encourage wider acceptance and sustained exoskeleton use. These insights may empower diverse populations to perform physically demanding tasks more independently and comfortably. Findings from this study are anticipated to inform the development of ergonomic solutions that accommodate a broader range of body types and preferences, directly supporting diversity and inclusion initiatives within the construction industry. Ultimately, this research aims to create more accessible work environments for women, expanding opportunities and promoting well-being across the workforce.",4,Waverly | Renaissance Waverly Hotel,Construction Engineering & Management: Addressing Risk Factors with Humans in Construction,249
899,6913.0,Enhancing Subcontractor Performance in Construction Projects through Gamification,Practitioner,"In the construction industry, subcontractors play a pivotal role in achieving project success. However, managing subcontractor performance remains challenging, with traditional methods often failing to maintain motivation and engagement. This study explores the use of gamification through employing game-like elements such as leaderboards, rewards, and real-time feedback mechanisms to improve subcontractor performance in Tehran's construction sector. Using a mixed-method approach, including surveys distributed to leading construction companies, the research examines the impact of gamification on key performance indicators such as productivity, teamwork, and quality adherence. Quantitative analysis through Spearman’s correlation shows significant positive effects of gamification on task completion rates and quality standards, while qualitative feedback highlights subcontractors’ enhanced motivation and sense of accountability. The findings suggest that gamification offers an innovative approach to performance management by aligning subcontractor goals with project requirements, fostering a culture of engagement, and improving overall project outcomes. This study contributes valuable insights for construction managers, highlighting gamification as a practical tool for enhancing subcontractor performance and suggesting avenues for future research in diverse construction environments.",5,Waverly | Renaissance Waverly Hotel,Construction Engineering & Management: Addressing Risk Factors with Humans in Construction,249
900,5311.0,A Simulation-Based Framework for Managing Risks in Modular Construction Projects,Practitioner,"Modular construction has gained significant attention for its potential benefits, such as shorter construction times, lower costs, and improved sustainability. However, it also presents unique challenges. The combination of off-site fabrication and on-site assembly in distributed modular construction introduces various risks, including labor risks, material delays, and environmental disruptions. All these risks and disruptions can severely impact project schedules and budgets. In fact, effectively managing these risks remains essential for the success of modular projects in distributed modular construction. This paper introduces an innovative framework for risk management in distributed modular construction. The framework applies advanced simulations, including Monte Carlo techniques to evaluate a wide range of risks and scenarios from common disruptions to rare, high-impact disruptive events. By focusing on risks specific to distributed modular construction, the tool offers detailed insights into potential delays and cost overruns. This approach equips project managers with better decision support capabilities and addresses a key gap in the literature by offering a systematic, practical method for managing modular construction risks, ultimately improving project performance and resilience. Future research avenues include exploring adaptive strategies that consider evolving risk factors and optimizing resilience across multi-project distributed modular construction supply chains.",1,Stanhope | Renaissance Waverly Hotel,Construction Engineering & Management: Modular Construction,250
901,5560.0,Safe Site Selection Approach for Modular and Mobile Hazard Category 2 U.S. Department of Energy Nuclear Facilities,Practitioner,"The U.S. Department of Energy (DOE) outlines safety basis requirements for Hazard Category 1, 2, and 3 nuclear facilities under Subpart B to 10 CFR Part 830. It mandates contractors to prepare for a preliminary documented safety analysis prior to construction or major modifications. This analysis must account for site-specific factors that influence safe design, including proximity to other facilities external hazards, meteorological conditions, and natural phenomena (e.g., wind and flooding). Traditionally, design strategy is based on a fixed-location facilities expected to operate for 50 years or more. However, the emergence of modular and mobile facility technologies adopted in the DOE nuclear sector has introduced new safety challenges that are not adequately addressed by existing regulations. This paper explores the unique parameters for safe site selection that arise from these innovative designs, emphasizing the need for revised safety analysis protocol. By identifying and analyzing these new safety considerations, this study aims to contribute to improved safety management practices for modular and mobile DOE nuclear facilities.",2,Stanhope | Renaissance Waverly Hotel,Construction Engineering & Management: Modular Construction,250
902,6643.0,"3D-Driven Process Improvement in Modular Construction: Propelling Efficiency, Quality and Safety",Practitioner,"Worldwide, traditional construction (TC) projects are typically executed at the erection site. However, TC is highly vulnerable to delays caused by unpredictable factors. In response, modular construction (MC) has emerged to reduce timelines by parallelizing construction processes. Under MC, components are manufactured and prepared offsite in assembly centers before being transported to the construction site for final installation. This factory-based approach shortens construction time by distributing the workload across various production stages, reducing pressure on the onsite construction phase. The essential role of assembly centers makes improving their operations critical. While optimization methods and simulation techniques can help, they often model an aggregated or steady-state view of the process, limiting the discovery of deeper inefficiencies. Since innovation is a cornerstone of MC, it is crucial to move beyond long-latency methods. Experimenting on demand with task parallelization scenarios, resource allocation adjustments, and layout modifications is essential for better capturing the real potential of MC. This study offers a threefold contribution. First, it proposes an immersive 3D platform that enables key decision-makers to visually identify inefficiencies, test potential workflow improvements, optimize asset allocation, and adjust task staffing - all with minimal time loss in a risk-free environment. Second, the platform provides extensive customization of worker actions while incorporating AI to autonomously make certain decisions, such as determining paths. This key feature enables more accurate task completion time estimates. Lastly, the platform integrates traditional KPIs as well as less commonly monitored indicators, including those related to ergonomic factors, contributing to improved worker safety.",3,Stanhope | Renaissance Waverly Hotel,Construction Engineering & Management: Modular Construction,250
903,8569.0,Modeling Supply Flow Orchestration In Modular Construction Project Supply Chains,Practitioner,"Modular construction has the potential for substantial cost savings, reduced construction time, and enhanced efficiency relative to conventional construction. Transitioning from conventional building techniques to distributed modular construction demands a new level of orchestration in supply flows to maximize efficiency. This paper introduces a conceptual framework for modular construction project supply chains and a dynamic model of operational orchestration, focusing on the supply flows through various stakeholders. The building erection site requires assembled modules from the module assembly center, which requires parts from the parts inventory. The methodology considers the flow and transition of parts dynamically from individual components to assembled modules. The model accommodates multiple concurrent modular construction projects with unique project timelines, staggered start dates, and varied module production rates. The methodology is illustrated through a case study that reflects realistic industrial scenarios, demonstrating the model's value and effectiveness in orchestrating supply chain flows for modular construction projects. This methodology supports multiple decision makers, starting from facilities designers and supply chain designers, to address supply chain orchestration on regional or national scales and can be embedded into the decision systems that manage the flow and transactions across the network.",4,Stanhope | Renaissance Waverly Hotel,Construction Engineering & Management: Modular Construction,250
905,6651.0,Quantifying the Impact of Hospital and Regional Factors on Length of Stay: A Multilevel Approach Using National Inpatient Sample Data,Practitioner,"Hospital length of stay (LOS) measures the duration of patient hospitalization and is a key metric in healthcare, affecting resource allocation, patient outcomes, and hospital efficiency. While patient characteristics influence LOS, hospital and regional factors also play a significant role. Using data from the National Inpatient Sample (NIS), this study develops a multilevel model to quantify the effects of hospital and regional factors on LOS, comparing it to a non-hierarchical approach that does not account for the nested structure of the data. Generalized Additive Models and Generalized Additive Mixed Models are used to assess the contributions of individual hospital characteristics as well as the combined effects of hospital and regional factors on LOS variability. The findings highlight critical hospital variables that impact LOS variability, with notable effects from cross-level interactions. This study establishes a foundation for a standardized LOS prediction model adaptable across diverse U.S. settings and provides targeted insights for hospital management and policymakers to improve resource allocation, enhance operational efficiency, and refine performance assessments.",1,Waverly | Renaissance Waverly Hotel,Data-Driven Strategies for Enhancing Healthcare Delivery,251
906,6250.0,Curiosity and Caution: Interpreting Standardized Maternal Health Data,Practitioner,"In the United States, pregnant people face a higher risk of morbidity and mortality than in any other high-income country, and the U.S. is the only one of these countries with rising maternal mortality. Profoundly, we know from maternal mortality reviews that occur in every U.S. state that 80% of these deaths are preventable. Nationwide efforts are underway to measure and understand causes of adverse outcomes – including the need for robust and standardized data to better understand the clinical and psychosocial drivers of outcomes for this population. Premier’s statistically deidentified PINC AI™ Healthcare Database (PHD), is a hospital-based, all-payer discharge database for geographically diverse inpatient and outpatient visits. The PHD also contains linked maternal and infant records which enable insights within the linked populations. International Classification of Diseases, Tenth Revision (ICD-10) coding is used to define maternal clinical conditions for analysis. The database is robust, standardized, and validated; but even such datasets come with their challenges. ICD-10 codes allow for standardized measurement, but when hospital coding practices are nonstandard, more questions need to be asked to understand the full story the data is telling. During this session, attendees will learn about how Premier collects disparate data sources, reconciles, validates, maps, and conducts quality assurance to create a robust, standardized database. They will then learn how even standardized data needs to be approached with caution and curiosity, to explore the data and create meaningful insights to better understand morbidity and mortality among the maternal population.",2,Waverly | Renaissance Waverly Hotel,Data-Driven Strategies for Enhancing Healthcare Delivery,251
907,8832.0,DESIGN PRUNING METHOD TO OBTAIN SIMPLE RULES FOR DIABETIC RETINOPATHY PREDICTION WITH ROUTINE LAB RESULTS,Academician,"Diabetic Retinopathy (DR) is a complication of diabetes in the retina. DR is among the leading causes of blindness among working age in the U.S. This study aims to develop a RuleFit model that can utilize routine lab results to identify patients with a high risk of DR and encourage them to take ophthalmic exams. The RuleFit model is employed for its ability to provide healthcare professionals with an accessible and interpretable analysis of DR risk factors, enhancing the model's decision-making process clarity. To improve interpretability and practical application in clinical settings, we introduce a novel “Pruning Method” that distills the RuleFit model’s extensive set of complex rules to a concise set of four simple rules, interpretable, and clinically practical rules that can aid physicians in understanding predictions. These rules can be directly used by healthcare providers without the need to run complex machine learning models, providing a straightforward tool for decision-making. The four simple rules results demonstrated an accuracy of 0.8589, a precision of 0.9866, a recall of 0.9871, and an F1 score of 0.9863. This study suggests that Diabetic Retinopathy is linked closely to diabetic neuropathy, nephropathy, creatinine, HbA1c, blood urea nitrogen, and anion gap. The RuleFit model, along with its integrated simple rules, shows good potential in DR screening with high explaining ability.",3,Waverly | Renaissance Waverly Hotel,Data-Driven Strategies for Enhancing Healthcare Delivery,251
908,6965.0,Disease Cluster Analysis in Electronic Health Records: Insights into Mortality and Comorbidity Patterns,Practitioner,"Understanding the relationships between diseases is crucial in managing patient health, especially when multiple conditions—known as comorbidities—occur together and increase the risk of poor outcomes. This study uses electronic health records (EHR) to identify clusters of co-occurring diseases associated with higher mortality. We apply hierarchical and k-means clustering methods to find patterns within these disease groups, then use the Apriori algorithm to examine associations between conditions within each cluster. Our analysis reveals comorbidity patterns that impact patient outcomes. These findings provide healthcare professionals with insights for early intervention and personalized treatment plans",1,Waverly | Renaissance Waverly Hotel,Modeling and Optimization for Effective Disease Prevention,252
909,6729.0,Optimizing Vaccination Strategies for Polio Eradication: A Case Study on Adaptive Supplementary Vaccination Campaigns Implementation in Nigeria,,"Thanks to extensive efforts by global and local health organizations, there has been much progress towards eradicating Polio; however, disease transmission and outbreaks continue to occur in some geographic areas. Recent guidelines for polio outbreak response emphasize strengthening routine immunization and implementing at least two rounds of high-coverage supplementary vaccination campaigns to curb the virus’ spread. However, operational challenges often create obstacles in vaccine delivery and coverage, limiting campaign effectiveness. Our research addresses these challenges by examining the complex interplay between vaccination coverage, timing, and their impact on outbreak size and cost-effectiveness. We conducted a comprehensive analysis of various vaccination strategies, assessing their potential to mitigate outbreaks and optimize resource allocation. Our study focused on the role of supplementary vaccination campaigns and explored approaches to integrate these campaigns into broader immunization efforts. To provide a robust comparison of different scenarios, we modified an existing simulation model that incorporates an automatic mechanism for initiating supplementary vaccination campaigns based on predefined detection thresholds. Our simulation results offer an analysis of the balance between outbreak reduction and resource allocation, providing a basis for future decision-making in vaccination strategy planning.",2,Waverly | Renaissance Waverly Hotel,Modeling and Optimization for Effective Disease Prevention,252
910,6314.0,Pediatric Tender Scheduling under uncertainty: A stochastic optimization approach,Practitioner,"Pediatric vaccine tender scheduling is critical for ensuring long-term stability and equitable coverage in low- and lower-middle-income markets. Effective scheduling not only facilitates equitable vaccine procurement but also helps prevent the spread of vaccine-preventable diseases. However, the stochastic nature of demand and year-to-year supply fluctuations presents significant challenges to tender planning. This study aims to address three key questions: (1) What is the optimal sequencing and scheduling of vaccine tenders to enhance affordability and profitability over extended planning horizons? (2) How do ongoing tenders influence long-term scheduling strategies? (3) How does an optimized tender scheduling approach compare to current year-to-year decision-making practices? To answer these questions, we propose a stochastic optimization model that accounts for demand uncertainty and supply variability. Additionally, we develop a current-practice approximation model for benchmarking purposes. The study explores multiple optimization objectives, examining how different priorities impact tender length, duration, and scheduling. We also analyze the role of quantity discount pricing and its influence on tendering decisions. Through this analysis, we aim to provide actionable insights into tender scheduling and procurement strategies, offering practical guidelines for maximizing affordability and profitability. Our findings highlight the importance of incorporating existing tender commitments into long-term planning and provide evidence-based recommendations on scheduling and procurement policies. Ultimately, we demonstrate that a long-term planning horizon fosters sustainable vaccine coverage, enhancing both market stability and public health outcomes.",3,Waverly | Renaissance Waverly Hotel,Modeling and Optimization for Effective Disease Prevention,252
911,6184.0,Measuring Student Attentiveness Using Eye-tracking and Visual-spatial Data Analytics,Academician,"With the increasing trend toward e-learning, accurately assessing student attentiveness has become a critical need for effective education. In traditional classrooms, teachers rely on nonverbal cues to gauge student engagement, but online environments limit this ability, hindering real-time feedback that can help adapt instructional methods. Thus, we proposed the student eye movements-based model to assess the student’s attentiveness more effectively. To train the model, the data is collected using ETVision glasses, and the features of eye movements, like the distance between the upper and lower eyelids, are captured. In addition, visual-spatial information such as the gaze point location on the object of interest is collected. The Closed Eye Aspect Ratio (CEAR) enhancement is used to track the eyelids movement. By analyzing the model’s experiments, different levels of student drowsiness will be detected. To further indicate the student’s visual cognition as a reflection of the representation of the external scene stimulus, we will use a Machine Learning model to map a relation between eye-tracking and the real world’s spatial perception. This vision perception-based approach will offer instructors a real-time attentiveness score that can categorize students into highly attentive, moderately engaged, or disengaged levels. The model’s robustness is further supported by its ability to identify trends across different lesson segments, enabling instructors to pinpoint challenging or disengaging content areas that may require more interactive or alternative teaching strategies. This model will bridge a significant gap in online education, offering data-driven insights to support personalized learning and improve overall instructional quality.",1,106 | Cobb Galleria Centre,AI and Analytics for Human-Centered Systems,253
912,6883.0,Advanced Computer Vision-Based Retail Analytics: A Novel Approach to Customer Behavior and Product Movement Tracking,Practitioner,"This study presents an innovative computer vision-based system for comprehensive retail analytics, offering unprecedented insights into customer behavior and product movement. Leveraging state-of-the-art object detection and tracking algorithms, the proposed system analyzes video data from multiple in-store cameras to track customer paths, product interactions, and shelf inventory levels in real-time. Results demonstrate the system's ability to generate granular data on customer dwell times, product popularity, and cross-shelf interactions. Key findings include the identification of high-traffic areas, patterns in product engagement, and temporal trends in shopping behavior. This research advances the field of retail analytics by providing a scalable, non-intrusive method for gathering rich behavioral data, with significant implications for store layout optimization, inventory management, and personalized marketing strategies.",2,106 | Cobb Galleria Centre,AI and Analytics for Human-Centered Systems,253
913,6960.0,Indigenous Fashion Industry: Case of the SIDS of the Caribbean,Practitioner,"Participation in the global fashion industry value stream is complex for countries of the global south, and this is intensified for the Small Island States of the Caribbean, which do not have substantial goods or labour markets. High, uncompetitive production labour cost as well as expensive imported raw materials are just two (2) issues experienced. Creativity and talent become secondary in the industry in the world famous for attracting the most counterfeiting and piracy. But what if the islands were to produce a 100% made in the Caribbean product? What would this look like? What materials and technology would be utilized? Most importantly, would it be technically, financially or operationally feasible?The paper drafts a conceptual framework for a sustainable 100% Caribbean fashion value stream, providing a roadmap for achieving this in the short- to medium-term. Desk research and interviews of industry participants are used and expert informants from across the Caribbean contribute to validating the framework.",1,119 | Cobb Galleria Centre,"Sustainable Production and Logistics in Textiles, Fashion and Education (SDG 9)",254
914,5747.0,"Optimizing Closed-Loop Supply Chains in the Textile Industry: A MILP Model for Sustainable Production, Recycling, and Logistics",Academician,"The increasing demand for sustainable practices across industries highlights the need for optimized supply chain models that balance profitability with environmental responsibility. In the textile industry, one major challenge is managing the life cycle of products to minimize waste and carbon emissions. This study addresses this issue by developing a closed-loop Mixed Integer Linear Programming (MILP) optimization model that identifies optimal recycling and transportation quantities to achieve a sustainable supply chain. The model focuses on balancing production, recycling, and logistics to reduce environmental impact while maintaining cost-effectiveness. Our research specifically examines the textile supply chain and integrates multiple factors: product type, quality levels, production sites, and distribution centers. The objective function minimizes emissions by incorporating both the forward and reverse logistics of new and recycled products, ensuring that supply chain emissions remain within a designated maximum entrance. Our conclusions from the model provide optimized solutions for CO₂ emissions reduction, cost-effective recycling, and efficient logistics organization. These solutions enable textile industries to make informed, environmentally responsible decisions in their supply chain operations. In addition, the model's versatility makes it applicable to different industrial sectors. It has the potential to be customized to different products, regions, and environmental constraints.",2,119 | Cobb Galleria Centre,"Sustainable Production and Logistics in Textiles, Fashion and Education (SDG 9)",254
915,9059.0,Engineering Education for a sustainable future: How reverse logistics and the circular economy are driving change.,Academician,"We examine the role of reverse logistics and the circular economy in advancing sustainability within Industrial Engineering and Engineering Education. Reverse logistics facilitates the collection and return of used products for recycling, remanufacturing, or proper disposal, reducing waste and conserving resources. The circular economy, on the other hand, extends product lifecycles by emphasizing reuse, repair, and recycling, creating a closed-loop system that minimizes environmental impact. Fortunately, there are industry examples such as those from Dell, IKEA, and Unilever to illustrate how these practices generate environmental, economic, and social benefits, while aligning with key Sustainable Development Goals (SDGs) such as SDG 12 (Responsible Consumption), SDG 8 (Decent Work), and SDG 13 (Climate Action). The essay highlights the crucial role of industrial engineering in optimizing processes, enabling businesses to reduce their carbon footprint and resource consumption while fostering economic growth and job creation in recycling and remanufacturing. Ultimately, reverse logistics and the circular economy offer a pathway for companies to contribute to a more sustainable and economically resilient future. Key Words: Engineering Education, Sustainability, Reverse Logistics, Circular Economy, SDG 8, SDG 12 and SDG 13",3,119 | Cobb Galleria Centre,"Sustainable Production and Logistics in Textiles, Fashion and Education (SDG 9)",254
916,6515.0,Arthur: An Artificial Intelligence Powered Teaching Assistant System for Engineering Economics Class,Academician,"Calculated Formula Questions (CFQs) are a prevalent assignment type in engineering courses to help students develop skills in solving real-world problems. Providing timely and personalized feedback for students’ CFQ assignments is critical but has been challenging in large classrooms. Artificial intelligence (AI) powered intelligent tutoring systems (ITSs) have the potential to help address this issue but their development and use have been constrained in assignment types with readily available structured digital data. This study developed a framework to train AI-powered ITS for CFQs. As a case study, we leveraged graded CFQ assignments to build a digitalized dataset. For each CFQ, we trained machine learning models as their solution diagnosis backbone to predict the mistakes. Our experiments demonstrate the feasibility of using these AI models for solution diagnosis, achieving on average a precision of 0.81, a recall of 0.79, and an accuracy of 0.65 in predicting mistakes. The AI backbone models were deployed on the web interface to form the AI-powered ITS ( Arthur ) to provide real-time and personalized feedback. Our framework offers a scalable approach for curating datasets and building AI-powered ITS for other engineering courses aiming to integrate ITSs.",1,103 | Cobb Galleria Centre,AI and Personalized Learning in Engineering Economics,255
917,9089.0,Strategic Decision-Making Under Disruption: Lessons from the Rental Car Industry’s Fleet Reductions During the COVID-19 Pandemic,Practitioner,"The COVID-19 pandemic triggered a dramatic decline in global travel, forcing car rental companies into critical decisions about their fleets. Many firms sold off substantial portions of their assets to preserve short-term liquidity, yet as travel rebounded, they faced challenges rebuilding operations amid supply chain disruptions and inflated vehicle costs. This study evaluates whether these fleet reductions were the optimal strategy for long-term sustainability and performance. Focusing primarily on revenue recovery time, with secondary insights into fleet utilization and market share, we employ system dynamics and economic modeling to assess the prudence of these decisions. Through scenario comparisons, we analyze the trade-offs between economic risk, exposure, and uncertainty, highlighting how reactive decisions made during the crisis may have undermined recovery efforts. The findings emphasize the need for more rigorous contingency planning and adaptive strategies to navigate extreme uncertainty. By examining how firms could have better anticipated the pandemic's trajectory and planned for post-disruption recovery, this study contributes to broader engineering economy principles. It underscores the importance of aligning short-term actions with long-term resilience, offering lessons for decision-makers in capital-intensive industries facing future crises.",2,103 | Cobb Galleria Centre,AI and Personalized Learning in Engineering Economics,255
918,5333.0,Multi-source Multi-task Weakly Supervised Transfer Learning for Telemonitoring of Parkinson’s Disease,Academician,"Telemonitoring involves using electronic devices to remotely monitor patients' health conditions, necessitating sophisticated models to interpret mobile-collected data for disease severity assessment. The task of transfer learning in this context is particularly challenging due to the heterogeneity of Parkinson’s disease manifestation across patients. This heterogeneity makes it impractical to combine all data into a single model for each patient, while training individual models for each patient is constrained by limited patient-specific data. Additionally, the variability in data labeling and availability, along with overlapping and conflicting source accuracies, further complicate the prediction task. We propose a novel Multi-source Multi-task Weakly Supervised Transfer Learning (M2WeST) method to address these challenges. M2WeST integrates diverse data sources and multi-task learning to enhance predictive performance. By effectively combining labeled and weakly supervised data, M2WeST significantly improves the accuracy of Parkinson's disease severity predictions based on tapping and voice activity data collected via mobile devices. Our approach demonstrates superior predictive accuracy compared to existing methods, providing a robust solution for telemonitoring Parkinson’s disease.",1,Ascot | Renaissance Waverly Hotel,Data-Driven Modeling and Monitoring of Health Conditions,256
919,5723.0,Personalized Prediction and Control of Blood Glucose Levels using Dynamic System Modeling,Academician,"Patients with diabetes in the ICU typically rely on two methods to monitor their blood glucose levels: blood tests analyzed in the laboratory and fingerstick tests. While fingerstick tests offer a cost-effective and convenient approach, they sacrifice accuracy, providing only approximate measurements. In this paper, we propose a dynamic system model combining deep learning models and Kalman filtering to accurately predict blood glucose trajectory and assist with personalized glucose control over time. The resulting model, called a deep Kalman filter (Deep-KF) model, can be applied in the ICU and other healthcare settings. We validate our model using randomized real-world glucose and insulin clinical data. We demonstrate how our model can deliver precise blood glucose forecasts and insulin delivery recommendations and be used as a decision-support tool for healthcare providers, hence making glucose monitoring more efficient and patient-centric.",2,Ascot | Renaissance Waverly Hotel,Data-Driven Modeling and Monitoring of Health Conditions,256
920,5731.0,All-printed wearable electrochemical biosensors for glucose detection in sweat,Academician,"Blood glucose measurement is a painful daily routine for diabetes patients. Hence, tremendous efforts have been attempted in developing noninvasive glucose sensors that focus on biofluids such as sweat, tear, saliva, and urine. Among all the alternative biofluids, sweat stands out as the most feasible candidate owing to the simplicity of collection, continuous secretion, and strong correlation with the glucose level in blood. This study presents the development and characterization of wearable electrochemical glucose sensors fabricated by electrohydrodynamic (EHD) printing technology. Silver electrodes were printed on a polyimide film, chosen for its flexibility, stability, and compatibility with wearable technologies. The working and counter electrodes were coated using carbon black, while the reference electrode was chloritized to form a stable Ag/AgCl reference. Glucose oxidase mixed with chitosan was immobilized on the working electrode, promoting stable enzyme adhesion and facilitating electron transfer. The sensor’s performance was evaluated for glucose detection prepared in a phosphate buffer solution (0.1 mM, pH 7.4) with a glucose concentration range of 0.1 to 1 mM, comparable to glucose concentrations in human sweat. Chronoamperometry at a potential of +0.6 V and cyclic voltammetry at a scan rate of 10-50 mVs -1 were employed to monitor the glucose oxidation, with measurements revealing sensitivity, stability, and reproducibility across multiple samples. Based on the obtained results, the fabricated electrochemical sensor highlights the potential for noninvasive glucose detection, leveraging wearable substrates and scalable EHD printing technology for accurate, affordable, and accessible biosensing solutions.",3,Ascot | Renaissance Waverly Hotel,Data-Driven Modeling and Monitoring of Health Conditions,256
921,5767.0,Machine Learning for Predicting NTSV Cesarean Risk: Transforming Obstetric Care through Data-Driven Insights,Practitioner,"The precise forecasting of Nulliparous, Term, Singleton, Vertex (NTSV) cesarean deliveries has become crucial for minimizing superfluous procedures and enhancing maternal and neonatal outcomes. This research employs machine learning approaches to evaluate cesarean delivery risk in NTSV patients within hospital environments, satisfying the requirements of proactive obstetric risk management. We assessed several machine learning techniques, including random forests, support vector machines, logistic regression, and neural networks, using a dataset of 1,065 patient records to predict cesarean compared to vaginal deliveries. The random forest classifier demonstrated the best performance amongst the different models, with an accuracy of 67.38%, an AUC of 0.6082, along with an F1 score of 0.6241, emphasizing maternal age, BMI, and systolic blood pressure as key factors in assessing cesarean risk. Our research demonstrates that using machine learning models with sophisticated visualization tools may substantially improve the prediction and control of NTSV cesarean deliveries, enhancing both maternal and neonatal outcomes. This technique illustrates the capability of incorporating predictive analytics into clinical processes to enhance evidence-based practices in obstetric care, facilitating wider applicability across other healthcare environments. The current approach exhibits intriguing outcomes; nevertheless, additional research will concentrate on improving accuracy through extensive feature engineering and hyperparameter optimization techniques.",4,Ascot | Renaissance Waverly Hotel,Data-Driven Modeling and Monitoring of Health Conditions,256
922,8720.0,Interdicting Networks to Maximize their Recovery Time: Applications to Disrupting Fentanyl Trafficking,Practitioner,"We consider an interdiction problem involving a network where the defender seeks to restructure the interdicted network to minimize the amount of recovery time required to reestablish a certain flow level, while the attacker aims to maximize this time. We model this problem as a bi-level mixed integer problem (BL-MIP). For certain realistic assumptions on the restructuring decisions, we use reformulation techniques to convert this problem into a single-level mixed integer problem (SL-MIP) by utilizing the special properties of the restructuring decisions and duality concepts. Additionally, we demonstrate how to solve the BL-MIP using a binary search method to provide a comparison with SL-MIP. We employ this novel approach to disrupting illegal drug trafficking networks by finding the interdiction plan that increases the restoration time of the networks- that is, the duration of the interdiction's effects within the network. We test our approaches over realistic fentanyl trafficking networks operating in South Carolina.",1,Ascot | Renaissance Waverly Hotel,IE Tools applied to Illicit Supply Networks,257
923,8824.0,Exploring the Formation of Fentanyl Supply Chains: An Analytical Approach,,"The opioid crisis remains a significant concern in the United States due to its widespread impact across various sectors. The growing consumption of synthetic opioids, such as fentanyl, has exacerbated the situation by significantly increasing the risk of overdose and fatalities. A key concern here is the inability to understand the discreet and complex supply chain associated with fentanyl. In this talk, we discuss how available limited information can be leveraged to derive possible insights into the supply chain of fentanyl and its analogs, including possible structures and goals. Concepts from integer programming and graph theory are used to attain this objective. We also demonstrate the efficacy of our approach through a case study.",2,Ascot | Renaissance Waverly Hotel,IE Tools applied to Illicit Supply Networks,257
924,5706.0,Assessing the Equity of Naloxone Distribution Across Communities in South Carolina,Academician,"The opioid epidemic remains a critical public health concern in U.S., with South Carolina (SC) experiencing its share of fatal consequences, including high rates of opioid overdose deaths. This study investigates the association between opioid overdose mortality and access to naloxone,a crucial opioid overdose reversal medication, focusing on the most affected areas in SC. Utilizing Geographic Information Systems (GIS), we perform spatial analyses of naloxone distribution across appropriate geographic units, identifying disparities in access related to overdose hotspots. Additionally, we explore various regression modeling techniques-including multivariate linear, Poisson, negative binomial, and spatial regression-to assess the relationship between naloxone distribution and opioid overdose mortality. Socioeconomic and demographic variables, such as unemployment rate, household income, and diversity index are integrated into the regression analysis to understand how these factors influence naloxone access and overdose outcomes. This study will provide insights into the equity of naloxone distribution in addressing the opioid crisis, particularly in areas with the greatest need. Findings from this research can inform targeted, data-driven interventions and policy measures to promote equitable access to naloxone and reduce opioid-related harms within SC communities.",3,Ascot | Renaissance Waverly Hotel,IE Tools applied to Illicit Supply Networks,257
925,5026.0,Impact of Naloxone Administration on 12-Month Recurrence Rates of Opioid-Related ED Visits in Illinois,Academician,"Opioid crisis in the United States has escalated to unprecedented levels, with overdose incidents and deaths rising sharply in recent years. In Illinois, as in many other states, opioid misuse has strained healthcare systems, resulting in a significant increase in emergency department (ED) visits for opioid-related issues. Naloxone, a crucial opioid antagonist administered in EDs to reverse overdoses, has become a primary intervention. However, its impact on long-term outcomes—particularly the recurrence of opioid-related ED visits—remains uncertain. This study aims to examine the 12-month recurrence rate of opioid-related ED visits and the role of naloxone administration, focusing on temporal trends and geographic variability. Additionally, a predictive model will be developed to estimate the likelihood of ED revisits. Using opioid-related ED visit data from Illinois (2018–2023), we first employ hierarchical logistic models to explore factors linked to naloxone administration and 12-month ED revisit rates. Temporal-spatial modeling and statistical approaches, including regression, generalized additive models (GAMs), and machine learning, will be compared to identify the optimal model for predicting ED revisit recurrence. Through this integrated approach, this study seeks to uncover critical factors of ED revisit rates, informing public health strategies and optimizing naloxone distribution policies to reduce recurrence and improve outcomes for individuals at high risk of opioid overdose.",4,Ascot | Renaissance Waverly Hotel,IE Tools applied to Illicit Supply Networks,257
926,6670.0,Optimizing Raw Material Ordering Policies for Efficient Precast Concrete Production under Fluctuating Demand and Price,Practitioner,"Purpose: This study addresses the challenge of optimizing raw material ordering policies in precast concrete (PC) manufacturing process. The specific objective of this research aims to find the optimal ordering quantities of multiple raw materials and cycle lengths for multi-period inventory systems with fluctuating demands for PC components and uncertain price jumps of raw materials by examining the challenges of varying product needs. Methodology: The study utilizes a stochastic optimization model to handle demand variability and material price fluctuations. This model integrates precast raw material batch sizing and procurement timing to achieve an optimal ordering policy that balances cost-effectiveness with production continuity of various PC products. Various scenarios reflecting demand uncertainty and price volatility are analyzed to assess the robustness of ordering strategies across fluctuating conditions. Findings: The proposed model effectively reduces total procurement costs and minimizes inventory holding requirements by adjusting batch sizes. By mitigating risks associated with demand uncertainty for PC products and price changes, the model enables more reliable production planning and inventory control. Research Limitations: This research focuses on PC manufacturing and may require adaptation for other sectors with similar supply chain complexities. The findings are sensitive to scenario assumptions, requiring validation with empirical data in large-scale applications. Practical Implications: The outcomes of this study provide valuable insights for manufacturers seeking to enhance supply chain efficiency in PC production. This optimization approach helps managers maintain a balance between inventory costs and production demands, ensuring timely availability of materials without overstocking.",1,Tyndall | Renaissance Waverly Hotel,20. Forecasting / Predictive Modeling Optimization,258
927,6466.0,Forecasting Container Throughput in Northwest Europe’s Inland Waterways: A Data-Driven Approach,Academician,"Accurate forecasting of container throughput volume (CTV), measured in Twenty-foot Equivalent Units (TEUs), is important for optimizing logistical planning and resource allocation in inland waterway transportation (IWT) systems. Failure to anticipate fluctuations in container volumes may lead to underutilization of resources and overwhelming congestion, resulting in significant financial losses. The literature shows the limitation of traditional individual CTV prediction models, highlights hybrid models' promise, and identifies a research gap in inland waterway CTV forecasting for Northwest Europe covering Belgium, Germany, Netherlands, and France. This study addresses this gap by employing a hybrid machine learning approach that integrates Artificial Neural Networks (ANN), Gated Recurrent Units (GRU), and Bayesian Neural Networks (BNN) with Support Vector Regression (SVR), enhanced by Random Forest-Recursive Feature Elimination (RF-RFE). Collected Eurostat (2007-2023) data includes economic, environmental, and logistical variables relevant to CTV. Results show that the hybrid model outperforms individual models, achieving the lowest error rates across all countries. In Belgium, it reached a Root Mean Squared Error (RMSE) of 12,661.12 TEU and a Mean Absolute Percentage Error (MAPE) of 2.29%. Similarly, it yielded strong results in Germany (RMSE 20,066.66 TEU, MAPE 2.81%), Netherlands (RMSE 31,593.75 TEU, MAPE 2.22%), and France (RMSE 9,164.19 TEU, MAPE 4.11%). These results affirm the hybrid model’s effectiveness in enhancing CTV forecasting accuracy, aiding stakeholders in making informed decisions, reducing congestion, and improving IWT reliability. Future work will focus on real-time forecasting with intermodal transport systems, and assessing the environmental impacts of predicted freight patterns.",2,Tyndall | Renaissance Waverly Hotel,20. Forecasting / Predictive Modeling Optimization,258
928,7021.0,Product Allocation Model For Curated Subscription Boxes With Dynamic Customer Pool,,"Subscription-based business models are getting increasingly popular in different areas such as streaming services, weekly food delivery, and periodic subscription boxes. This paper focuses on curated subscription-based businesses, which send their customers periodically a package that contains a selected set of products for the customers to try. The main challenges with this business model are that the curated boxes should not contain repeated products, and the customer subscription behavior is uncertain. We present a mathematical optimization model that determines the items in each customer’s curated box for each period over the planning horizon, while keeping customer satisfaction within a desired level. The dynamic nature of the customer subscription is modeled using a Markov Chain, and customer preferences toward different categories products are assumed to be collected through questionnaires at the time of subscription agreement. The mathematical optimization model can be used to find the best allocation for customer packages over a number of periods, and can also be used for long term planning for determining the necessary product portfolio and inventory levels for the company.",3,Tyndall | Renaissance Waverly Hotel,20. Forecasting / Predictive Modeling Optimization,258
929,6819.0,Integrated Optimization of Production and Inventory Management in Distributed Manufacturing,Academician,"Optimizing distributed manufacturing (DM) systems for efficient resource utilization and cost-effective production poses significant challenges due to the complex interdependencies among entities and dynamic operational conditions. These conditions can be classified into internal and external factors; for instance, internal factors include raw material shortages and capacity limitations, while order cancellations may be considered as an external factor. Manual decisions in offloading and transshipping products across DM systems sites often lead to inefficient operations costs and bottlenecks in order scheduling. In response to the need for automated and optimized decision-making, this research proposes a discrete, time-dependent integer programming approach that optimizes offloading and transshipping decisions while accounting for inventory levels and capacity constraints. The model was evaluated using a case study from a distributed manufacturing system of an international electronics manufacturing company. The objective is to achieve optimality in minimizing operations costs and identify optimal timeframes for system operation. Although our model demonstrates notable performance improvements, the inherent NP-hard nature of the problem introduces runtime gaps for large-scale instances, presenting opportunities for future research to enhance conventional integer programming methods.",4,Tyndall | Renaissance Waverly Hotel,20. Forecasting / Predictive Modeling Optimization,258
930,8737.0,A Data-Driven Deep Learning Framework for Package Delivery Time Estimation and Delay Duration Prediction,Academician,"Timely and reliable delivery time predictions are vital for maintaining efficiency and customer satisfaction in modern supply chains. However, the inherent complexity of logistics operations, including high-dimensional data, regional variability, and imbalanced data distributions, presents significant challenges to accurate prediction. This study proposes a framework leveraging deep learning and uncertainty quantification to effectively tackle these challenges and deliver robust and accurate predictions. The framework is structured around two sequential predictive steps: (1) a classification stage that determines whether a shipment will be on-time or delayed, and (2) a regression stage that estimates the delay duration for shipments predicted to be delayed. Advanced deep learning techniques, such as numerical and categorical embeddings, are utilized to enhance feature representation, enabling the models to be able to capture complex interactions in logistics data. Location-specific predictive models are developed to account for regional heterogeneity. Furthermore, uncertainty quantification is performed through conformal prediction and quantile regression, providing empirically valid prediction intervals and actionable insights. Experimental evaluations, conducted on real-world imbalanced logistics data, demonstrate the efficacy of the proposed method, outperforming traditional machine learning approaches across key metrics. The proposed framework not only improves prediction accuracy but also enhances decision-making by quantifying uncertainty, allowing customers to proactively identify high-risk scenarios. This work lays a foundation for scalable, data-driven methodologies that effectively balance predictive performance with operational reliability in the logistics domain.",1,Tyndall | Renaissance Waverly Hotel,25. Supply Chain / Allocation,259
931,5729.0,Distributed Programmatic Demand Forecasting in Modular Construction Through Machine Learning and Simulation,Academician,"Forecasting demand in the construction sector is essential for strategic planning, especially in distributed modular construction, where managing multiple projects across diverse locations requires precise resource allocation and timing. Accurate demand forecasts for multi-location distributed modular construction allow developers to optimize projects, allocate resources efficiently, reduce risks, and handle uncertainties. This paper focuses on contexts where industry leaders have major construction programs with target projects distributed over the territory (e.g., all 3-digit ZIP codes in the USA) in the coming years, with uncertainty around which projects will use modular construction, proceed to launch, and their size and starting times. Paper proposes a machine learning and simulation-based approach for forecasting programmatic demand in distributed modular construction, utilizing historical data on construction programs, launched projects from industry leaders, and economic indicators. Forecasting outcomes predicts the project numbers and sizes for each specific program or the entire territory, covering zones in each period within the planning horizon. By capturing complex relationships in the data, the model improves forecast accuracy, while simulations and scenario analyses handle economic uncertainties by adjusting key variables, enabling stakeholders to evaluate various future scenarios and support informed decision-making even in uncertain situations. This study offers a practical, data-driven methodology for program-based, multi-project, multi-location modular construction demand planning, enhancing strategic management in distributed modular construction. This research also opens avenues for expanding scope by adding features that enhance understanding of influencing factors, like dynamic news streams, enabling more nuanced insights for demand forecasting, planning, and shaping across modular construction scenarios.",2,Tyndall | Renaissance Waverly Hotel,25. Supply Chain / Allocation,259
932,6385.0,An Artistic Approach to Small Business Supply Chain Management,Academician,"Many small businesses are challenged with supply chain issues that, if unresolved, can lead to a company’s demise. Challenges such as becoming too reliant on one supplier and lack of capital are prevalent issues that small businesses must overcome. To address these concerns, this research develops guidelines and best practices for supply chain management at small businesses. This study focuses on a local skincare business that highly values protecting its public image and customer experience, so in some instances, it may trade off optimality for an increased cosmetic appeal. In light of this, this research carefully introduces supplier diversification and inventory management to the skincare company while still keeping the customer experience in mind. Showcasing supply chain management techniques with supplier quality, supplier quantity, and number of stockouts as metrics of success, this research displays the real-world feasibility of integrating different practices at a small business. This investigation offers a different perspective on balancing an optimal supply chain based on calculations while accommodating the artistry of a skincare business hoping to maintain its public image and user experience, laying the foundation for informed decision-making in the development of future small business supply chains.",3,Tyndall | Renaissance Waverly Hotel,25. Supply Chain / Allocation,259
933,6662.0,Branch and Incremental Benders cuts for two stage stochastic hub and spoke location-allocation problem,,"Two-stage stochastic optimization has applications in various domains, including location-allocation, scheduling, routing, and other network flow problems. In practice, most of these problems are solved following a decomposition framework such as L-shaped algorithms based on Benders decomposition. Conventional multi-cut and single-cut Benders decomposition algorithms often struggle to reach optimality due to time constraints or the complexity of effective cut generation. In this talk, we are going to discuss an incremental Benders decomposition algorithm where we generate cuts for a subset of scenarios and then incrementally add cuts until optimality reaches for all scenarios. We incorporated our proposed framework into a branch-and-cut framework to solve stochastic hub and spoke location-allocation problems. Additionally, a set of valid inequalities and warm-up heuristics are implemented to accelerate the cut-generation process. We perform numerical studies for different test instances to compare the performance of the proposed algorithm with off-the-shelf MIP solvers and classical Bender decomposition algorithms to emphasize the efficiency of our approach.",4,Tyndall | Renaissance Waverly Hotel,25. Supply Chain / Allocation,259
